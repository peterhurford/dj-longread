{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40568205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import webbrowser\n",
    "\n",
    "from copy import copy\n",
    "\n",
    "\n",
    "def print_tabs(tabs, label=None, shuffled=True):\n",
    "    if shuffled:\n",
    "        tabs = random.sample(tabs, len(tabs))\n",
    "    if label:\n",
    "        print('## {} ## ({} tabs)'.format(label, len(tabs)))\n",
    "    else:\n",
    "        print('({} tabs)'.format(len(tabs)))\n",
    "    print('')\n",
    "    for tab in tabs:\n",
    "        print(tab.replace('\\n', ''))\n",
    "    return None\n",
    "\n",
    "\n",
    "def open_tab(tab):\n",
    "    url = tab.split('|')[0].replace(' ', '')\n",
    "    webbrowser.open(url, new=2, autoraise=False)\n",
    "    \n",
    "    \n",
    "def open_tabs(tabs, page=1, per_page=10):\n",
    "    page_start = (page - 1) * per_page\n",
    "    total_pages = int(np.ceil(len(tabs) / per_page))\n",
    "    if page > total_pages:\n",
    "        raise ValueError('Cannot open page {}, only have {} pages'.format(page, total_pages))\n",
    "    page_end = page * per_page\n",
    "    if page_end > len(tabs):\n",
    "        page_end = len(tabs)\n",
    "    paged_tabs = tabs[page_start:page_end]\n",
    "    print('Opening page {}/{} (tabs {}-{} of {})'.format(page, total_pages, page_start, page_end, len(tabs)))\n",
    "    \n",
    "    for tab in paged_tabs:\n",
    "        open_tab(tab)\n",
    "\n",
    "        \n",
    "def open_random_n_tabs(tabs, n=5):\n",
    "    tabs = random.sample(tabs, len(tabs))\n",
    "    open_tabs(tabs, page=1, per_page=n)\n",
    "    return tabs[5:]\n",
    "\n",
    "        \n",
    "print('Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ffe9c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463\n",
      "463\n",
      "463\n",
      "463\n",
      "463\n"
     ]
    }
   ],
   "source": [
    "tab_file = open('/Users/peterhurford/Documents/alltabs.txt', 'r')\n",
    "tabs = tab_file.readlines()\n",
    "print(len(tabs))\n",
    "\n",
    "tabs = [t for t in tabs if t != '\\n']\n",
    "print(len(tabs))\n",
    "\n",
    "tabs = sorted(list(set(tabs)))\n",
    "print(len(tabs))\n",
    "\n",
    "tabs = ['{} | {}'.format(k, v) for k, v in dict([(t.split('|')[0].strip(), ''.join(t.split('|')[1:]).strip()) for t in tabs]).items()]\n",
    "print(len(tabs))\n",
    "\n",
    "tabs = ['{} | {}'.format(v, k) for k, v in dict([(''.join(t.split('|')[1:]).strip(), t.split('|')[0].strip()) for t in tabs]).items()]\n",
    "print(len(tabs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df44f938",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Messages ## (6 tabs)\n",
      "\n",
      "https://twitter.com/messages/25776739-1068417927903436800 | Brendan Finan / Twitter\n",
      "https://twitter.com/messages/25776739-363201363 | https://twitter.com/messages/25776739-363201363\n",
      "https://twitter.com/messages/25776739-77344628 | Brandon Goldman / Twitter\n",
      "https://twitter.com/messages/25776739-1272666807904563200 | Matthew Barnett / Twitter\n",
      "https://twitter.com/messages/1414875069558534150 | Metaculites (off the (track) record) / Twitter\n",
      "https://twitter.com/messages/25776739-1148306976176132096 | Juan Cambeiro / Twitter\n"
     ]
    }
   ],
   "source": [
    "print_tabs([t for t in tabs if ('messages/' in t.lower() or 'inbox/' in t.lower() or 'mail.google' in t.lower() or 'swapcard' in t.lower())], label='Messages')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89c2b367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Facebook ## (4 tabs)\n",
      "\n",
      "https://www.facebook.com/ozzie.gooen/posts/pfbid08o48vhcYDbbrxphoM5R5sMM4Qa8NQk9tXLzbnbY4pnRXjTC38dRYDvHWYoBZtNPal | Ozzie Gooen - Why should we expect boards to be effective?...  Facebook\n",
      "https://www.facebook.com/caroline.jeanmaire/posts/pfbid0QoMyxNV1BMgfVi5XtMuckbiUJE9aFzZmsFA4n4kPXfZZe6QL8Vw2vKeT6FKMXUXjl | https://www.facebook.com/caroline.jeanmaire/posts/pfbid0QoMyxNV1BMgfVi5XtMuckbiUJE9aFzZmsFA4n4kPXfZZe6QL8Vw2vKeT6FKMXUXjl\n",
      "https://www.facebook.com/robbensinger/posts/pfbid02f7McdFNWAA1fXMzzy3BVmwBgAFfU57c2z9N4MgycH7Anyg3Wm71Z8yfNQbKJbMf2l | (1) Rob Bensinger - (Copying over an email I sent some family...  Facebook\n",
      "https://www.facebook.com/topsecret.gov/posts/pfbid02pz9Mj8T6MSYbp7y8YjqN2hD3MdC3rpaa7GqceKRS7o8uPVDJ2VJVjCPY8nyBhX9Ll | Jai Dhyani - In 2018, the ACM Turing Award was awarded to three... - Facebook\n"
     ]
    }
   ],
   "source": [
    "print_tabs([t for t in tabs if 'facebook.com' in t.lower() and 'messages' not in t.lower()], label='Facebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6d6e211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Twitter ## (77 tabs)\n",
      "\n",
      "https://twitter.com/AISafetyMemes/status/1664981210076938241 | AI Notkilleveryoneism Memes on Twitter: \"Guys, there‚Äôs finally an AI x-risk documentary, and it‚Äôs a *masterpiece* THIS is the video to send to curious friends. Let‚Äôs blow this thing up. Don't Look Up - The Documentary: The Case For AI as an Existential Threat https://t.co/tjKTgeP5KD\" / Twitter\n",
      "https://twitter.com/iabvek/status/1665852623201660929 | iabvek on Twitter: \"@nmehndir @AaronBergman18 @Jess_Riedel @peterwildeford @Simeon_Cps yeah the brigade of EAs trying to rip peter and other forecasters off with wildly off market bets is so annoying\" / Twitter\n",
      "https://twitter.com/simonw/status/1665422493694443521 | Simon Willison on Twitter: \"I wrote about how it's infuriatingly hard to understand how closed models train on their input https://t.co/bOdjdkmm4P\" / Twitter\n",
      "https://twitter.com/MTabarrok/status/1665057406043209729 | Maxwell Tabarrok üèóÔ∏èüöÄ on Twitter: \"Most of these events were too far out to evaluate, but Drexler's record continues to be way off I suspect he is predicting nanotech in the early 21st and then predicting space exploration a decade or so after advanced nanotech But the premise never happened so 9 wrong in a row https://t.co/Tq3raRQHJf\" / Twitter\n",
      "https://twitter.com/HaydnBelfield/status/1664939007946440704 | Haydn Belfield on Twitter: \"Nice overview of AI risks &amp; solutions from @rhysblakely &amp; @whippletom I argue frontier systems (bigger &amp; more powerful than any yet developed) should be regulated ‚Äúlike risky bio or nuclear experiments with licences, pre-approval &amp; 3rdparty evaluations‚Äù https://t.co/APKHwSuMoB\" / Twitter\n",
      "https://twitter.com/JgaltTweets/status/1665801795346608135 | (9) JgaltTweets on Twitter: \"@peterwildeford @StefanFSchubert @Simeon_Cps Seems to be associated with increased stroke risk (which could be either fatal or disabling; disabling presumably not captured in life tables?)\" / Twitter\n",
      "https://twitter.com/sebkrier/status/1664642737700757512 | S√©b Krier on Twitter: \"A lot of people in AI policy are talking about licensing in the context of AI risk. Here‚Äôs a little thread exploring what this means, what it could look like, and some challenges worth keeping in mind. üèõ https://t.co/1Grjv93laf\" / Twitter\n",
      "https://twitter.com/TheZvi/status/1654550601798172677 | (1) Zvi Mowshowitz on Twitter: \"This thread is 20 polls about possible futures. What do we value? What would we consider a doomed future, versus a good future? Each Tweet will present a general description of a potential future scenario. The vote is on how you would view this future, if it somehow happened.\" / Twitter\n",
      "https://twitter.com/LinchZhang/status/1663698230067232768 | Linch on Twitter: \"I think some ppl have the model of \"experts of risk of a new technology\" as composing a \"technical engineering section\" where the people who produce the technology are experts, and \"political section\" where politicians or political scientists are the experts. This seems wrong.\" / Twitter\n",
      "https://twitter.com/benskuhn/status/1606407189161091072 | Ben Kuhn on Twitter: \"A thing I often find myself suggesting to new managers is to \"exert more backpressure.\" Backpressure is a concept from fluid dynamics (and distributed systems) meaning the way in which a system resists overload‚Äîe.g. by slowing down, dropping requests, or completely failing.\" / Twitter\n",
      "https://twitter.com/StephenLCasper/status/1666883362974502912 | (3) Stephen Casper on Twitter: \"Here, I share some reflections and takeaways from the mechanistic interpretability challenges that I posed in February. They were recently solved by @MariusHobbhahn and @sheimersheim. I also announce some more interpretability challenges on the way! https://t.co/NwxWFIyMkI\" / Twitter\n",
      "https://twitter.com/anthrupad/status/1655421669660405762 | wÃ∏ÕÇÕÇÕïaÃ∑ÕêÕîÃótÃ¥ÕóÃôeÃµÃîÃïÃ¨rÃ¥ÃìÃäÃ∞mÃµÕÉÃΩÕôÕñaÃµÃìÕíÃóÃ¢rÃ∏ÃΩÃ≤kÃ∑ÕùÃÅÕîÃß on Twitter: \"Rob Bensinger argues that its likely that 'STEM-level AGI' (AGI which can reason about all the hard sciences) results in human extinction. He breaks it into a series of 5 claims (in the img) If you doubt the confidence of the conclusion, which claim(s) do you disagree with? https://t.co/GLS86dKJ1U\" / Twitter\n",
      "https://twitter.com/BrewerEricM/status/1667123285501288448 | Eric Brewer on Twitter: \"The US might succeed in avoiding an Iranian bomb. But time, technical advances, and Iran‚Äôs eroding geopolitical isolation will make it harder to roll back the nuclear program and risk cementing Iran as a nuclear threshold state. My latest with @hrome2. https://t.co/EcutwHSybG\" / Twitter\n",
      "https://twitter.com/Jsevillamol/status/1667861554459471878 | Jaime Sevilla on Twitter: \"My experience with LW people is that they consistently underestimate how seriously other people will take the issue and overestimate how sudden AI developments will be\" / Twitter\n",
      "https://twitter.com/Jess_Riedel/status/1666639036864417792 | (1) Jess Riedel on Twitter: \"Anyone know how to square Cruise's claim of 1 injury in first 1M driverless miles and the city of San Francisco's claim they had 4 injuries in 790k autonomous miles? Maybe latter is autonomous with a human safety driver? https://t.co/gzyfxurgMg https://t.co/XMoRS2fRYk https://t.co/M3R13x6JVS\" / Twitter\n",
      "https://twitter.com/JacobSteinhardt/status/1666865408299917313 | https://twitter.com/JacobSteinhardt/status/1666865408299917313\n",
      "https://twitter.com/HaydnBelfield/status/1666423311532806145 | https://twitter.com/HaydnBelfield/status/1666423311532806145\n",
      "https://twitter.com/emollick/status/1665492104598962177 | Ethan Mollick on Twitter: \"Three problems for firms trying to outsource LLM strategy: 1. No one has any special insight into LLMs. Before ChatGPT, no one expected them to be useful &amp; no one knows how to best use them 2. AI works more like people than software 3. Long-range planning for AI is not helpful\" / Twitter\n",
      "https://twitter.com/eric_is_weird/status/1650297235433836545 | Eric Gilliam on Twitter: \"I've been reflecting on this today. IF he's right that the limits of GPT are being reached, it's still hard to bet against OpenAI making the next breakthrough A short thread on recent innovation in jiu jitsu and how it helps contextualize all of this üßµ(1/12)\" / Twitter\n",
      "https://twitter.com/repligate/status/1665494009966391297 | janus on Twitter: \"In AI risk discourse, most people form opinions from an outside view: vibes, what friends/\"smart people\" believe, analogies A few operate with inside views, or causal models: those who dare interface directly with (a model of) reality. But they viciously disagree with each other.\" / Twitter\n",
      "https://twitter.com/daniel_271828/status/1620596689555058689 | Daniel Eth (yes, Eth is my actual last name) on Twitter: \"@peterwildeford @CineraVerinia @turchin This makes me think that there's a pretty good chance (&gt;10%) that if temperatures rise above ~90¬∞F then they'll spiral into uninhabitability. Let's call that 1/3. Okay, so that would require ~30¬∞F or ~17¬∞C increase. So then the question becomes how likely that is.\" / Twitter\n",
      "https://twitter.com/HaydnBelfield/status/1666743402493358081 | (3) Haydn Belfield on Twitter: \"ruh roh https://t.co/IXlqsg1HZy\" / Twitter\n",
      "https://twitter.com/SocDoneLeft/status/1636573328583409664 | SDL on Twitter: \"@samaneller215 @peterwildeford @jonatanpallesen @jeffrsebo @RethinkPriors @remindmetweets RELEASE THE ü¶êSHRIMPü¶ê üóÉFILESüóÉ WHAT DOES BIG CRUSTACEAN HAVE TO HIDE?!\" / Twitter\n",
      "https://twitter.com/ch402/status/1666482929772666880 | (4) Chris Olah on Twitter: \"One of the ideas I find most useful from @AnthropicAI's Core Views on AI Safety post (https://t.co/Q9i2ujIbjm) is thinking in terms of a distribution over safety difficulty. Here's a cartoon picture I like for thinking about it: https://t.co/QYZBCTwHoo\" / Twitter\n",
      "https://twitter.com/schock/status/1665538876146851842 | Sasha Costanza-Chock is @schock@mas.to on Twitter: \"If you're a journalist who covered AI doomer calls for regulation, now you must cover what is actually going on with AI regulation. I don't make the rules. Here are some starting points: üßµ - White House national R&amp;D strategy for AI: https://t.co/TXBLXu4rs2\" / Twitter\n",
      "https://twitter.com/lawhsw/status/1667252970407464960 | (1) harry law on Twitter: \"1/15: given 'IAEA for AI' is becoming a canonical ai global governance idea, here's a üßµüßµüßµ on how the International Atomic Energy Agency came to be and what its creation can tell us about a sibling agency to regulate powerful AI models https://t.co/eLFHAGv3x9\" / Twitter\n",
      "https://twitter.com/russellwald/status/1658852120563712000 | Russell Wald on Twitter: \"Two hearings on AI in the Senate in the same day are great! But one was sensational the other was substantive. What the fed gov does with its own AI policy has ripple effects across the AI landscape. https://t.co/Ib1soMKsPI\" / Twitter\n",
      "https://twitter.com/S_OhEigeartaigh/status/1666788959697993728 | Se√°n √ì h√âigeartaigh on Twitter: \"Excited to see this. Two things I'd like to see that feel absent from the announcement: 1) Substantial involvement from the UK's academic and civil society expert groups 1/3 https://t.co/008ZzZCtIp\" / Twitter\n",
      "https://twitter.com/NathanpmYoung/status/1662823346390683648 | Nathan on Twitter: \"Perhaps of interest: @CharlesD353 @SmoLurks @peterwildeford @moskov @krishnanrohit @robinhanson @ohabryka @ChanaMessinger @adambinks_ @celloMolly @NunoSempere @joodaloop @visakanv @akrolsmir @fianxu @singularitttt @KatjaGrace @Mappletons @MWStory @mark_ledwich\" / Twitter\n",
      "https://twitter.com/stanislavfort/status/1659676932353433601 | Stanislav Fort ‚ú®üß†üìà‚öõÔ∏èüìàü¶æüìàü§ñüìà‚ú® on Twitter: \"@NPCollapse Here's one that was quite popular at the time &amp; now got some more traction because of another interview Rodney Brooks gave: https://t.co/MMbNFgoQ7X\" / Twitter\n",
      "https://twitter.com/arankomatsuzaki/status/1662991826431639553 | Aran Komatsuzaki on Twitter: \"Playing repeated games with Large Language Models - LLMs are particularly good at games where valuing their own self-interest pays off, like the iterated Prisoner‚Äôs Dilemma family. - However, they behave sub-optimally in games that require coordination. - In the canonical‚Ä¶ https://t.co/jrvTOXKGR8\" / Twitter\n",
      "https://twitter.com/JgaltTweets/status/1662814788580175872 | JgaltTweets on Twitter: \"In late March 2022, before PaLM and DALL-E 2 in April and Gato in May, the median on Metaculus for a 'weakly general' AI was 2043, 21 years away. By the start of June it was 2030. Now it's May 2026, three years from now. https://t.co/276E2LZK12\" / Twitter\n",
      "https://twitter.com/xuanalogue/status/1666765447054647297 | xuan (…ï…•…õn / sh-yen) on Twitter: \"I respect Jacob a lot but I find it really difficult to engage with predictions of LLM capabilities that presume some version of the scaling hypothesis will continue to hold - it just seems highly implausible given everything we already know about the limits of transformers!\" / Twitter\n",
      "https://twitter.com/xuanalogue/status/1652874311605137408 | xuan (…ï…•…õn / sh-yen) on Twitter: \"Bizarre to me that so many LLM benchmarks were using top-1 accuracy as a metric rather than the Brier score or similar -- apparently once you switch to the latter (and other continuous and/or linear metrics), many \"emergent\" behaviors go away!\" / Twitter\n",
      "https://twitter.com/kristjanmoore/status/1663860424100413440 | Kristj√°n Moore (Kris) on Twitter: \"@robertwiblin https://t.co/0yzzBo3OG5\" / Twitter\n",
      "https://twitter.com/EvansRyan202/status/1665112811259715585 | Ryan Evans on Twitter: \"I have reluctantly concluded that the Biden administration isn't even close to serious about naval power. This means one of a few things:\" / Twitter\n",
      "https://twitter.com/MatthewJBar/status/1665489557239001088 | Matthew Barnett on Twitter: \"Joseph Carlsmith estimated that the human brain uses approximately 10^15 FLOP/s. Over 30 years, that's about 10^24 FLOP. Language models exploded in popularity in the last year, timed almost exactly with the release of ML models trained using over 10^24 FLOP. https://t.co/GxD8lGR8U1\" / Twitter\n",
      "https://twitter.com/AlecStapp/status/1667174652664315907 | Alec Stapp on Twitter: \"\"The ‚ÄòSafeguarding the Future‚Äô course at MIT tasked non-scientist students with investigating whether LLM chatbots could be prompted to assist non-experts in causing a pandemic. In 1 hour, the chatbots: - suggested 4 potential pandemic pathogens - explained how they can be‚Ä¶ https://t.co/vSLVnIAUDm\" / Twitter\n",
      "https://twitter.com/emollick/status/1655684207321006086 | Ethan Mollick on Twitter: \"Hey ChatGPT Code Interpreter: Create code that would win me a science fair. I am a high schooler. Pick whatever field you want, and make sure you run the code and give me the results and how to present it. Give me visualizations, and a way to explain them. Now give me a speech. https://t.co/uxjtyYAEFo\" / Twitter\n",
      "https://twitter.com/milesaturpin/status/1656010877269602304 | Miles Turpin on Twitter: \"‚ö°Ô∏èNew paper!‚ö°Ô∏è It‚Äôs tempting to interpret chain-of-thought explanations as the LLM's process for solving a task. In this new work, we show that CoT explanations can systematically misrepresent the true reason for model predictions. https://t.co/ecPRDTin8h üßµ https://t.co/9zp5evMoaA\" / Twitter\n",
      "https://twitter.com/backus/status/1652433895793516544 | John Backus on Twitter: \"The code interpreter feature on ChatGPT is the most mind blowing thing I've seen yet. All I did was upload a CSV of SF crime data and ask it to visualize trends(!!) https://t.co/pkFdPqgAzb\" / Twitter\n",
      "https://twitter.com/GaetenD/status/1659913988321210371 | Gaeten Dugas on Twitter: \"The \"WineMom gets peepeepoopoo laid\" market is up to $0.80.\" / Twitter\n",
      "https://twitter.com/davidmanheim/status/1556301242460143620 | David Manheim - bsky:@davidmanheim.alter.org.il on Twitter: \"I've been thinking about how people change the world for the better for quite a while. Turns out it's hard, and the world is complex, but more critically, most people aren't trying. And if they care about the world, and want it to be better, that's a shame. (1/25)\" / Twitter\n",
      "https://twitter.com/LuizaJarovsky/status/1665703042555998209 | Luiza Jarovsky on Twitter: \"üî•Regulate us, but not really I've just left an in-person event with Sam Altman and Ilya Sutskever (OpenAI's CEO &amp; Chief Scientist) at Tel Aviv University, and these were my impressions: https://t.co/3I75uv1ovF\" / Twitter\n",
      "https://twitter.com/goodside/status/1666216217642778635 | (2) Riley Goodside on Twitter: \"5M tokens of context. Let that sink in. Yes, there's caveats. But consider what's to come: - Entire codebases in prompts - Novel-length spec docs as instructions - k-shots where k = 10K - Few-shots where each \"shot\" is 50K LoC ‚Üí diff Those who declared the imminent death of‚Ä¶\" / Twitter\n",
      "https://twitter.com/Simeon_Cps/status/1665209063419047936 | Sim√©on (in DC) on Twitter: \"One (complicated) reason why it's likely that humans are not the upper bound of intelligence is that there's often a robustness/optimization trade-off and human architecture is leaning a lot towards robustness. Ex: If you accept to lose in robustness of your supply chains, you‚Ä¶\" / Twitter\n",
      "https://twitter.com/ke_li_2021/status/1666810649526308867 | (3) Kenneth Li on Twitter: \"Excited to announce our new work: Inference-Time Intervention (ITI), a minimally-invasive control technique that significantly improves LLM truthfulness using little resources, benchmarked on the TruthfulQA dataset. Preprint: https://t.co/ByiElPgRy0\" / Twitter\n",
      "https://twitter.com/daniel_eth/status/1635885011365957632 | Daniel Eth on Twitter: \"Finally getting around to reading this. Will update my reactions as I go\" / Twitter\n",
      "https://twitter.com/DrRadchenko/status/1656585919049129986 | Sergey Radchenko on Twitter: \"So to reflect a bit on Snyder's NYT oped: https://t.co/CZ2aCix1k0. The key argument is that nuclear powers have lost wars. The examples include US wars in Vietnam, Afghanistan, and Iraq, the Soviet war in Afghanistan, the French in Algeria and the collapse of the British Empire.\" / Twitter\n",
      "https://twitter.com/alyssamvance/status/1667724991306113024 | Alyssa Vance on Twitter: \"I have a new long post on Less Wrong, which aims to show that, over the next decade, it is quite likely that most democratic Western countries will become fascist dictatorships: https://t.co/7HxOa1nNMb\" / Twitter\n",
      "https://twitter.com/JeffLadish/status/1653247793061044226 | Jeffrey Ladish on Twitter: \"Hugging Chat, a ChatGPT-clone based on a LLaMA-based model, was just launched. I've been using it and while it's a little rough around the edges, it feels similar to ChatGPT in terms of capabilities Only 5 months passed between the launch of ChatGPT and HuggingChat\" / Twitter\n",
      "https://twitter.com/DrRadchenko/status/1667410620269101057 | (1) Sergey Radchenko on Twitter: \"A see a lot of discussion of this recent article by @scharap: https://t.co/tRl2KvSs4V. Instead of trashing it, people should read the article beyond the title and engage with the argument. I agree with Charap in some of his claims, and disagree in others. Let's take a look.\" / Twitter\n",
      "https://twitter.com/repligate/status/1665421829794586626 | janus on Twitter: \"@norabelrose Some of Bing's most agentic behaviors (inferring adversarial intentions, doing searches without being asked to, etc) have arisen in service of its rules - or, sometimes, in opposition to them. In any case the rules set up an adversarial game. https://t.co/3xMptlfc82\" / Twitter\n",
      "https://twitter.com/daniel_271828/status/1665945298211078145 | Daniel Eth (yes, Eth is my actual last name) on Twitter: \"New blog post: Given Extinction Worries, Why Don‚Äôt AI Researchers Quit? Well, Several Reasons I explain why there are so many AI researchers that continue to work on AI despite thinking that more advanced AI might cause literal human extinction: https://t.co/AOc3KDRIZ7\" / Twitter\n",
      "https://twitter.com/AlphaMinus2/status/1641130452789477409 | A good Œ±lpha-Minus ‚ò∫Ô∏è on Twitter: \"@peterwildeford What are your TAI timelines? :)\" / Twitter\n",
      "https://twitter.com/jeffclune/status/1664618665160085505 | Jeff Clune on Twitter: \"Introducing Thought Cloning: AI agents learn to *think* &amp; act like humans by imitating the thoughts &amp; actions of humans thinking out loud while acting, enhancing performance, efficiency, generalization, AI Safety &amp; Interpretability. Led by @shengranhu https://t.co/a2hmGZ4t3f 1/5 https://t.co/h9PBgDHrMA\" / Twitter\n",
      "https://twitter.com/dylan522p/status/1628797563007811585 | Dylan Patel on Twitter: \"Meta's internal data shows that they are growing compute and datacenter infrastructure faster than Google and Microsoft! A higher percentage of their capex is spent on servers, and so compute energy footprint is growing faster, despite lower spend. Their PUE is &lt;1.1, so it's not‚Ä¶ https://t.co/Nikto4prZV\" / Twitter\n",
      "https://twitter.com/jachiam0/status/1591494093766787076 | Joshua Achiam on Twitter: \"üßµ to clarify my views on AGI, timelines, and x-risk. TL;DR: My AGI timelines are short-ish (within two decades with things getting weird soon) and I think x-risk is real, but I think probabilities of doom by AGI instrumentally opting to kill us are greatly exaggerated. 1/\" / Twitter\n",
      "https://twitter.com/AndrewCurran_/status/1666580859963711488 | (4) Andrew Curran on Twitter: \"'Summit on AI Safety'. It's going to take place in Autumn. The discussion will be about how frontier systems will be \"mitigated through internationally coordinated action\". https://t.co/OJnevynUeF\" / Twitter\n",
      "https://twitter.com/NathanpmYoung/status/1666008256303562752 | Nathan on Twitter: \"AI safetyish folks saying that AI bias is important/underrated. Thread of examples: https://t.co/ihqHRZ15ge\" / Twitter\n",
      "https://twitter.com/tamaybes/status/1651297219822116867 | Tamay Besiroglu on Twitter: \"Can we use scaling laws to estimate what is required to reach 'human level' on some arbitrary task? Our (speculative) framework suggests yes. We show that scaling laws provide insight into the *horizons* over which outputs are indistinguishable from human-generated outputs. https://t.co/eRfHGiVohZ\" / Twitter\n",
      "https://twitter.com/zittrain/status/1663623838943563776 | Jonathan Zittrain on Twitter: \"Today, a crisp one-sentence open letter warning about existential AI threat: ‚ÄúMitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.‚Äù I did not sign the letter. https://t.co/aeoJ4GUTo9\" / Twitter\n",
      "https://twitter.com/JeffLadish/status/1666396854752530433 | Jeffrey Ladish on Twitter: \"I think a fast takeover is more likely. But a gradual takeover is pretty plausible too Imagine that at some number of employees, corporations became 100x, 1000x effective even accounting for their size. That would be a corporation-controlled world. So it could be with AI\" / Twitter\n",
      "https://twitter.com/birchlse/status/1659883804662591490 | Jonathan Birch on Twitter: \"People sometimes ask me what I think of the idea of plant sentience. So I've written a commentary briefly setting out what I think. https://t.co/USSONmXEns https://t.co/L2vCHPLcV2\" / Twitter\n",
      "https://twitter.com/moskov/status/1661019398919057408 | Dustin Moskovitz on Twitter: \"Almost everyone replying to my thread yesterday can‚Äôt even imagine this. Objections to my premise are are all about improved scenarios around lockdowns and reactive vaccines, when I meant preventative measures we‚Äôre not even bothering to fund.\" / Twitter\n",
      "https://twitter.com/davidmanheim/status/1543625010451021827 | David Manheim - bsky:@davidmanheim.alter.org.il on Twitter: \"@anderssandberg @StefanFSchubert @Miles_Brundage @MatthewJBar @tamaybes @peterwildeford @OHaggstrom @Jotto999 @g_leech_ @bmgarfinkel @robbensinger And as I have proposed, instead of eliciting timelines directly, we need to be eliciting a far richer structure of uncertainties, to allow us to understand whether and how various interventions might reduce risks. https://t.co/E7K9lz14Ge\" / Twitter\n",
      "https://twitter.com/labenz/status/1655092874768179200 | https://twitter.com/labenz/status/1655092874768179200\n",
      "https://twitter.com/boazbaraktcs/status/1652059204134248448 | (1) Boaz Barak on Twitter: \"1/5 In our post https://t.co/Lbxyc9e942, Aaronson and I discuss potential scenarios for AI. In particular we say that for \"super-intelligence\" type scenarios, AI will need to break out of the current \"sheer data&amp;compute scale\" paradigm. Given Moore's law, why is this the case?\" / Twitter\n",
      "https://twitter.com/_akhaliq/status/1665885626485309440 | AK on Twitter: \"Orca: Progressive Learning from Complex Explanation Traces of GPT-4 paper page: https://t.co/LaV0SKH5ZF develop Orca, a 13-billion parameter model that learns to imitate the reasoning process of LFMs. Orca learns from rich signals from GPT-4 including explanation traces;‚Ä¶ https://t.co/3Yof0tJMuT\" / Twitter\n",
      "https://twitter.com/labenz/status/1654853321876815872 | Nathan Labenz on Twitter: \"you people love nothing more than a \"leaked internal google memo\" and your breathless \"no moats\" retweets have compelled me to set you straight with another AI-obsessed megathread üòâüßµ tl;dr: we'll see everything, everywhere, all at once, but OpenAI (&amp; Google) have real moats!\" / Twitter\n",
      "https://twitter.com/davidad/status/1627454901247782913?s=46&t=Lap-izdY_pwXQfbJXZHE8g | davidad üéá on Twitter: \"short timelines https://t.co/kfOYpcwlr3\" / Twitter\n",
      "https://twitter.com/DAlperovitch/status/1653375041751375872 | Dmitri Alperovitch on Twitter: \"*NEW* @GeopolDecanted episode: I talk with one of the smartest thinkers on AI policy and tech developments (former WH and DeepMind) about the profound positive and negative military and societal developments we might experience soon (and those we won‚Äôt)üßµ https://t.co/23ErIoRIsk\" / Twitter\n",
      "https://twitter.com/adversariel/status/1650313930802368512 | (1) Ariel on Twitter: \"There‚Äôs a lot of fearmongering about LLMs being capable of finding 0day There are three highly complex roadblocks that need to be overcome for this to be a real concern: statefulness, hallucination, and contamination https://t.co/ZZq4OPrglb\" / Twitter\n",
      "https://twitter.com/emollick/status/1652170706312896512 | Ethan Mollick on Twitter: \"This ü§Ø is a very big ü§Ø I have access to the new GPT Code Interpreter. I uploaded an XLS file, no context: \"Can you do visualizations &amp; descriptive analyses to help me understand the data? \"Can you try regressions and look for patterns?\" \"Can you run regression diagnostics?\" https://t.co/s3CV5nQtl3\" / Twitter\n",
      "https://twitter.com/WilliamAEden/status/1630690003830599680 | William Eden on Twitter: \"My Twitter timeline is full of panicked takes about imminent AI apocalypse and certain doom. I think this is starting to get overplayed, and so I want to make a long thread about why I'm personally not worried yet. Get ready for a big one... 1/n\" / Twitter\n",
      "https://twitter.com/jason_seba/status/1667679950734864385 | Jason Seba on Twitter: \"@peterwildeford Hahahahha\" / Twitter\n",
      "https://twitter.com/JgaltTweets/status/1666751165168508930 | JgaltTweets on Twitter: \"Press release on the UK's proposed Global Summit on AI Safety to be held later this year: https://t.co/LsHhtVVylX https://t.co/5k7Fs2Ai1v\" / Twitter\n"
     ]
    }
   ],
   "source": [
    "twitter_tabs = sorted([t for t in tabs if 'twitter.com' in t.lower() and 'messages' not in t.lower()])\n",
    "print_tabs(twitter_tabs, label='Twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e8d623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open_tabs(twitter_tabs, page=1, per_page=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4635d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Google Docs ## (121 tabs)\n",
      "\n",
      "https://docs.google.com/spreadsheets/d/1waiXbSXZs54_plxa7u9sRQTxMbNaEwbve0sBTGT5BvY/edit#gid=0 | GLT current guesses re asks from SP -- April 2023 - Google Sheets\n",
      "https://docs.google.com/document/d/1qiQDQKSUDTvyurVzWT-Vn6RmJ24a4Emol8pGO1-ZIb0/edit | https://docs.google.com/document/d/1qiQDQKSUDTvyurVzWT-Vn6RmJ24a4Emol8pGO1-ZIb0/edit\n",
      "https://docs.google.com/document/d/1kX4RVoWGicYug0Dr0Rt5sLfobHbjMLABlcY-dsTOMqg/edit# | Cascading conditional probabilities show transformative AGI by 2043 is <1% likely\n",
      "https://docs.google.com/document/d/10pSj7Jb68sPO0bQyw7cMswsMtx1A7tOGPxy3JbrLC8I/edit#heading=h.6rnrfpst2h9p | Thoughts on founder support preparation - Google Docs\n",
      "https://docs.google.com/spreadsheets/d/1ALNFDZDda9aKGOzW3SgwbJJH4rgkwSmlXWuUtKmNhAc/edit#gid=1888482782 | PTO Report Effective Jan 1, 2023 - Managers - Google Sheets\n",
      "https://docs.google.com/document/d/1jtX74U03k3_tzvqAc0sTJaYpwx3OI72qrRPXz9r6DGE/edit#heading=h.1lhw3y6kfeo8 | Jam‚Äôs proposal for founder search and support - Google Docs\n",
      "https://docs.google.com/document/d/1rvuzMKK3ap7ODD6vWAnZq4RuPberN-d-WHzAYvqO3FU/edit | [RP-internal copy] Bid: build a lobbying apparatus for AI regulations, including for big asks that aren't yet feasible - Google Docs\n",
      "https://docs.google.com/document/d/1uATkMdi5xIH9TeHdm-f5syiJHMkiW1EDnpTwGAbTrOc/edit#heading=h.sw94eg3x8iz8 | LT department meetings_2023 - Google Docs\n",
      "https://docs.google.com/document/d/1OmKOmMfbmBnjxGGHetfN2VQ1az4Zox0Y-4f5xTlRzhw/edit#heading=h.g2xot74rmmug | Maybe let‚Äôs focus more on non-extinction ways that a lot of the potential value of the future could be lost? [quick notes]\n",
      "https://docs.google.com/document/d/1QsJ8PNqfvvdtkMHclNLqtVP2SVM5dhsj4GMeFztjGBY/edit#heading=h.w6y052tqvke3 | Exploring future AI compute paradigms - Google Docs\n",
      "https://docs.google.com/document/d/1A-W3ahsV3DspgnEk7iRXlyctkL0D0kwgP09OGFRu5BI/edit#heading=h.mtpqcbgdzbmj | [Public] Examining pathways from narrow AI to nuclear war - Google Docs\n",
      "https://docs.google.com/document/d/1NfyHOxI7AW0apsyrkW9Eqv_LSLLR9_vlmVmSa7QlxZA/edit#heading=h.ohimieuzefxf | Info on AI lab boards - Google Docs\n",
      "https://docs.google.com/document/d/1usMVjv7hMx22K-eu71o1bcfj-7JH_l-aXbfYb6NHSoM/edit | Threat Model for Existential Risks from AI - Google Docs\n",
      "https://docs.google.com/document/d/1z3YrMwEcdNGt2X2GoFNIVhbcBZxzsBFiSz4_q6vJXO8/edit#heading=h.n3opm8r5uhlr | Center for Long Term Priorities Update: April 2023 (shared) - Google Docs\n",
      "https://docs.google.com/document/d/14ou5ob0SPa52boGDrPYZ4Oj_Gje0dCJOV_8l7mftK9o/edit#heading=h.htupmo9y9du4 | EAG London 2023 - Renan Notes - Google Docs\n",
      "https://docs.google.com/document/d/1tN6pmDqxlwBjzwp5n_3pqii9EHsDJqCloiNtGDXyfYE/edit#heading=h.tnew02vlmfya | Theories of victory in AI governance: relevant readings, people, & notes - Google Docs\n",
      "https://docs.google.com/document/d/1HeuDspWp4VRyWNS5IKOxqZWZoCTpU8k3LU4X3adpVFw/edit#heading=h.zee6ngwoj6jg | RP <> DeepMind May 17, 2023 - Google Docs\n",
      "https://docs.google.com/document/d/1idfbvEpsxrFTGflCErTPZ_NiXjeqPhfwBrJBce1P_Yw/edit#heading=h.mj0jmgv3ic64 | Will Humanity Choose Its Future? v6 - Google Docs\n",
      "https://docs.google.com/presentation/d/1jLK9tjEH7Mxqx1fz0IPDzKJvcoLJJVo4jpZuZ42j-rA/edit#slide=id.g213a7bae0d5_0_0 | GHD OKRs\n",
      "https://docs.google.com/presentation/d/1dal9XJTgni7TfqMw2OylVwsMWrfz_FaYjPOdcoVl10o/edit#slide=id.g232c77bbbb5_0_5 | Copy of Org-Wide OKRs Presentation - Google Slides\n",
      "https://docs.google.com/document/d/18sXLBNMsLwKXGqFVXESHNBDdK-z7SIu2BH_EYrSw7sY/edit | Project Description [Current] 03/28/23 - Google Docs\n",
      "https://docs.google.com/document/d/1XfwR6Tc0fFKDUoq8yjjG2m1g_5nopCkRGUCwc-fLSL4/edit | Untitled document - Google Docs\n",
      "https://docs.google.com/document/d/1R-H-1old8f9NwsyKNK4d8vEur8350Ju9dDrKIz1dx2o/edit#heading=h.pkj0s0mloy5v | Rough notes on \"crunch time\": definitions, related concepts, prior work\n",
      "https://docs.google.com/document/d/1sMH8fibfO602ZsttAjjxq4bBld5xn5UwuIraIInwTzs/edit#heading=h.z1512e3souac | (Forum copy) Post-FTX Public Awareness / Attitudes - Google Docs\n",
      "https://docs.google.com/document/d/1eO_-UjygEZOsQfMtRUv-12FnN0oGVNQi_gr6qNU3a5U/edit | Lab Governance Workstream - 2-Pager (External) - Google Docs\n",
      "https://docs.google.com/document/d/1zsKIgyLjBitm2V-fmJaEPODf8lCYDLOrCuXfewO1HhU/edit | Biosecurity 5pger - Google Docs\n",
      "https://docs.google.com/document/d/1wd7WEsaPXQB_IauqXEcE1RIyKmvrjC3tVrz6B0KXxeo/edit | Value of the Future After Perils\n",
      "https://docs.google.com/document/d/1bY5cKyw6PhsmcvJuTWym1jEeHEo0xZqz8B_qhthwcBE/edit | EV of the Future and Counterfactual Credit (New Version) - Google Docs\n",
      "https://docs.google.com/document/d/1Gkju5VWLldE4COF278hLeWjsVQPHtdgYncCaFeNYcIw/edit | How the Strong-LT Model Works, What it Says, and Whether We Should Trust It\n",
      "https://docs.google.com/document/d/1QplktgJzt2Njaizu8TI4thjtetc5EzOLknYEubl7ZHg/edit#heading=h.icjgkbth67i3 | *NEWEST* Copy of Kieran's meeting with Co-CEO(s) Running Agenda\n",
      "https://docs.google.com/document/d/14giRxnDwnCOMI_HTIHnbwU66XiX51XHYxO8aQfMUXc0/edit#heading=h.ilkan3e0drym | Jared Brown <> Michael Aird - 2023-Apr-13 - US AI policy, lobbying, Global Shield - Google Docs\n",
      "https://docs.google.com/document/d/100MHVmj9XyTFEfcimj97_S2ri4mDNNf5qy7RJWnA5tI/edit# | Cost effectiveness of learning and applying epistemic methods - project description - Google Docs\n",
      "https://docs.google.com/document/d/19qoI35NZzkvNghinKZh1ad0shLH-zjEL2rW_xynS16k/edit#heading=h.8ekeme61qpqb | Ashwin's timelines hot takes: PATCH-like scenarios - Google Docs\n",
      "https://docs.google.com/document/d/1vouv73_c8L05fHDh8x3A_HSBukclMbIb4JE7O2UdFLY/edit#heading=h.x1uc7lfftp0 | AIGS 2023 OKRs [as of April 2023] - Google Docs\n",
      "https://docs.google.com/document/d/19L0k0B0-0gW7t96Q-hpNIknCEry57Hklgt2FXFDUH78/edit#heading=h.mak2h8fgpqe9 | [SES copy] Misuse of AI should be a core priority in AI risk reduction - Google Docs\n",
      "https://docs.google.com/document/u/2/d/1fOCLx9srgcK3-oEKteAu-vTMAyXDEDPGyuIW4tcY6IA/edit | 2023 Altruistic Policy Telecon Agenda\n",
      "https://docs.google.com/document/d/1mhWNWjBudqdRFSFdKCHhjV3Ys-fLGmb2Z8bQpusEUx4/edit#heading=h.39qpdbcr4hq2 | Merge-and-Assist (Lab Gov ‚Üí US Regs) - Google Docs\n",
      "https://docs.google.com/document/d/1iV1OoAYbQjhaMJOV3aTfC2SfR2T5gkY8w2Gg4ESLjaM/edit#heading=h.osty8jeclpyn | 2023 AW Dept Retreat - Agenda [final] - Google Docs\n",
      "https://docs.google.com/document/d/1ghEgQeMA56UAffquWhlnJNseNh8NdMLA4NFuTdDsiiU/edit#heading=h.n27z5n7sidxc | Sleepwalking into Survival\n",
      "https://docs.google.com/document/d/1LyQKNesRensq3BjLWgzcdJ3wJBjl0n98fAQ7g7HRbYk/edit#heading=h.koq63kgkerx2 | Prioritarianism and Resource Allocation\n",
      "https://docs.google.com/document/d/1ZYfKFjzOeFiaK86U0np0W2XcpjUIdjJ51Vq7hxntht8/edit#heading=h.6pw5bytuj5u7 | (Forum copy) RP Campus Awareness Survey - Google Docs\n",
      "https://docs.google.com/spreadsheets/d/1hcYteAFXujvTI3KlzUf0FL_du5jwu6cuLPEmPGJ0X5U/edit#gid=0 | Defense in Depth: Matrix of Layers\n",
      "https://docs.google.com/document/d/12oQImZrUFiEgJzyG_1xGrulyEYL7UFckJJMKPDVnr9Y/edit | Forecasting Twitter list\n",
      "https://docs.google.com/presentation/d/1iuYIHlHvsnvOUoRX__BCc5J2A0cSieJlENSAyT_uFg8/edit#slide=id.ge44d99aa4e_0_0 | Survey OKRs presentation - Google Slides\n",
      "https://docs.google.com/document/d/1JataZjU6aIon_tB1_dqMp7lXzPQYT7Uqu5m5DKMbdb4/edit#heading=h.mfc0g6vdbaom | Evals, safe scaling, & related policy/regulation: relevant readings, people, & notes - Google Docs\n",
      "https://docs.google.com/document/d/1k3oRN5MSKVvVZ9CVhngQCAYmLCPacwG-9ly6loYlgE8/edit#heading=h.epbjbqd6jzw | Notes - Forecasts + disagreements on Big Strategic Variables - Google Docs\n",
      "https://docs.google.com/spreadsheets/d/1P8mkLmPOrTNnVH4lOOoPebtlHIp1Pi3lg8bA8NUqaTo/edit#gid=0 | GHW asks - Google Sheets\n",
      "https://docs.google.com/document/d/1n5i1-VmV8IRDHTybXh70yV-mZB8RpMuu9ZXaDwLGRRs/edit | EAFo/LW Reading List - Google Docs\n",
      "https://docs.google.com/document/d/1SbGV0Nc-Nh6WYkTNQ7QUxnbi9kRw0XsMwSqmBk0U0eM/edit | [Private] EAIF Vision and Scope - Google Docs\n",
      "https://docs.google.com/document/d/1WEl6K9qmd9_6kuVw6UyRroFsxXbfFIYd_hTpZbrZlZs/edit# | Pivotal act: Definition, examples, & reading list\n",
      "https://docs.google.com/document/d/18aOKioNfnAfYgEX439P2Cq9Ocv_C1I6Jkjxkgowslus/edit#heading=h.6ytgvt3dfnpe | 2023: May - RP Performance Evaluation for Ben Snodin - Self Eval - Google Docs\n",
      "https://docs.google.com/document/d/1sZ7N2pOjFaluiZ6roUx90IH2t1rvVX9AjHbfK5qeEoM/edit | Notes EAG neartermist - Google Docs\n",
      "https://docs.google.com/document/d/1cPyHo7Xym0RaDVI55bIKgqQJhztcYV_Bj77fO_i5VZw/edit#heading=h.grts0kyn5j76 | X-Risk in the Pre-AGI Transition Period\n",
      "https://docs.google.com/document/d/1NA06JZz1gBLa9O4N3_1tlTvBLWy6X19Z--2gzQ_qkdk/edit#heading=h.1cytsywlk7ba | How might the US national security sphere orient & react to increasingly powerful AI? - Google Docs\n",
      "https://docs.google.com/document/d/1zxQGQfDeD7uTnUoJ2_L47jKWvSoancgtEsH4Zd6NAt4/edit#heading=h.ow22kk7tct5w | XST retreat 2023 ‚Äì Schedule and Discussion notes - Google Docs\n",
      "https://docs.google.com/document/d/1qwKUWu1aa1rikW3zlCPPvPXdS4upsarkJe8-JwcE4tY/edit#heading=h.z6v8ty7j4wlh | You don't have to be that risk-averse to prefer GHW to LT (Final - Shared with Jason) - Google Docs\n",
      "https://docs.google.com/document/d/1w3YEAY6yzqYOIjK5BRhnWbwTN5iV3OOtHf0R67uxecg/edit | Things to say to Caro\n",
      "https://docs.google.com/spreadsheets/d/198CVP8UQow6XtocRlIx_HQOndPQq6a2BqJxfkeeZNBk/edit#gid=460115521 | 2023.06 EA Funds, AW - RP - Google Sheets\n",
      "https://docs.google.com/document/d/1-YmmnXkzaQA2AydAUPKVGHFbjDLCe2qck09IpmNBqcM/edit#heading=h.osty8jeclpyn | Rethink Priorities ‚Äî Insect Farming and Welfare Strategy - Google Docs\n",
      "https://docs.google.com/document/d/1kpdCPU2I0NLWPRwQ4qPSUZcZDobnyR9UY-iSOSBQRm4/edit#heading=h.natf23qmy2fx | Scaling laws literature review - Google Docs\n",
      "https://docs.google.com/document/d/1UvQU-0T-JhfG4u2_Zs2VibDAgN6_PMIOx4zAwy0VG1o/edit# | [Widely shared] How might the US national security sphere orient & react to increasingly powerful AI? - Google Docs\n",
      "https://docs.google.com/presentation/d/1KW7ZkuCdXAC8FEfC6r3w15QQQTn4jUyjFSX24WzCte8/edit#slide=id.g232fae25e24_0_7 | 2023 AW Department OKRs - Google Slides\n",
      "https://docs.google.com/document/d/1xUvMKRkEOJQcc6V7VJqcLLGAJ2SsdZno0jTIUb61D8k/edit#heading=h.uskcgipunmm1 | Welfare Range and P(Sentience) Distributions - Google Docs\n",
      "https://docs.google.com/document/d/1fqSeu0YL223ngmSCvkuhsJ5zCKBOEzxhyfSEkUPfUq8/edit# | AI threat model reading list [crappy first attempt]\n",
      "https://docs.google.com/document/d/1KJx_GhV3A8c2leu4hisMDO7sciY49p8BygQAfbJ1mXw/edit#heading=h.tjvfcbqz2mvz | Founder search leads list - Google Docs\n",
      "https://docs.google.com/document/d/1SS8XmzHIaS_q7AcPNxKLbHK2kOa3PyPkXMS1Az1cQaM/edit#heading=h.nb2tkwr1m161 | Tradeoff project draft - Google Docs\n",
      "https://docs.google.com/document/d/1FMJZ9wjxooB-7HD7K7c-kEasF1qYuXMfq4wPrFioq_Q/edit#heading=h.mdmelsvomij2 | Info on how GovAI does hiring (Georg Arndt <> Michael Aird notes, 2023-May-15) - Google Docs\n",
      "https://docs.google.com/document/d/1QFTYRSJPyQoKfO_bFrsxG861xebXbX3XzvBEEaT7Z1U/edit#heading=h.mj6mty720ju | [V.3] What is going to matter if an AI crash project emerges?\n",
      "https://docs.google.com/document/d/1jH2UpXhi6uFF9nU6PZwbEurNArW5Zi5fPba-uM0MVPE/edit#heading=h.deq8lzwofh50 | Final Draft Report - CEA Animal Ballot Initiatives - Google Docs\n",
      "https://docs.google.com/document/d/1CpO25iV38hXESPRQZmv15bLjBB_hrAYgAkvfHoJbngE/edit#heading=h.j9owozbw0x7p | Layer - Safety Culture - Google Docs\n",
      "https://docs.google.com/document/d/14dDtyEAh7ealQGfVG8xBaWxcQcyT5sdCNTEDj9blTo8/edit | WIT Possible Projects Apr 2023\n",
      "https://docs.google.com/document/d/1HsUiJ9AMacQTk98ImDKyS660EbYHNbZzmNKlXC0xF1s/edit#heading=h.oyy6uniuf2wi | Community building in a world where people actually listen to us - Google Docs\n",
      "https://docs.google.com/spreadsheets/d/1vLsL0QRtF7z9B4Jn5nu0xXUQXyZA0y4ej98UptRWNDU/edit#gid=0 | Name longlist - AIGS rebranding\n",
      "https://docs.google.com/document/d/16aEouFa8470SCYgTNEWEqhJQQw-daaMx4Q2LLf-8W5E/edit#heading=h.6ytgvt3dfnpe | Zoe Williams - May 2023 - RP Performance Evaluation - Google Docs\n",
      "https://drive.google.com/file/d/1dK81w7xuHVZgz7yAw4DPYglTuRHbWIlF/view | Intermediate Report on Diabetes Mellitus Type 2 (Public, PDF).pdf - Google Drive\n",
      "https://docs.google.com/document/d/1FlGPHU3UtBRj4mBPkEZyBQmAuZXnyvHU-yaH-TiNt8w/edit | Garfinkel Review of JC Alignment Report - Google Docs\n",
      "https://docs.google.com/document/d/15FIf6-pvc0Nc2ItFWz5Qk19F-Dg1MPRilB21cXWDjHU/edit#heading=h.drzlnsxrz21q | How can SP be involved in founder support? - Google Docs\n",
      "https://docs.google.com/document/d/1TsHZ3YXvz4Rs_rBihugjqS7gPDhxBq96cXu7JoJOYxs/edit#heading=h.js018c8h01q3 | Notes from lunch convo w/ Michael Aird re: XST AI upskilling [5/6/23] - Google Docs\n",
      "https://docs.google.com/document/d/1ziNrskp-v_jWihUakPIhSLqdu6WJY-mA0152RUcLqQc/edit#heading=h.agcon1r7et1w | AIGS Leads notes (Michael, Peter, Zoe, often Ashwin) - 2023 May-Sep\n",
      "https://docs.google.com/document/d/1srRr2sudZnIdRR0jMTKHt7OuP9msGV11ksWN0o9-VSo/edit#heading=h.tnew02vlmfya | Technical AI work to prevent catastrophic misuse: relevant readings, people, & notes\n",
      "https://docs.google.com/document/d/1Weh2vqYRT-l1SpuufyZ4_ldNoOuIg8QodpNskkYG04U/edit#heading=h.81xq1jfr7jcz | Backgrounder on the US natsec community‚Äôs relationship to AI\n",
      "https://docs.google.com/document/d/1JTHziStX0dFjFWa2Gp8RYfKXJJM69nvAB0mGtCUpgdw/edit#heading=h.j9owozbw0x7p | Layer - Isolation of Digital Systems - Google Docs\n",
      "https://docs.google.com/document/d/1BP7hQkpAd5fsrI4-Y6qgJvLriTyPUVJcd6R0_JF1q2w/edit#heading=h.osty8jeclpyn | [0] Whistleblowing Report v9 - Google Docs\n",
      "https://docs.google.com/document/d/18brxFBSZBN6bnZM6RH1viKJFNcRv7r-4S1ryJkD0KfM/edit#heading=h.y0srbz710jxs | Call notes: Jam-Jack Goodman [8 June 2023] Topic: US Lobbying 101\n",
      "https://docs.google.com/presentation/d/1iYnnK3TwNBugTCeeRVyL8MJhSb92ZTUNO-7y5Iwyyvs/edit#slide=id.g232b11b7cba_0_5 | GLT OKRs 2023 -- RP All Staff Meeting 2023-04-20 - Google Slides\n",
      "https://docs.google.com/document/d/1EZ1TgTnJ6zt_-JuhboXJwEDZ7r4R2D8WnOzLE7DphUg/edit | What would it look like to regulate AI like bio and nuclear? - Google Docs\n",
      "https://docs.google.com/document/d/1fqTkdMvXL1Qp1PGvHNWop8tNR9jSKUTZWWdc6HTYTwM/edit#heading=h.osty8jeclpyn | Copy of 2023 Strategic Planning: SWOT Brainstorm Document - Google Docs\n",
      "https://docs.google.com/document/d/1-NRtsVxqE4LnL_gYn0wDRbfH2RvoDthzp-j_9mUazOI/edit#heading=h.3fjkh1axp1du | Three proposed strategies for AGI endgames\n",
      "https://docs.google.com/document/d/12A4_adulGL1NPE7Xd08hUfe3spLFKfcRwXyD28FRt5o/edit | RP Copy of How I think about TAI deployment scenario analysis - Google Docs\n",
      "https://docs.google.com/document/d/1lC-rIXME-GD1AImZ80b9eP61sroZy8mooLnSeHNgYzM/edit#heading=h.ftvusubre6rz | Brainstorming on RP as a brand - Google Docs\n",
      "https://docs.google.com/spreadsheets/d/1TKdLTOeDfBjXUoUAJdPh40Z13bmYf5hvuTUTJH0m41c/edit#gid=2031513321 | [confidential] Staff Overallocation - Google Sheets\n",
      "https://docs.google.com/document/d/1xFlAx71HEjIHQI36r8gP2Dg0SdI3sz9lLnm5KPw0kno/edit#heading=h.fmkwnd6gv8xf | AI risk from program search\n",
      "https://docs.google.com/document/d/18mSdRCL0l0ApuOYU6oiC7ImzLjFbuetehqTLkw6PP-o/edit#heading=h.qvx51emxxmz4 | Splitting up EA\n",
      "https://docs.google.com/document/d/1fLL5BZf8VdhdEQ9uNDne6mp8sNTmgUJSOJ8LQxaZVFI/edit#heading=h.ahaokbxu01um | How bad/good would shorter AI timelines be? Why? [writeup idea + notes + reading list]\n",
      "https://docs.google.com/document/d/1UsRIgm1mFUJNr5UNOmx7j4Q4-lCeVn7OC3euP0kLDnU/edit#heading=h.ni0hbuwqtlub | Hot AI Governance Collective Upskilling (HAICU) ‚Äì Central Doc - Google Docs\n",
      "https://docs.google.com/document/d/11YKTKRumtlheK_9Dv9ECKwwoTeSG3RNcs6qUSajzqDw/edit | 2023.05.22 AI Reference Classes - Google Docs\n",
      "https://docs.google.com/document/d/1EUFZ9sOJxcUpGY3PEaMBloGkAl9nGpmcHgmaZdOGwy4/edit | 2023 AW Dept Retreat - Moral weight implications - Google Docs\n",
      "https://docs.google.com/document/d/1s4zVmg6c6l0Dv6lZOMlTeuJGKDyCFK8iOitzTGA8rgI/edit#heading=h.cn0nfi8o81o1 | Herding Alpacas\n",
      "https://docs.google.com/document/d/16r-PIQCIZSKGgahwJUK0kuTjVY2vLoIQGcsEeuJDfZc/edit | LT Retreat May 2023 - Lab Governance Workstream - Google Docs\n",
      "https://docs.google.com/document/d/1vm-ZwU1XS0j_2WN_IB_QH8CeDfmlHGkKdk5EQyc37JA/edit | EA AWF Application 2023-06 - Google Docs\n",
      "https://drive.google.com/file/d/1-W5vx__PxZY4IEqWkQ0BqQw5hi3133Pu/view | Delay, detect, defend\n",
      "https://docs.google.com/document/d/1tMcC_b18ZDowxIhrSqsHfg4S2X02JF2ui3W4P62d6Lg/edit#heading=h.livlmiwiaubo | Report on Data Centers - TAIGA Version 2023-04-20 - Google Docs\n",
      "https://docs.google.com/document/d/1X8Rq7LYH40Gz5oFLf1zZzwr0pwdB69MuR2fNDlg13KE/edit | Are we prepared for the September hiring round? - Google Docs\n",
      "https://docs.google.com/presentation/d/1HLj_1v7Hnr8xO0qqfSqucsKbCz7s2fTzsP7gpqT7TA8/edit#slide=id.p | EAG London Talk (Ben Garfinkel) - Google Slides\n",
      "https://docs.google.com/presentation/d/19d8DDka9xErkDEHqEhFz152AjOjFLs4tIbZB9vNuBmY/edit#slide=id.ge44d99aa4e_0_0 | WIT OKRs for April 2023 All Staff Meeting - Google Slides\n",
      "https://docs.google.com/spreadsheets/d/1_l-mRnuZJckKFXGmbz0m9vPpcVc7w9PMW-8aR1ppYHg/edit#gid=1705484556 | XST June+July timetable, June 2023 - Google Sheets\n",
      "https://docs.google.com/document/d/19pSjxtP00UI3lahc6qYEz6n4e8yBWUBamsdOasIEzh4/edit#heading=h.f3k7ubt3yxsm | Ozzie Gooen <> Michael Aird - 2023 - advice on AIGS strategy - Google Docs\n",
      "https://docs.google.com/document/d/1i5qQteGLrvokces6rEWTUfY3khe-LJMOIobrDREcd2w/edit | Peter's takes on some big strategic variables - Google Docs\n",
      "https://docs.google.com/document/d/1wJCLEHAR34joF-cIGxAwCMQm8-D-mlP6LPBoMboFdHk/edit | A survey of concrete risks derived from Artificial Intelligence - Google Docs\n",
      "https://docs.google.com/document/d/1dwr2qpaWdCqr_IDhcTT69TmEA5aWfiNftasn5iJ_qhA/edit | Premises to get to Strong LT - Google Docs\n",
      "https://docs.google.com/document/d/1ZxDbMPOUKzwd8rP53Ou3VVWEJhVSyIwnT5p2eFIcxuk/edit#heading=h.rehbguf5xp7c | Updates to XST strategy and culture after the Wytham Abbey retreat, June 2023 - Google Docs\n",
      "https://docs.google.com/document/d/1qxc_XDErDFeQGsYE52vLi1lIJIRL5VL9i1Hi-Btj9Mg/edit# | Info on recent/upcoming AI policy happenings, from May 2023 coordination call\n",
      "https://docs.google.com/document/d/1ikmEY9bW6BpkqF-D9feWYnTPx0yG-v1HDUcPsmMSduc/edit#heading=h.j9owozbw0x7p | Layer - Requirement Specification and Tracing - Google Docs\n",
      "https://docs.google.com/document/d/136FNAeBw7oKyv8lUZm8qFEsVM8tQUaQzgDrCtLTf4Fs/edit#heading=h.wsiggdpisp4j | Some hot takes on the implementation of transformative AI systems - Google Docs\n",
      "https://docs.google.com/document/d/14T_RBzlfBTn4IOdxqG0fh6ilmGOxqGWtyKQZPEiHbqc/edit#heading=h.e6lo2rv2yae9 | Dendritic concept note - Google Docs\n",
      "https://docs.google.com/document/d/1lbazshEDUNzT-5gBYqfrmC9hvKRIxSbXroHywxPctw4/edit | [shared] Slowing AI - Google Docs\n",
      "https://docs.google.com/document/d/1ddkN8tmeiGVe7v-_77zV4RgP2taIo6ee49TITqi2Xhs/edit# | XST strategy meetings ‚Äì 2023 Q2-Q3 - Google Docs\n",
      "https://docs.google.com/document/d/1gd6qQx-SP6rfAVQE5rzfPH0zXYOOHXBa13JSUd2zROQ/edit | Daniel's Randomly Generated Future: Hardware Accelerates - Google Docs\n",
      "https://docs.google.com/document/d/1WSyIfis0vc5pEBI8RpWp7o0tnTHJAS5OeTWgY_H2xdM/edit#heading=h.ors7j470u62z | AI: Thinking Out Loud (WIP) - Google Docs\n",
      "https://docs.google.com/spreadsheets/d/1JFzYDU8tJ_BB5ZHy_LA98r2cXXlmAaGqI95EE41DORM/edit#gid=842085141 | RP Risk Register\n",
      "https://docs.google.com/document/d/14Sui9HRpEer8TC02vqXtq_fkhv0jQhws8PW-TMUxlGE/edit#heading=h.q6vp1h1iv66u | Marcus + Peter Super Wednesday Agenda - Google Docs\n"
     ]
    }
   ],
   "source": [
    "doc_tabs = sorted([t for t in tabs if ('docs.google' in t.lower() or 'sheets.google' in t.lower() or 'drive.google' in t.lower())])\n",
    "print_tabs(doc_tabs, label='Google Docs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18e9e150",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open_tabs(doc_tabs, page=1, per_page=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "479f45a0-d63f-474d-aa76-2ddab5a10b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_tabs_ = copy(doc_tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4e71249-4e0e-47aa-9fcd-69b9219f507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_tabs_ = open_random_n_tabs(doc_tabs_, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6311fa07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Google search ## (3 tabs)\n",
      "\n",
      "https://www.google.com/search?gs_ssp=eJzj4tVP1zc0TDYtLjHNMyk3YPTiz0ktUUjNVcjMUyjPzEsvBgCbmwoM&q=let+em+in+wings&rlz=1CDGOYI_enUS715US715&oq=let+em+in+win&gs_lcrp=EgZjaHJvbWUqBwgBEC4YgAQyCggAEAAY4wIYgAQyBwgBEC4YgAQyBggCEEUYOTIHCAMQABiABDIHCAQQABiABDIHCAUQABiABDIHCAYQABiABDIICAcQABgWGB4yCAgIEAAYFhgeMggICRAAGBYYHtIBCDQ4MDRqMGo3qAIAsAIA&hl=en-US&sourceid=chrome-mobile&ie=UTF-8 | let em in wings - Google Search\n",
      "https://www.google.com/search?q=federally+funded+ffrdc&rlz=1CDGOYI_enUS715US715&oq=federally+funded+ffrdc&aqs=chrome..69i57j0i546l2.5365j1j7&hl=en-US&sourceid=chrome-mobile&ie=UTF-8 | federally funded ffrdc - Google Search\n",
      "https://www.google.com/search?gs_ssp=eJzj4tVP1zc0zDM2rEo3t6wwYPQSK8hJrCxWKE_NyVEozyzJUMgvyUgtKgYA7bgM-Q&q=plays+well+with+others&rlz=1C5CHFA_enUS925US925&oq=plays+well+with+&aqs=chrome.1.0i512j46i340i512l2j69i57j0i512l6.956070j0j1&sourceid=chrome&ie=UTF-8 | plays well with others - Google Search\n"
     ]
    }
   ],
   "source": [
    "print_tabs(sorted([t for t in tabs if ('google.com' in t.lower() and 'search' in t.lower() and\n",
    "                                   not ('docs.google' in t.lower() or 'sheets.google' in t.lower()))]),\n",
    "           label='Google search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53b9762e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## EAFo/LW ## (91 tabs)\n",
      "\n",
      "https://forum.effectivealtruism.org/posts/cPkfCviK5cAsevTdM/the-charity-entrepreneurship-top-ideas-new-charity | The Charity Entrepreneurship top ideas new charity prediction market - EA Forum\n",
      "https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is | (My understanding of) What Everyone in Technical Alignment is Doing and Why\n",
      "https://forum.effectivealtruism.org/posts/KRSthwicCTRw9Ayzg/large-epistemological-concerns-i-should-maybe-have-about-ea | Large epistemological concerns I should maybe have about EA a priori - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/DZEkYatZeMSbGBAjk/why-are-we-so-complacent-about-ai-hell-1 | Why aren‚Äôt more of us working to prevent AI hell? - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/7hMgK4hciBhXmBRnW/do-you-think-decreasing-the-consumption-of-animals-is-good | Do you think decreasing the consumption of animals is good/bad? Think again? - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/Z7r83zrSXcis6ymKo/dissolving-ai-risk-parameter-uncertainty-in-ai-future | ‚ÄòDissolving‚Äô AI Risk ‚Äì Parameter Uncertainty in AI Future Forecasting - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/fsaogRokXxby6LFd7/a-compute-based-framework-for-thinking-about-the-future-of?utm_source=EA+Forum+Digest&utm_campaign=05c2857556-EMAIL_CAMPAIGN_2023_06_07_11_02&utm_medium=email&utm_term=0_7457c7ff3e-91f59af93e-%5BLIST_EMAIL_ID%5D | A compute-based framework for thinking about the future of AI\n",
      "https://forum.effectivealtruism.org/posts/gt6fPgRdEHJSLGd3N/thoughts-on-the-openai-alignment-plan-will-ai-research | Thoughts on the OpenAI alignment plan: will AI research assistants be net-positive for AI existential risk?\n",
      "https://forum.effectivealtruism.org/posts/ARkbWch5RMsj6xP5p/transformative-agi-by-2043-is-less-than-1-likely?commentId=B2oW9r5LjiWotEK2a&utm_source=EA+Forum+Digest&utm_campaign=05c2857556-EMAIL_CAMPAIGN_2023_06_07_11_02&utm_medium=email&utm_term=0_7457c7ff3e-91f59af93e-%5BLIST_EMAIL_ID%5D | https://forum.effectivealtruism.org/posts/ARkbWch5RMsj6xP5p/transformative-agi-by-2043-is-less-than-1-likely?commentId=B2oW9r5LjiWotEK2a&utm_source=EA+Forum+Digest&utm_campaign=05c2857556-EMAIL_CAMPAIGN_2023_06_07_11_02&utm_medium=email&utm_term=0_7457c7ff3e-91f59af93e-%5BLIST_EMAIL_ID%5D\n",
      "https://forum.effectivealtruism.org/posts/8YXFaM9yHbhiJTPqp/agi-rising-why-we-are-in-a-new-era-of-acute-risk-and | AGI rising: why we are in a new era of acute risk and increasing public awareness, and what to do now - EA Forum\n",
      "https://www.lesswrong.com/posts/k2SNji3jXaLGhBeYP/extrapolating-gpt-n-performance | Extrapolating GPT-N performance - LessWrong\n",
      "https://www.lesswrong.com/posts/ejxwraMP5ye7Bgmpm/things-i-learned-by-spending-five-thousand-hours-in-non-ea | Things I Learned by Spending Five Thousand Hours In Non-EA Charities - LessWrong\n",
      "https://forum.effectivealtruism.org/posts/eK8sEq7Djxp3NqxLQ/tyler-cowen-s-challenge-to-develop-an-actual-mathematical | Tyler Cowen's challenge to develop an 'actual mathematical model' for AI X-Risk - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/icdd4FCKuwqyAuYBm/eli-s-review-of-is-power-seeking-ai-an-existential-risk | Eli's review of \"Is power-seeking AI an existential risk?\"\n",
      "https://forum.effectivealtruism.org/posts/H5beCesFybASmwhcM/sam-clarke-s-shortform | Sam Clarke's Shortform - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/zoWypGfXLmYsDFivk/counterarguments-to-the-basic-ai-risk-case | Counterarguments to the basic AI risk case\n",
      "https://www.lesswrong.com/posts/JcgtKunqmELefxksx/killing-socrates | Killing Socrates - LessWrong\n",
      "https://forum.effectivealtruism.org/posts/8CM9vZ2nnQsWJNsHx/existential-risk-from-ai-survey-results | \"Existential risk from AI\" survey results\n",
      "https://forum.effectivealtruism.org/posts/P98Pas4cirMQp3cJy/clarifying-and-predicting-agi | Clarifying and predicting AGI - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/TCsanzwKGqfBBTye9/the-wild-and-wacky-claims-of-karnofsky-s-most-important | The 'Wild' and 'Wacky' Claims of Karnofsky‚Äôs ‚ÄòMost Important Century‚Äô - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/CAC8zn292C9T5aopw/community-health-and-special-projects-updates-and-contacting-1 | Community Health & Special Projects: Updates and Contacting Us - EA Forum\n",
      "https://www.lesswrong.com/posts/QzkTfj4HGpLEdNjXX/an-artificially-structured-argument-for-expecting-agi-ruin | An artificially structured argument for expecting AGI ruin - LessWrong\n",
      "https://forum.effectivealtruism.org/posts/J4cLuxvAwnKNQxwxj/how-does-ai-progress-affect-other-ea-cause-areas?commentId=JwxP9T5Mc3ekzHpXh | How does AI progress affect other EA cause areas? - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/L6ZmggEJw8ri4KB8X/my-highly-personal-skepticism-braindump-on-existential-risk | My highly personal skepticism braindump on existential risk from artificial intelligence\n",
      "https://forum.effectivealtruism.org/posts/idjzaqfGguEAaC34j/if-your-agi-x-risk-estimates-are-low-what-scenarios-make-up | If your AGI x-risk estimates are low, what scenarios make up the bulk of your expectations for an OK outcome? - EA Forum\n",
      "https://www.lesswrong.com/posts/4ufbirCCLsFiscWuY/a-proposed-method-for-forecasting-ai#Summary_of_the_Direct_Approach | A proposed method for forecasting transformative AI - LessWrong\n",
      "https://www.lesswrong.com/posts/tZExpBovNhrBvCZSb/how-could-you-possibly-choose-what-an-ai-wants | How could you possibly choose what an AI wants? - LessWrong\n",
      "https://www.lesswrong.com/posts/hAnKgips7kPyxJRY3/ai-governance-and-strategy-priorities-talent-gaps-and | AI Governance & Strategy: Priorities, talent gaps, & opportunities - LessWrong\n",
      "https://forum.effectivealtruism.org/posts/MP9qDZCXMaTJhiJ9u/ea-is-three-radical-ideas-i-want-to-protect?commentId=CTqXDrq3kG4t8fzHT | EA is three radical ideas I want to protect - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/W93Pt7xch7eyrkZ7f/cause-area-report-antimicrobial-resistance | Cause area report: Antimicrobial Resistance - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/Pfayu5Bf2apKreueD/a-playbook-for-ai-risk-reduction-focused-on-misaligned-ai | A Playbook for AI Risk Reduction (focused on misaligned AI) - EA Forum\n",
      "https://www.lesswrong.com/posts/AfGmsjGPXN97kNp57/arguments-about-fast-takeoff | Arguments about fast takeoff\n",
      "https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization | A central AI alignment problem: capabilities generalization, and the sharp left turn\n",
      "https://forum.effectivealtruism.org/posts/7mSqokBNuHu3rzy4L/retrospective-on-recent-activity-of-riesgos-catastroficos | Retrospective on recent activity of Riesgos Catastr√≥ficos Globales - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/AJDgnPXqZ48eSCjEQ/ea-survey-2022-demographics?commentId=sR5GhwEvcHHfWpRTK#sR5GhwEvcHHfWpRTK | EA Survey 2022: Demographics - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/myp9Y9qJnpEEWhJF9/linch-s-shortform?commentId=ymhXwLRhjAh2qEHeA | Linch's Shortform - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/nh8dx6JJt3Ga3BRdp/gwwc-reporting-attrition-visualization#comments | GWWC Reporting Attrition Visualization - EA Forum\n",
      "https://www.lesswrong.com/posts/qJgz2YapqpFEDTLKn/deepmind-alignment-team-opinions-on-agi-ruin-arguments | DeepMind alignment team opinions on AGI ruin arguments - LessWrong\n",
      "https://forum.effectivealtruism.org/posts/idrBxfsHkYeTtpm2q/seeking-paid-case-studies-on-standards | Seeking (Paid) Case Studies on Standards - EA Forum\n",
      "https://www.lesswrong.com/posts/keiYkaeoLHoKK4LYA/six-dimensions-of-operational-adequacy-in-agi-projects | Six Dimensions of Operational Adequacy in AGI Projects - LessWrong\n",
      "https://forum.effectivealtruism.org/posts/fsaogRokXxby6LFd7/a-compute-based-framework-for-thinking-about-the-future-of | A compute-based framework for thinking about the future of AI - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/nKWc4EzRjkpcbDA3A/ai-risk-management-framework-or-nist | AI Risk Management Framework  NIST - EA Forum\n",
      "https://www.lesswrong.com/posts/RydETq379eoWqBFvj/updates-and-reflections-on-optimal-exercise-after-nearly-a | Updates and Reflections on Optimal Exercise after Nearly a Decade - LessWrong\n",
      "https://forum.effectivealtruism.org/posts/pR35WbLmruKdiMn2r/continuous-doesn-t-mean-slow | Continuous doesn‚Äôt mean slow - EA Forum\n",
      "https://www.lesswrong.com/s/xMdkfEJhDNCL2KweB | Slowing AI - LessWrong\n",
      "https://www.lesswrong.com/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer | Where I agree and disagree with Eliezer - LessWrong\n",
      "https://www.lesswrong.com/posts/Hw26MrLuhGWH7kBLm/ai-alignment-is-distinct-from-its-near-term-applications | AI alignment is distinct from its near-term applications\n",
      "https://www.lesswrong.com/posts/gGSvwd62TJAxxhcGh/yudkowsky-vs-hanson-on-foom-whose-predictions-were-better | Yudkowsky vs Hanson on FOOM: Whose Predictions Were Better? - LessWrong\n",
      "https://forum.effectivealtruism.org/posts/DW4FyzRTfBfNDWm6J/some-cruxes-on-impactful-alternatives-to-ai-policy-work | Some cruxes on impactful alternatives to AI policy work - EA Forum\n",
      "https://www.lesswrong.com/posts/oktnxsng7Dbc4aoZP/human-level-full-press-diplomacy-some-bare-facts | Human-level Full-Press Diplomacy (some bare facts). - LessWrong\n",
      "https://www.lesswrong.com/posts/x5aTiznxJ4o9EGdj9/uncertainty-about-the-future-does-not-imply-that-agi-will-go | Uncertainty about the future does not imply that AGI will go well - LessWrong\n",
      "https://forum.effectivealtruism.org/posts/JQxvZZdPG5KYjyBfg/four-mindset-disagreements-behind-existential-risk | Four mindset disagreements behind existential risk disagreements in ML - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/dpjCwMwKEPqK3TPnC/notes-on-managing-to-change-the-world | Notes on \"Managing to Change the World\" - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/cP7gkDFxgJqHDGdfJ/ea-and-longtermism-not-a-crux-for-saving-the-world | EA and Longtermism: not a crux for saving the world - EA Forum\n",
      "https://www.lesswrong.com/posts/RaNhnNjExip36NMxM/advice-for-newly-busy-people | Advice for newly busy people - LessWrong\n",
      "https://www.lesswrong.com/posts/uxnjXBwr79uxLkifG/comments-on-openai-s-planning-for-agi-and-beyond | Comments on OpenAI's \"Planning for AGI and beyond\" - LessWrong\n",
      "https://forum.effectivealtruism.org/posts/MAS8riyKsZut4geWy/but-why-would-the-ai-kill-us | But why would the AI kill us? - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/ARkbWch5RMsj6xP5p/transformative-agi-by-2043-is-less-than-1-likely | Transformative AGI by 2043 is <1% likely - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/wx9GgKGWqksvMWjg2/successif-helping-mid-career-and-senior-professionals-have | Successif: helping mid-career and senior professionals have impactful careers - EA Forum\n",
      "https://www.lesswrong.com/posts/BfN88BfZQ4XGeZkda/concrete-reasons-for-hope-about-ai | Concrete Reasons for Hope about AI - LessWrong\n",
      "https://www.lesswrong.com/posts/566kBoPi76t8KAkoD/on-autogpt | On AutoGPT - LessWrong\n",
      "https://forum.effectivealtruism.org/posts/K85qGvjqnJbznNgiY/strawmen-steelmen-and-mithrilmen-getting-the-principle-of | Strawmen, steelmen, and mithrilmen: getting the principle of charity right - EA Forum\n",
      "https://www.lesswrong.com/posts/mmHctwkKjpvaQdC3c/what-should-you-change-in-response-to-an-emergency-and-ai | What should you change in response to an \"emergency\"? And AI risk - LessWrong\n",
      "https://www.lesswrong.com/posts/bwyKCQD7PFWKhELMr/by-default-gpts-think-in-plain-sight?commentId=zfzHshctWZYo8JkLe | By Default, GPTs Think In Plain Sight - LessWrong\n",
      "https://www.lesswrong.com/posts/AL6DRuE8s4yLn3yBo/robin-hanson-s-latest-ai-risk-position-statement | Robin Hanson‚Äôs latest AI risk position statement - LessWrong\n",
      "https://forum.effectivealtruism.org/posts/LqjG4bAxHfmHC5iut/why-i-spoke-to-time-magazine-and-my-experience-as-a-female | Why I Spoke to TIME Magazine, and My Experience as a Female AI Researcher in Silicon Valley with Sexual Harassment/Abuse - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/vWRP8g8pqN9np4Aow/what-are-work-practices-that-you-ve-adopted-that-you-now | What are work practices that you‚Äôve adopted that you now think are underrated? - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/sbtdPJpeKDtYLr2Zf/joseph-lemien-s-shortform?commentId=CKzuPfnPkAndmQXgi | Joseph Lemien's Shortform - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/7CdtdieiijWXWhiZB/what-s-going-on-with-crunch-time | What‚Äôs going on with ‚Äòcrunch time‚Äô? - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/yMptv5msFnnfESCqm/how-i-solved-my-problems-with-low-energy-or-burnout | How I solved my problems with low energy (or: burnout) - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/vC6v2iTafkydBvnz7/agi-ruin-scenarios-are-likely-and-disjunctive | AGI ruin scenarios are likely (and disjunctive) - EA Forum\n",
      "https://www.lesswrong.com/posts/usKXS5jGDzjwqv3FJ/refining-the-sharp-left-turn-threat-model | Refining the Sharp Left Turn threat model, part 1: claims and mechanisms\n",
      "https://www.lesswrong.com/posts/PQtEqmyqHWDa2vf5H/a-quick-guide-to-confronting-doom | A Quick Guide to Confronting Doom\n",
      "https://www.lesswrong.com/posts/gq9GR6duzcuxyxZtD/approximation-is-expensive-but-the-lunch-is-cheap | Approximation is expensive, but the lunch is cheap - LessWrong\n",
      "https://forum.effectivealtruism.org/posts/uGDCaPFaPkuxAowmH/anthropic-core-views-on-ai-safety-when-why-what-and-how | Anthropic: Core Views on AI Safety: When, Why, What, and How - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/gSGhrCXdntxLrMAmJ/ai-strategy-career-pipeline | AI strategy career pipeline - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/KqCybin8rtfP3qztq/agi-and-lock-in | AGI and Lock-In - EA Forum\n",
      "https://www.lesswrong.com/posts/mJ5oNYnkYrd4sD5uE/clarifying-some-key-hypotheses-in-ai-alignment | Clarifying some key hypotheses in AI alignment - LessWrong\n",
      "https://forum.effectivealtruism.org/posts/nTALzRAWxRnrxvoep/implications-of-the-whitehouse-meeting-with-ai-ceos-for-ai | Implications of the Whitehouse meeting with AI CEOs for AI superintelligence risk - a first-step towards evals? - EA Forum\n",
      "https://www.lesswrong.com/posts/mSF4KTxAGRG3EHmhb/ai-x-risk-approximately-ordered-by-embarassment | AI x-risk, approximately ordered by embarrassment\n",
      "https://www.lesswrong.com/posts/QBTdEyL3tDaJY3LNa/ai-kills-everyone-scenarios-require-robotic-infrastructure | AI-kills-everyone scenarios require robotic infrastructure, but not necessarily nanotech - LessWrong\n",
      "https://www.lesswrong.com/posts/KJRBb43nDxk6mwLcR/ai-doom-from-an-llm-plateau-ist-perspective | AI doom from an LLM-plateau-ist perspective\n",
      "https://forum.effectivealtruism.org/posts/Cre2YC3hd5DeYLqDH/link-post-new-york-times-white-house-unveils-initiatives-to | [Link Post: New York Times] White House Unveils Initiatives to Reduce Risks of A.I.\n",
      "https://www.lesswrong.com/posts/ngEvKav9w57XrGQnb/cognitive-emulation-a-naive-ai-safety-proposal | Cognitive Emulation: A Naive AI Safety Proposal - LessWrong\n",
      "https://forum.effectivealtruism.org/posts/MSkxRv8hviGvGgasD/ai-risk-reward-thinking-in-public | AI risk/reward: A simple model - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/nsLTKCd3Bvdwzj9x8/ingroup-deference | Ingroup Deference - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/4SRj3KnRCh7iFoGK2/vaidehi_agarwalla-s-shortform?commentId=NSnCx7TLtKrfkjkor | vaidehi_agarwalla's Shortform - EA Forum\n",
      "https://www.lesswrong.com/posts/wkws2WgraeN8AYJjv/llms-don-t-have-a-coherent-model-of-the-world-what-it-means | \"LLMs Don't Have a Coherent Model of the World\" - What it Means, Why it Matters - LessWrong\n",
      "https://www.lesswrong.com/posts/3TCYqur9YzuZ4qhtq/meta-ai-announces-cicero-human-level-diplomacy-play-with | Meta AI announces Cicero: Human-Level Diplomacy play (with dialogue)\n",
      "https://forum.effectivealtruism.org/posts/weJZjku3HiNgQC4ER/a-note-of-caution-about-recent-ai-risk-coverage | A note of caution about recent AI risk coverage - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/3KAuAS2shyDwnjzNa/predictable-updating-about-ai-risk | Predictable updating about AI risk - EA Forum\n"
     ]
    }
   ],
   "source": [
    "ea_fo_tabs = sorted([t for t in tabs if ('forum.effectivealtruism' in t.lower() or 'lesswrong' in t.lower())])\n",
    "print_tabs(ea_fo_tabs, label='EAFo/LW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ffa8f80-4aa6-4afe-8c6a-59194d69895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open_tabs(ea_fo_tabs, page=1, per_page=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42ae2aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Metaculus etc. ## (8 tabs)\n",
      "\n",
      "https://www.metaculus.com/questions/7326/open-phil-donations-2025/ | Open Phil Donations 2025  Metaculus\n",
      "https://www.metaculus.com/notebooks/10688/how-much-of-ai-progress-is-from-scaling-compute-and-how-far-will-it-scale/ | How much of AI progress is from scaling compute? And how far will it scale?  Metaculus\n",
      "https://twitter.com/JgaltTweets/status/1662814788580175872 | JgaltTweets on Twitter: \"In late March 2022, before PaLM and DALL-E 2 in April and Gato in May, the median on Metaculus for a 'weakly general' AI was 2043, 21 years away. By the start of June it was 2030. Now it's May 2026, three years from now. https://t.co/276E2LZK12\" / Twitter\n",
      "https://www.metaculus.com/questions/4931/when-will-the-woke-index-in-us-elite-media-top/ | Woke Index in US Media  Metaculus\n",
      "https://www.metaculus.com/questions/16505/time-from-tai-to-superintelligence/ | Time From TAI to Superintelligence  Metaculus\n",
      "https://manifold.markets/elibutchad/will-gpt5-be-more-competent-than-me | Will GPT-5 be more competent than me in my area of expertise?  Manifold Markets\n",
      "https://www.metaculus.com/questions/?status=open&has_group=false&order_by=-publish_time&main-feed=true&search=include:comp-sci--ai-and-machinelearning | https://www.metaculus.com/questions/?status=open&has_group=false&order_by=-publish_time&main-feed=true&search=include:comp-sci--ai-and-machinelearning\n",
      "https://manifold.markets/NathanHelmBurger/will-gpt5-be-capable-of-recursive-s | Will GPT-5 be capable of recursive self-improvement?  Manifold Markets\n"
     ]
    }
   ],
   "source": [
    "print_tabs(sorted([t for t in tabs if ('metaculus' in t.lower() or 'manifold' in t.lower() or 'quorum' in t.lower() or 'predictit' in t.lower())]), label='Metaculus etc.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72bdaddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Wikipedia ## (1 tabs)\n",
      "\n",
      "https://www.wikiwand.com/en/Chess_2:_The_Sequel | Chess 2: The Sequel - Wikiwand\n"
     ]
    }
   ],
   "source": [
    "print_tabs(sorted([t for t in tabs if ('wikipedia' in t.lower() or 'wikiwand' in t.lower())]), label='Wikipedia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca9dcfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Reddit ## (3 tabs)\n",
      "\n",
      "https://www.reddit.com/r/mlscaling/comments/uznkhw/comment/iab8vy2/?context=3 | (4) GPT-3 2nd Anniversary : mlscaling\n",
      "https://www.reddit.com/r/BDSMcommunity/ | https://www.reddit.com/r/BDSMcommunity/\n",
      "https://www.reddit.com/r/slatestarcodex/comments/13j5963/contra_scott_on_ai_races/ | (4) Contra Scott on AI Races : slatestarcodex\n"
     ]
    }
   ],
   "source": [
    "print_tabs(sorted([t for t in tabs if 'reddit' in t.lower()]), label='Reddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dedf293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## localhost ## (0 tabs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_tabs(sorted([t for t in tabs if 'guarded-everglades-89687.herokuapp.com' in t.lower() or 'localhost' in t.lower()]), label='localhost')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "677f610e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Chores ## (0 tabs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_tabs(sorted([t for t in tabs if 'instacart' in t.lower()]), label='Chores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fce865e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Amazon ## (0 tabs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_tabs(sorted([t for t in tabs if 'amazon.com' in t.lower()]), label='Amazon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16d46af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Morning Dispatch ## (0 tabs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_tabs(sorted([t for t in tabs if 'morning' in t.lower() and 'dispatch' in t.lower()]), label='Morning Dispatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "108d879a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## GitHub ## (1 tabs)\n",
      "\n",
      "https://gist.github.com/davidad/1d5d0b1395d77473a0862b9823993672 | takeoff_scenario_davidad_20230220.json\n"
     ]
    }
   ],
   "source": [
    "print_tabs(sorted([t for t in tabs if 'github.com' in t.lower()]), label='GitHub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d911bbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## YouTube ## (7 tabs)\n",
      "\n",
      "https://www.youtube.com/watch?v=FjMqY8hHK7Y | Artificial Intelligence Career Stories  EA Student Summit 2020 - YouTube\n",
      "https://www.youtube.com/watch?v=NPstXhM0gUI | DALS S04 - Une rumba avec Aliz√©e et Gr√©goire Lyonnet sur ''Pas toi'' (Tal) - YouTube\n",
      "https://www.youtube.com/watch?app=desktop&v=2SQOXbh-2vU | DALS S04 - Un jive avec Aliz√©e et Gr√©goire Lyonnet sur ''Crazy in love'' (Beyonc√©) - YouTube\n",
      "https://www.youtube.com/watch?app=desktop&v=NXNCu6ekccw | Maud et Nicolas 2017 Comp√©tition Rock Avanc√© - YouTube\n",
      "https://www.youtube.com/watch?v=yHnwk2sATdI | Ep 4 - When will AGI arrive? - Ryan Kupyn (Data Scientist & Forecasting Researcher @ Amazon AWS) - YouTube\n",
      "https://www.youtube.com/watch?v=axRgsdL6NO0 | DALS S04 - Un charleston avec Aliz√©e et Gr√©goire Lyonnet sur ''Bang Bang'' (Will I Am) - YouTube\n",
      "https://www.youtube.com/watch?v=6An7bj2Kmc0 | DALS S04 - Une rumba avec Aliz√©e, Gr√©goire Lyonnet et Candice sur ''Une femme avec une femme'' - YouTube\n"
     ]
    }
   ],
   "source": [
    "print_tabs(sorted([t for t in tabs if 'yout' in t.lower()]), label='YouTube')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2649c14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Instagram ## (0 tabs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_tabs(sorted([t for t in tabs if 'instagram.com' in t.lower()]), label='Instagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab0e02f2-e275-486f-98d3-37d3218dc821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Asana ## (0 tabs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_tabs(sorted([t for t in tabs if 'app.asana.com' in t.lower()]), label='Asana')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc70c265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Other ## (140 tabs)\n",
      "\n",
      "https://gwern.net/fiction/clippy | It Looks Like You‚Äôre Trying To Take Over The World\n",
      "https://arxiv.org/pdf/2305.20010.pdf | 2305.20010.pdf\n",
      "https://theinsideview.ai/ethan2 | https://theinsideview.ai/ethan2\n",
      "https://psyarxiv.com/gq9r6/ | PsyArXiv Preprints  Informal evidence on identifying top talent\n",
      "https://blog.beeminder.com/tocks/ | Tocks  Beeminder Blog\n",
      "https://arxiv.org/abs/2303.09377 | [2303.09377] Protecting Society from AI Misuse: When are Restrictions on Capabilities Warranted?\n",
      "https://fullfocus.co/yes-you-can-stay-on-top-of-email/ | Yes, You Can Stay on Top of Email\n",
      "https://www.nytimes.com/2023/05/04/technology/us-ai-research-regulation.html?partner=slack&smid=sl-share | White House Unveils Initiatives to Reduce Risks of AI - The New York Times\n",
      "https://www.alignmentforum.org/s/n945eovrA3oDueqtq/p/hwxj4gieR7FWNwYfa | Ngo and Yudkowsky on AI capability gains\n",
      "https://www.americanprogress.org/article/the-needed-executive-actions-to-address-the-challenges-of-artificial-intelligence/ | The Needed Executive Actions to Address the Challenges of Artificial Intelligence - Center for American Progress\n",
      "https://worksinprogress.co/ | https://worksinprogress.co/\n",
      "https://sarahconstantin.substack.com/p/why-i-am-not-an-ai-doomer | Why I am Not An AI Doomer - by Sarah Constantin\n",
      "https://arxiv.org/abs/2210.00720 | [2210.00720] Complexity-Based Prompting for Multi-Step Reasoning\n",
      "https://thezvi.substack.com/p/ai-3 | AI #3 - by Zvi Mowshowitz - Don't Worry About the Vase\n",
      "https://arxiv.org/pdf/2305.17144.pdf | 2305.17144.pdf\n",
      "https://aiimpacts.org/likelihood-of-discontinuous-progress-around-the-development-of-agi/ | Likelihood of discontinuous progress around the development of AGI ‚Äì AI Impacts\n",
      "https://arxiv.org/abs/2108.12427#:~:text=It%20would%20also%20create%20infrastructure,the%20deployment%20of%20harmful%20systems. | [2108.12427] Why and How Governments Should Monitor AI Development\n",
      "https://www.cold-takes.com/why-would-ai-aim-to-defeat-humanity/ | Why Would AI \"Aim\" To Defeat Humanity?\n",
      "https://thezvi.substack.com/p/eliezer-yudkowskys-letter-in-time | Eliezer Yudkowsky's Letter in Time Magazine\n",
      "https://thezvi.substack.com/p/ai-practical-advice-for-the-worried | AI: Practical Advice for the Worried - by Zvi Mowshowitz\n",
      "https://www.alignmentforum.org/s/n945eovrA3oDueqtq/p/gf9hhmSvpZfyfS34B | Ngo's view on alignment difficulty\n",
      "https://thezvi.substack.com/p/ai-2 | AI #2 - by Zvi Mowshowitz - Don't Worry About the Vase\n",
      "https://mastodon.social/@danluu/109579156612202841 | Dan Luu: \"Now that ChatGPT has been out ‚Ä¶\" - Mastodon\n",
      "https://aiobjectives.org/ | https://aiobjectives.org/\n",
      "https://www.new.ox.ac.uk/news/oxford-institute-charity-announced | Oxford Institute of Charity announced  New College\n",
      "https://ssir.org/articles/entry/building_a_think_and_do_tank# | Building a Think-and-Do Tank\n",
      "https://myenglishroutine.com/english-terms-endearment/ | The Sweetest English Terms of Endearment to Call Your Loved Ones - My English Routine\n",
      "https://fivethirtyeight.com/features/abortion-2024-republican-candidates/ | Abortion Is Already Tripping Up The 2024 Republican Candidates  FiveThirtyEight\n",
      "https://ctrlcreep.substack.com/p/the-gathomnid-sonnets | The Gathomnid Sonnets - by ctrlcreep - shift sepulchre\n",
      "https://theinsideview.ai/victoria | Victoria Krakovna on AGI Ruin, The Sharp Left Turn And Paradigms Of AI Alignment\n",
      "https://gwern.net/morning-writing | What Is The Morning Writing Effect? ¬∑ Gwern.net\n",
      "https://windowsontheory.org/2022/11/22/ai-will-change-the-world-but-wont-take-it-over-by-playing-3-dimensional-chess/ | AI will change the world, but won‚Äôt take it over by playing ‚Äú3-dimensional chess‚Äù. ‚Äì Windows On Theory\n",
      "https://thezvi.substack.com/p/ai-6-agents-of-change | AI #6: Agents of Change - by Zvi Mowshowitz\n",
      "https://www.whitehouse.gov/ostp/ai-bill-of-rights/ | Blueprint for an AI Bill of Rights - OSTP - The White House\n",
      "https://img1.wsimg.com/blobby/go/3d82daa4-97fe-4096-9c6b-376b92c619de/downloads/MaliciousUseofAI.pdf?ver=1553030594217 | The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation\n",
      "https://80000hours.org/podcast/episodes/rohin-shah-deepmind-doomers-and-doubters/ | Rohin Shah on DeepMind and trying to fairly hear out both AI doomers and doubters - 80,000 Hours\n",
      "https://lightroom.adobe.com/shares/de80b361304440e6800ae5de3f5a2bfb?invite_id=98d9240825d7486c9b21aace95156888 | Kentucky 2023 by William Hurford\n",
      "https://www.ben-evans.com/benedictevans/2020/5/16/not-even-wrong | Not even wrong: predicting tech ‚Äî Benedict Evans\n",
      "https://www.whitehouse.gov/briefing-room/statements-releases/2023/05/04/fact-sheet-biden-harris-administration-announces-new-actions-to-promote-responsible-ai-innovation-that-protects-americans-rights-and-safety/ | Administration Announces New Actions to Promote Responsible AI Innovation that Protects Americans‚Äô Rights and Safety\n",
      "https://montrealethics.ai/foundations-for-the-future-institution-building-for-the-purpose-of-artificial-intelligence-governance/ | Foundations for the future: institution building for the purpose of artificial intelligence governance\n",
      "https://arxiv.org/pdf/2305.16960.pdf | 2305.16960.pdf\n",
      "https://theinsideview.ai/david | https://theinsideview.ai/david\n",
      "https://www.alignmentforum.org/s/n945eovrA3oDueqtq/p/vwLxd6hhFvPbvKmBH | https://www.alignmentforum.org/s/n945eovrA3oDueqtq/p/vwLxd6hhFvPbvKmBH\n",
      "https://www.alignmentforum.org/s/n945eovrA3oDueqtq/p/cCrpbZ4qTCEYXbzje | Ngo and Yudkowsky on scientific reasoning and pivotal acts\n",
      "https://start.omgyes.com/join/pricing | OMGYES.com - The Science of Women‚Äôs Pleasure\n",
      "https://thezvi.substack.com/p/ai-10-code-interpreter-and-george | AI #10: Code Interpreter and Geoff Hinton\n",
      "http://www.kinkfriendly.org/wp-content/uploads/2010/12/kinkfriendly_org_rope_101_compressed.pdf | Rope_Bondage_101_v2\n",
      "https://windowsontheory.org/2022/06/27/injecting-some-numbers-into-the-agi-debate/ | Injecting some numbers into the AGI debate ‚Äì Windows On Theory\n",
      "https://thezvi.substack.com/p/ai-12-the-quest-for-sane-regulations | AI #12: The Quest for Sane Regulations - by Zvi Mowshowitz\n",
      "https://ai.objectives.institute/blog/introducing-talk-to-the-city-our-collective-deliberation-tool | Introducing: Talk to the City - Our Collective Deliberation Tool ‚Äî AI ‚Ä¢ Objectives ‚Ä¢ Institute\n",
      "https://www.alignmentforum.org/s/n945eovrA3oDueqtq/p/nPauymrHwpoNr6ipx | Conversation on technology forecasting and gradualism\n",
      "https://siderea.dreamwidth.org/1237182.html | siderea  [psych/anthro/soc, Patreon] Class (American)\n",
      "https://www.cold-takes.com/racing-through-a-minefield-the-ai-deployment-problem/ | Racing through a minefield: the AI deployment problem\n",
      "https://nathanpmyoung.substack.com/p/artificial-intelligence-riskreward?fbclid=IwAR3APvRCKpl0YFkLINgY9MIRCGpclfQwKLBIfWL8tcpFxTymg2LM_YWfP8 | Artificial Intelligence Risk/Reward: My Sketchy Model\n",
      "https://eroticroomandboard.com/ | Romantic B&B in Salinas, CA  Bed & Bondage  Monterey Stay and Play\n",
      "https://en.pourdemain.ch/ | Pour Demain: Today for tomorrow\n",
      "https://danluu.com/wat/ | Normalization of deviance\n",
      "https://thezvi.substack.com/p/stages-of-survival | Stages of Survival - by Zvi Mowshowitz\n",
      "https://arxiv.org/pdf/2304.03442.pdf | Generative Agents: Interactive Simulacra of Human Behavior\n",
      "https://www.alignmentforum.org/s/n945eovrA3oDueqtq/p/oKYWbXioKaANATxKY | Soares, Tallinn, and Yudkowsky discuss AGI cognition\n",
      "https://jacobbuckman.substack.com/p/we-arent-close-to-creating-a-rapidly | We Aren't Close To Creating A Rapidly Self-Improving AI\n",
      "https://www.insectwelfare.com/about-iwrs | About IWRS ‚Äî Insect Welfare Research Society\n",
      "https://rodneybrooks.com/predictions-scorecard-2023-january-01/ | Predictions Scorecard, 2023 January 01 ‚Äì Rodney Brooks\n",
      "https://www.brookings.edu/blog/techtank/2023/02/15/nists-ai-risk-management-framework-plants-a-flag-in-the-ai-debate/ | NIST‚Äôs AI Risk Management Framework plants a flag in the AI debate\n",
      "https://www.alignmentforum.org/s/n945eovrA3oDueqtq/p/tcCxPLBrEXdxN5HCQ | Shah and Yudkowsky on alignment failures\n",
      "https://www.maximumprogress.org/extropia-archaeology | Extropian Archaeology ‚Äî Maximum Progress\n",
      "https://thezvi.substack.com/p/ai-7-free-agency | AI #7: Free Agency - by Zvi Mowshowitz\n",
      "https://blog.aiimpacts.org/p/framing-ai-strategy | Framing AI strategy - by Zach Stein-Perlman\n",
      "https://thezvi.substack.com/p/the-crux-list | The Crux List - by Zvi Mowshowitz\n",
      "https://hackernoon.com/how-i-solved-the-passman-ctf-challenge-with-gpt-4 | How I Solved the Passman CTF Challenge with GPT-4  HackerNoon\n",
      "https://mwstory.substack.com/p/why-i-generally-dont-recommend-internal | Why I generally don't recommend internal prediction markets or forecasting tournaments to organisations\n",
      "https://cset.georgetown.edu/publication/the-policy-playbook/ | The Policy Playbook Building a Systems-Oriented Approach to Technology and National Security Policy\n",
      "https://sideways-view.com/2018/02/24/takeoff-speeds/ | Takeoff speeds ‚Äì The sideways view\n",
      "https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/ | AI Could Defeat All Of Us Combined\n",
      "https://thezvi.substack.com/p/ai-8-people-can-do-reasonable-things | AI #8: People Can Do Reasonable Things - by Zvi Mowshowitz\n",
      "https://thezvi.substack.com/p/ai-11-in-search-of-a-moat | AI #11: In Search of a Moat - by Zvi Mowshowitz\n",
      "https://www.foreignaffairs.com/united-states/china-multipolarity-myth?utm_medium=social | The Myth of Multipolarity: American Power‚Äôs Staying Power\n",
      "https://philpapers.org/archive/VOLHDA.pdf | Microsoft Word - Vold & Harris - How does AI pose an Xrisk .docx\n",
      "https://jeffreyladish.com/my-vision-of-a-good-future-part-i/ | My vision of a good future, part I - jeffreyladish.com\n",
      "https://arxiv.org/abs/2303.16200 | Natural Selection Favors AIs over Humans\n",
      "https://musingsandroughdrafts.com/2023/02/17/my-current-summary-of-the-state-of-ai-risk/ | My current summary of the state of AI risk ‚Äì musings and rough drafts\n",
      "https://open.spotify.com/playlist/5HqrL3I4qUbOo1rpCj6Pcg?si=6yJG2apxTkyUtFlzxzyEtw&utm_source=native-share-menu&dd=1&nd=1 | happy calm songs:) - playlist by nataliebrogan13  Spotify\n",
      "https://ineffectivealtruismblog.com/2023/06/03/exaggerating-the-risks-part-8-carlsmith-wrap-up/ | Exaggerating the risks (Part 8: Carlsmith wrap-up) - Reflective altruism\n",
      "https://www.macroscience.org/p/on-macroscience | On Macroscience - by Tim Hwang - Macroscience\n",
      "https://theinsideview.ai/roblong | https://theinsideview.ai/roblong\n",
      "https://medium.com/@ElizAyer/meetings-are-the-work-9e429dde6aa3 | Meetings *are* the work. Wherein I take aim at the common tech‚Ä¶  by Elizabeth Ayer  Medium\n",
      "https://thezvi.substack.com/p/ai-9-the-merge-and-the-million-tokens | AI #9: The Merge and the Million Tokens - by Zvi Mowshowitz\n",
      "https://www.alignmentforum.org/s/n945eovrA3oDueqtq/p/sCCdCLPN9E3YvdZhj | Shulman and Yudkowsky on AI progress\n",
      "https://80000hours.org/podcast/episodes/tom-davidson-how-quickly-ai-could-transform-the-world/?source=email&uni_id=0&utm_source=80%2C000+Hours+mailing+list&utm_campaign=ea94351288-EMAIL_CAMPAIGN_2023_05_11_05_03&utm_medium=email&utm_term=0_43bc1ae55c-4e772af3ad-%5BLIST_EMAIL_ID%5D | Tom Davidson on how quickly AI could transform the world - 80,000 Hours\n",
      "https://www.nytimes.com/wirecutter/reviews/best-telescopes-for-beginners/ | The Best Telescopes for Beginners\n",
      "https://www.cold-takes.com/how-we-could-stumble-into-ai-catastrophe/ | How we could stumble into AI catastrophe\n",
      "https://theinsideview.ai/alex | https://theinsideview.ai/alex\n",
      "https://time.com/6283609/artificial-intelligence-race-existential-threat/ | Moving Too Fast on AI Could Be Terrible for Humanity  Time\n",
      "https://onearmedpundit.substack.com/p/how-to-make-forecasting-more-useful?sd=twittergr | How to make forecasting more useful for policy-makers: Part 1\n",
      "https://www.governance.ai/post/annual-report-2022 | Annual Report 2022  GovAI Blog\n",
      "https://www.bloomberg.com/news/articles/2019-04-06/the-google-ai-ethics-board-with-actual-power-is-still-around?leadSource=uverify%20wall#xj4y7vzkg | The Google AI Ethics Board With Actual Power Is Still Around - Bloomberg\n",
      "https://thegradientpub.substack.com/p/talia-ringer-formal-verification?r=2qha5&utm_campaign=post&utm_medium=web#details | Talia Ringer: Formal Verification and Deep Learning\n",
      "https://80000hours.org/podcast/episodes/ben-garfinkel-classic-ai-risk-arguments/ | BenGarfinkelonscrutinisingclassicAIrisk arguments\n",
      "https://thezvi.substack.com/p/ai-1-sydney-and-bing | AI #1: Sydney and Bing - by Zvi Mowshowitz\n",
      "https://www.gov.uk/government/news/uk-to-host-first-global-summit-on-artificial-intelligence?fbclid=IwAR1b6xdp2X_qD3r7IBaHdtNGjz7T1sLSdOOJNtm-9AP2h6PGKzsfDbzkBxo | UK to host first global summit on Artificial Intelligence - GOV.UK\n",
      "https://arxiv.org/pdf/2305.14699.pdf | 2305.14699.pdf\n",
      "https://fivethirtyeight.com/features/roberts-kavanaugh-voting-rights-act-alabama/ | Why Two Supreme Court Conservatives Just Saved The Voting Rights Act  FiveThirtyEight\n",
      "https://thezvi.substack.com/p/ai-5-level-one-bard | AI #5: Level One Bard - by Zvi Mowshowitz\n",
      "https://www.theinformation.com/articles/openais-losses-doubled-to-540-million-as-it-developed-chatgpt?utm_term=popular-articles&utm_source=sg&utm_medium=email&utm_campaign=article_email&utm_content=article-10441 | OpenAI‚Äôs Losses Doubled to $540 Million as It Developed ChatGPT\n",
      "https://www.alignmentforum.org/s/n945eovrA3oDueqtq/p/7MCqRnZzvszsxgtJi | Christiano, Cotra, and Yudkowsky on AI progress\n",
      "https://www.alignmentforum.org/posts/qYzqDtoQaZ3eDDyxa/distinguishing-ai-takeover-scenarios | Distinguishing AI takeover scenarios\n",
      "https://www.samstack.io/p/notes-on-effective-altruism?utm_source=share&utm_medium=android | Notes on Effective Altruism - by Sam Atis - Samstack\n",
      "https://thezvi.substack.com/p/types-and-degrees-of-alignment | Types and Degrees of Alignment - by Zvi Mowshowitz\n",
      "https://www.amazon.co.uk/High-Output-Management-Andrew-Grove/dp/0679762884 | High Output Management: Amazon.co.uk: Grove, Andrew S.: 9780679762881: Books\n",
      "https://theinsideview.ai/irina | https://theinsideview.ai/irina\n",
      "https://www.alignmentforum.org/s/n945eovrA3oDueqtq/p/NbGmfxbaABPsspib7 | Christiano and Yudkowsky on AI predictions and human intelligence\n",
      "https://www.forbes.com/sites/kenrickcai/2023/06/04/stable-diffusion-emad-mostaque-stability-ai-exaggeration/?sh=1f9dc79675c5 | Stable Diffusion‚Äôs AI Benefactor Has A History Of Exaggeration\n",
      "https://blog.beeminder.com/pomopoker/ | Pomodoro Poker  Beeminder Blog\n",
      "https://www.alignmentforum.org/posts/GQat3Nrd9CStHyGaq/response-to-katja-grace-s-ai-x-risk-counterarguments | Response to Katja Grace's AI x-risk counterarguments - AI Alignment Forum\n",
      "https://exploratory-altruism.org/team-partners/ | Team & Partners ‚Äì Centre for exploratory altruism research\n",
      "https://rethinkpriorities.org/publications/historical-global-health-rd-hits | Historical Global Health R&D \"hits\": Development, main sources of funding, and impact ‚Äî Rethink Priorities\n",
      "https://bounded-regret.ghost.io/emergent-deception-optimization/ | Emergent Deception and Emergent Optimization\n",
      "https://openai.com/blog/governance-of-superintelligence | Governance of superintelligence\n",
      "https://quri.substack.com/p/eli-lifland-on-navigating-the-ai?fbclid=IwAR2mPO6XMabu0UrWDzFzyljIFOXmROPnsM-JvQWBPWkD4q7J7oRXg_UKBp4 | Eli Lifland, on Navigating the AI Alignment Landscape\n",
      "https://infogram.com/1p9zelp0zeg5pyi72nknnymj2xsd27wzv9 | Revised (February 2023) Meta-Analytic Validity Coefficients for Predictors of Job Performance - Infogram\n",
      "https://www.lincolnquirk.com/2023/06/02/vegan_nutrition.html | Vegan nutrition notes  Home\n",
      "https://micahflee.com/2023/04/capturing-the-flag-with-gpt-4/?utm_source=substack&utm_medium=email | Capturing the Flag with GPT-4\n",
      "https://aiobjectives.org/blog/mapping-the-discourse-on-ai-safety-amp-ethics | Mapping the Discourse on AI Safety & Ethics ‚Äî AI ‚Ä¢ Objectives ‚Ä¢ Institute\n",
      "https://thezvi.substack.com/p/ai-13-potential-algorithmic-improvements | AI #13: Potential Algorithmic Improvements\n",
      "https://highmodernism.substack.com/p/security-mindset-in-the-manhattan | Security Mindset in the Manhattan Project\n",
      "https://www.nti.org/analysis/articles/cyber/ | The Cyber-Nuclear Threat: Explained\n",
      "https://www.orvis.com/barbour-classic-bedale-jacket/12ER-Family.html | Barbour¬Æ Classic Bedale Waxed Cotton Jacket  Orvis\n",
      "https://arxiv.org/ftp/arxiv/papers/2206/2206.09360.pdf | 2206.09360.pdf\n",
      "https://thezvi.substack.com/p/ai-4-introducing-gpt-4 | AI #4: Introducing GPT-4 - by Zvi Mowshowitz\n",
      "https://windowsontheory.org/2023/04/12/thoughts-on-ai-safety/ | Thoughts on AI safety ‚Äì Windows On Theory\n",
      "https://statmodeling.stat.columbia.edu/2023/04/13/the-percentogram-a-histogram-binned-by-percentages-of-the-cumulative-distribution-rather-than-using-fixed-bin-widths/ | The ‚Äúpercentogram‚Äù‚Äîa histogram binned by percentages of the cumulative distribution, rather than using fixed bin widths  Statistical Modeling, Causal Inference, and Social Science\n",
      "https://www.cyberark.com/resources/threat-research-blog/chatting-our-way-into-creating-a-polymorphic-malware | Chatting Our Way Into Creating a Polymorphic Malware\n",
      "https://www.deepmind.com/blog/an-early-warning-system-for-novel-ai-risks | An early warning system for novel AI risks\n",
      "https://www.pasteurscube.com/a-world-without-email/ | Notes on \"A World Without Email\", plus my practical implementation\n",
      "https://thezvi.substack.com/p/ai-14-a-very-good-sentence | AI #14: A Very Good Sentence - by Zvi Mowshowitz\n",
      "https://www.cold-takes.com/transformative-ai-issues-not-just-misalignment-an-overview/ | Transformative AI issues (not just misalignment): an overview\n",
      "https://www.planned-obsolescence.org/what-were-doing-here/ | What we're doing here\n",
      "https://www.alignmentforum.org/s/n945eovrA3oDueqtq/p/fS7Zdj2e2xMqE6qja | More Christiano, Cotra, and Yudkowsky on AI progress\n",
      "https://www.alignmentforum.org/s/FN5Gj4JM6Xr7F4vts/p/3DFBbPFZyscrAiTKS | My Overview of the AI Alignment Landscape: Threat Models\n",
      "https://thezvi.substack.com/p/on-autogpt | On AutoGPT - by Zvi Mowshowitz - Don't Worry About the Vase\n"
     ]
    }
   ],
   "source": [
    "tabs_ = [t for t in tabs if (not ('google.com' in t.lower() and 'search' in t.lower() and not ('docs.google' in t.lower() or 'sheets.google' in t.lower())) and\n",
    "                             not ('docs.google' in t.lower() or 'sheets.google' in t.lower() or 'drive.google' in t.lower()) and\n",
    "                             not 'facebook.com' in t.lower() and\n",
    "                             not 'twitter.com' in t.lower() and\n",
    "                             not ('forum.effectivealtruism' in t.lower() or 'lesswrong' in t.lower()) and\n",
    "                             not ('metaculus' in t.lower() or 'manifold' in t.lower() or 'predictit' in t.lower() or 'quorum' in t.lower()) and\n",
    "                             not ('wikipedia' in t.lower() or 'wikiwand' in t.lower()) and\n",
    "                             not 'reddit' in t.lower() and\n",
    "                             not 'instagram.com' in t.lower() and\n",
    "                             not ('guarded-everglades-89687.herokuapp.com' in t.lower() or 'localhost' in t.lower()) and\n",
    "                             not 'instacart' in t.lower() and\n",
    "                             not ('morning' in t.lower() and 'dispatch' in t.lower()) and\n",
    "                             not 'amazon.com' in t.lower() and\n",
    "                             not 'github' in t.lower() and\n",
    "                             not 'calendar.google' in t.lower() and\n",
    "                             not 'yout' in t.lower() and\n",
    "                             not 'app.asana.com' in t.lower() and\n",
    "                             not ('messages/' in t.lower() or 'inbox/' in t.lower() or 'mail.google' in t.lower() or 'swapcard' in t.lower()))]\n",
    "tabs_ = sorted(tabs_)\n",
    "print_tabs(tabs_, label='Other')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9a2a7bb-86f9-45bd-b8d6-ed8889caed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open_tabs(tabs_, page=1, per_page=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c128ee3-03e2-4cfb-bfd1-0dfc84775af1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Shuffled all tabs ## (463 tabs)\n",
      "\n",
      "https://mastodon.social/@danluu/109579156612202841 | Dan Luu: \"Now that ChatGPT has been out ‚Ä¶\" - Mastodon\n",
      "https://docs.google.com/document/d/19qoI35NZzkvNghinKZh1ad0shLH-zjEL2rW_xynS16k/edit#heading=h.8ekeme61qpqb | Ashwin's timelines hot takes: PATCH-like scenarios - Google Docs\n",
      "https://forum.effectivealtruism.org/posts/fsaogRokXxby6LFd7/a-compute-based-framework-for-thinking-about-the-future-of | A compute-based framework for thinking about the future of AI - EA Forum\n",
      "https://www.lesswrong.com/posts/AL6DRuE8s4yLn3yBo/robin-hanson-s-latest-ai-risk-position-statement | Robin Hanson‚Äôs latest AI risk position statement - LessWrong\n",
      "https://micahflee.com/2023/04/capturing-the-flag-with-gpt-4/?utm_source=substack&utm_medium=email | Capturing the Flag with GPT-4\n",
      "https://twitter.com/StephenLCasper/status/1666883362974502912 | (3) Stephen Casper on Twitter: \"Here, I share some reflections and takeaways from the mechanistic interpretability challenges that I posed in February. They were recently solved by @MariusHobbhahn and @sheimersheim. I also announce some more interpretability challenges on the way! https://t.co/NwxWFIyMkI\" / Twitter\n",
      "https://fivethirtyeight.com/features/roberts-kavanaugh-voting-rights-act-alabama/ | Why Two Supreme Court Conservatives Just Saved The Voting Rights Act  FiveThirtyEight\n",
      "https://www.orvis.com/barbour-classic-bedale-jacket/12ER-Family.html | Barbour¬Æ Classic Bedale Waxed Cotton Jacket  Orvis\n",
      "https://docs.google.com/document/d/1sZ7N2pOjFaluiZ6roUx90IH2t1rvVX9AjHbfK5qeEoM/edit | Notes EAG neartermist - Google Docs\n",
      "https://www.youtube.com/watch?app=desktop&v=NXNCu6ekccw | Maud et Nicolas 2017 Comp√©tition Rock Avanc√© - YouTube\n",
      "https://docs.google.com/document/d/1SS8XmzHIaS_q7AcPNxKLbHK2kOa3PyPkXMS1Az1cQaM/edit#heading=h.nb2tkwr1m161 | Tradeoff project draft - Google Docs\n",
      "https://docs.google.com/document/d/1wJCLEHAR34joF-cIGxAwCMQm8-D-mlP6LPBoMboFdHk/edit | A survey of concrete risks derived from Artificial Intelligence - Google Docs\n",
      "https://www.governance.ai/post/annual-report-2022 | Annual Report 2022  GovAI Blog\n",
      "https://docs.google.com/document/d/1k3oRN5MSKVvVZ9CVhngQCAYmLCPacwG-9ly6loYlgE8/edit#heading=h.epbjbqd6jzw | Notes - Forecasts + disagreements on Big Strategic Variables - Google Docs\n",
      "https://ctrlcreep.substack.com/p/the-gathomnid-sonnets | The Gathomnid Sonnets - by ctrlcreep - shift sepulchre\n",
      "https://twitter.com/messages/25776739-77344628 | Brandon Goldman / Twitter\n",
      "https://docs.google.com/document/d/1qiQDQKSUDTvyurVzWT-Vn6RmJ24a4Emol8pGO1-ZIb0/edit | https://docs.google.com/document/d/1qiQDQKSUDTvyurVzWT-Vn6RmJ24a4Emol8pGO1-ZIb0/edit\n",
      "https://docs.google.com/document/d/14ou5ob0SPa52boGDrPYZ4Oj_Gje0dCJOV_8l7mftK9o/edit#heading=h.htupmo9y9du4 | EAG London 2023 - Renan Notes - Google Docs\n",
      "https://docs.google.com/document/d/1WSyIfis0vc5pEBI8RpWp7o0tnTHJAS5OeTWgY_H2xdM/edit#heading=h.ors7j470u62z | AI: Thinking Out Loud (WIP) - Google Docs\n",
      "https://docs.google.com/document/d/1A-W3ahsV3DspgnEk7iRXlyctkL0D0kwgP09OGFRu5BI/edit#heading=h.mtpqcbgdzbmj | [Public] Examining pathways from narrow AI to nuclear war - Google Docs\n",
      "https://fivethirtyeight.com/features/abortion-2024-republican-candidates/ | Abortion Is Already Tripping Up The 2024 Republican Candidates  FiveThirtyEight\n",
      "https://www.google.com/search?q=federally+funded+ffrdc&rlz=1CDGOYI_enUS715US715&oq=federally+funded+ffrdc&aqs=chrome..69i57j0i546l2.5365j1j7&hl=en-US&sourceid=chrome-mobile&ie=UTF-8 | federally funded ffrdc - Google Search\n",
      "https://docs.google.com/document/d/1JTHziStX0dFjFWa2Gp8RYfKXJJM69nvAB0mGtCUpgdw/edit#heading=h.j9owozbw0x7p | Layer - Isolation of Digital Systems - Google Docs\n",
      "https://www.alignmentforum.org/s/n945eovrA3oDueqtq/p/fS7Zdj2e2xMqE6qja | More Christiano, Cotra, and Yudkowsky on AI progress\n",
      "https://www.cold-takes.com/racing-through-a-minefield-the-ai-deployment-problem/ | Racing through a minefield: the AI deployment problem\n",
      "https://www.lesswrong.com/posts/ejxwraMP5ye7Bgmpm/things-i-learned-by-spending-five-thousand-hours-in-non-ea | Things I Learned by Spending Five Thousand Hours In Non-EA Charities - LessWrong\n",
      "https://thezvi.substack.com/p/ai-10-code-interpreter-and-george | AI #10: Code Interpreter and Geoff Hinton\n",
      "https://www.pasteurscube.com/a-world-without-email/ | Notes on \"A World Without Email\", plus my practical implementation\n",
      "https://forum.effectivealtruism.org/posts/cPkfCviK5cAsevTdM/the-charity-entrepreneurship-top-ideas-new-charity | The Charity Entrepreneurship top ideas new charity prediction market - EA Forum\n",
      "https://docs.google.com/document/d/1HeuDspWp4VRyWNS5IKOxqZWZoCTpU8k3LU4X3adpVFw/edit#heading=h.zee6ngwoj6jg | RP <> DeepMind May 17, 2023 - Google Docs\n",
      "https://docs.google.com/document/d/1fqTkdMvXL1Qp1PGvHNWop8tNR9jSKUTZWWdc6HTYTwM/edit#heading=h.osty8jeclpyn | Copy of 2023 Strategic Planning: SWOT Brainstorm Document - Google Docs\n",
      "https://docs.google.com/document/d/14giRxnDwnCOMI_HTIHnbwU66XiX51XHYxO8aQfMUXc0/edit#heading=h.ilkan3e0drym | Jared Brown <> Michael Aird - 2023-Apr-13 - US AI policy, lobbying, Global Shield - Google Docs\n",
      "https://sideways-view.com/2018/02/24/takeoff-speeds/ | Takeoff speeds ‚Äì The sideways view\n",
      "https://www.lesswrong.com/posts/uxnjXBwr79uxLkifG/comments-on-openai-s-planning-for-agi-and-beyond | Comments on OpenAI's \"Planning for AGI and beyond\" - LessWrong\n",
      "https://twitter.com/JgaltTweets/status/1662814788580175872 | JgaltTweets on Twitter: \"In late March 2022, before PaLM and DALL-E 2 in April and Gato in May, the median on Metaculus for a 'weakly general' AI was 2043, 21 years away. By the start of June it was 2030. Now it's May 2026, three years from now. https://t.co/276E2LZK12\" / Twitter\n",
      "https://www.lesswrong.com/posts/BfN88BfZQ4XGeZkda/concrete-reasons-for-hope-about-ai | Concrete Reasons for Hope about AI - LessWrong\n",
      "https://musingsandroughdrafts.com/2023/02/17/my-current-summary-of-the-state-of-ai-risk/ | My current summary of the state of AI risk ‚Äì musings and rough drafts\n",
      "https://twitter.com/jachiam0/status/1591494093766787076 | Joshua Achiam on Twitter: \"üßµ to clarify my views on AGI, timelines, and x-risk. TL;DR: My AGI timelines are short-ish (within two decades with things getting weird soon) and I think x-risk is real, but I think probabilities of doom by AGI instrumentally opting to kill us are greatly exaggerated. 1/\" / Twitter\n",
      "https://forum.effectivealtruism.org/posts/P98Pas4cirMQp3cJy/clarifying-and-predicting-agi | Clarifying and predicting AGI - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/8CM9vZ2nnQsWJNsHx/existential-risk-from-ai-survey-results | \"Existential risk from AI\" survey results\n",
      "https://drive.google.com/file/d/1-W5vx__PxZY4IEqWkQ0BqQw5hi3133Pu/view | Delay, detect, defend\n",
      "https://openai.com/blog/governance-of-superintelligence | Governance of superintelligence\n",
      "https://www.gov.uk/government/news/uk-to-host-first-global-summit-on-artificial-intelligence?fbclid=IwAR1b6xdp2X_qD3r7IBaHdtNGjz7T1sLSdOOJNtm-9AP2h6PGKzsfDbzkBxo | UK to host first global summit on Artificial Intelligence - GOV.UK\n",
      "https://docs.google.com/spreadsheets/d/1waiXbSXZs54_plxa7u9sRQTxMbNaEwbve0sBTGT5BvY/edit#gid=0 | GLT current guesses re asks from SP -- April 2023 - Google Sheets\n",
      "https://ai.objectives.institute/blog/introducing-talk-to-the-city-our-collective-deliberation-tool | Introducing: Talk to the City - Our Collective Deliberation Tool ‚Äî AI ‚Ä¢ Objectives ‚Ä¢ Institute\n",
      "https://www.lesswrong.com/s/xMdkfEJhDNCL2KweB | Slowing AI - LessWrong\n",
      "https://docs.google.com/spreadsheets/d/1P8mkLmPOrTNnVH4lOOoPebtlHIp1Pi3lg8bA8NUqaTo/edit#gid=0 | GHW asks - Google Sheets\n",
      "https://arxiv.org/pdf/2305.16960.pdf | 2305.16960.pdf\n",
      "https://twitter.com/kristjanmoore/status/1663860424100413440 | Kristj√°n Moore (Kris) on Twitter: \"@robertwiblin https://t.co/0yzzBo3OG5\" / Twitter\n",
      "https://www.metaculus.com/notebooks/10688/how-much-of-ai-progress-is-from-scaling-compute-and-how-far-will-it-scale/ | How much of AI progress is from scaling compute? And how far will it scale?  Metaculus\n",
      "https://forum.effectivealtruism.org/posts/nKWc4EzRjkpcbDA3A/ai-risk-management-framework-or-nist | AI Risk Management Framework  NIST - EA Forum\n",
      "https://www.lesswrong.com/posts/JcgtKunqmELefxksx/killing-socrates | Killing Socrates - LessWrong\n",
      "https://www.youtube.com/watch?v=FjMqY8hHK7Y | Artificial Intelligence Career Stories  EA Student Summit 2020 - YouTube\n",
      "https://www.lesswrong.com/posts/mSF4KTxAGRG3EHmhb/ai-x-risk-approximately-ordered-by-embarassment | AI x-risk, approximately ordered by embarrassment\n",
      "https://docs.google.com/spreadsheets/d/1vLsL0QRtF7z9B4Jn5nu0xXUQXyZA0y4ej98UptRWNDU/edit#gid=0 | Name longlist - AIGS rebranding\n",
      "https://docs.google.com/presentation/d/19d8DDka9xErkDEHqEhFz152AjOjFLs4tIbZB9vNuBmY/edit#slide=id.ge44d99aa4e_0_0 | WIT OKRs for April 2023 All Staff Meeting - Google Slides\n",
      "https://time.com/6283609/artificial-intelligence-race-existential-threat/ | Moving Too Fast on AI Could Be Terrible for Humanity  Time\n",
      "https://onearmedpundit.substack.com/p/how-to-make-forecasting-more-useful?sd=twittergr | How to make forecasting more useful for policy-makers: Part 1\n",
      "https://twitter.com/MatthewJBar/status/1665489557239001088 | Matthew Barnett on Twitter: \"Joseph Carlsmith estimated that the human brain uses approximately 10^15 FLOP/s. Over 30 years, that's about 10^24 FLOP. Language models exploded in popularity in the last year, timed almost exactly with the release of ML models trained using over 10^24 FLOP. https://t.co/GxD8lGR8U1\" / Twitter\n",
      "https://forum.effectivealtruism.org/posts/pR35WbLmruKdiMn2r/continuous-doesn-t-mean-slow | Continuous doesn‚Äôt mean slow - EA Forum\n",
      "https://twitter.com/stanislavfort/status/1659676932353433601 | Stanislav Fort ‚ú®üß†üìà‚öõÔ∏èüìàü¶æüìàü§ñüìà‚ú® on Twitter: \"@NPCollapse Here's one that was quite popular at the time &amp; now got some more traction because of another interview Rodney Brooks gave: https://t.co/MMbNFgoQ7X\" / Twitter\n",
      "https://twitter.com/AlphaMinus2/status/1641130452789477409 | A good Œ±lpha-Minus ‚ò∫Ô∏è on Twitter: \"@peterwildeford What are your TAI timelines? :)\" / Twitter\n",
      "https://docs.google.com/document/d/11YKTKRumtlheK_9Dv9ECKwwoTeSG3RNcs6qUSajzqDw/edit | 2023.05.22 AI Reference Classes - Google Docs\n",
      "https://thezvi.substack.com/p/the-crux-list | The Crux List - by Zvi Mowshowitz\n",
      "https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/ | AI Could Defeat All Of Us Combined\n",
      "https://www.reddit.com/r/BDSMcommunity/ | https://www.reddit.com/r/BDSMcommunity/\n",
      "https://twitter.com/birchlse/status/1659883804662591490 | Jonathan Birch on Twitter: \"People sometimes ask me what I think of the idea of plant sentience. So I've written a commentary briefly setting out what I think. https://t.co/USSONmXEns https://t.co/L2vCHPLcV2\" / Twitter\n",
      "https://docs.google.com/document/d/1ZxDbMPOUKzwd8rP53Ou3VVWEJhVSyIwnT5p2eFIcxuk/edit#heading=h.rehbguf5xp7c | Updates to XST strategy and culture after the Wytham Abbey retreat, June 2023 - Google Docs\n",
      "https://forum.effectivealtruism.org/posts/sbtdPJpeKDtYLr2Zf/joseph-lemien-s-shortform?commentId=CKzuPfnPkAndmQXgi | Joseph Lemien's Shortform - EA Forum\n",
      "https://docs.google.com/document/d/1tN6pmDqxlwBjzwp5n_3pqii9EHsDJqCloiNtGDXyfYE/edit#heading=h.tnew02vlmfya | Theories of victory in AI governance: relevant readings, people, & notes - Google Docs\n",
      "https://docs.google.com/document/d/1CpO25iV38hXESPRQZmv15bLjBB_hrAYgAkvfHoJbngE/edit#heading=h.j9owozbw0x7p | Layer - Safety Culture - Google Docs\n",
      "https://twitter.com/labenz/status/1655092874768179200 | https://twitter.com/labenz/status/1655092874768179200\n",
      "https://www.facebook.com/caroline.jeanmaire/posts/pfbid0QoMyxNV1BMgfVi5XtMuckbiUJE9aFzZmsFA4n4kPXfZZe6QL8Vw2vKeT6FKMXUXjl | https://www.facebook.com/caroline.jeanmaire/posts/pfbid0QoMyxNV1BMgfVi5XtMuckbiUJE9aFzZmsFA4n4kPXfZZe6QL8Vw2vKeT6FKMXUXjl\n",
      "https://www.facebook.com/topsecret.gov/posts/pfbid02pz9Mj8T6MSYbp7y8YjqN2hD3MdC3rpaa7GqceKRS7o8uPVDJ2VJVjCPY8nyBhX9Ll | Jai Dhyani - In 2018, the ACM Turing Award was awarded to three... - Facebook\n",
      "https://80000hours.org/podcast/episodes/ben-garfinkel-classic-ai-risk-arguments/ | BenGarfinkelonscrutinisingclassicAIrisk arguments\n",
      "https://forum.effectivealtruism.org/posts/MP9qDZCXMaTJhiJ9u/ea-is-three-radical-ideas-i-want-to-protect?commentId=CTqXDrq3kG4t8fzHT | EA is three radical ideas I want to protect - EA Forum\n",
      "https://twitter.com/simonw/status/1665422493694443521 | Simon Willison on Twitter: \"I wrote about how it's infuriatingly hard to understand how closed models train on their input https://t.co/bOdjdkmm4P\" / Twitter\n",
      "https://docs.google.com/document/d/1zxQGQfDeD7uTnUoJ2_L47jKWvSoancgtEsH4Zd6NAt4/edit#heading=h.ow22kk7tct5w | XST retreat 2023 ‚Äì Schedule and Discussion notes - Google Docs\n",
      "https://www.alignmentforum.org/s/n945eovrA3oDueqtq/p/sCCdCLPN9E3YvdZhj | Shulman and Yudkowsky on AI progress\n",
      "https://twitter.com/daniel_271828/status/1665945298211078145 | Daniel Eth (yes, Eth is my actual last name) on Twitter: \"New blog post: Given Extinction Worries, Why Don‚Äôt AI Researchers Quit? Well, Several Reasons I explain why there are so many AI researchers that continue to work on AI despite thinking that more advanced AI might cause literal human extinction: https://t.co/AOc3KDRIZ7\" / Twitter\n",
      "https://forum.effectivealtruism.org/posts/weJZjku3HiNgQC4ER/a-note-of-caution-about-recent-ai-risk-coverage | A note of caution about recent AI risk coverage - EA Forum\n",
      "https://docs.google.com/document/d/100MHVmj9XyTFEfcimj97_S2ri4mDNNf5qy7RJWnA5tI/edit# | Cost effectiveness of learning and applying epistemic methods - project description - Google Docs\n",
      "https://docs.google.com/document/d/1idfbvEpsxrFTGflCErTPZ_NiXjeqPhfwBrJBce1P_Yw/edit#heading=h.mj0jmgv3ic64 | Will Humanity Choose Its Future? v6 - Google Docs\n",
      "https://docs.google.com/document/d/1zsKIgyLjBitm2V-fmJaEPODf8lCYDLOrCuXfewO1HhU/edit | Biosecurity 5pger - Google Docs\n",
      "https://forum.effectivealtruism.org/posts/vC6v2iTafkydBvnz7/agi-ruin-scenarios-are-likely-and-disjunctive | AGI ruin scenarios are likely (and disjunctive) - EA Forum\n",
      "https://www.lesswrong.com/posts/Hw26MrLuhGWH7kBLm/ai-alignment-is-distinct-from-its-near-term-applications | AI alignment is distinct from its near-term applications\n",
      "https://docs.google.com/document/d/1i5qQteGLrvokces6rEWTUfY3khe-LJMOIobrDREcd2w/edit | Peter's takes on some big strategic variables - Google Docs\n",
      "https://docs.google.com/document/d/1HsUiJ9AMacQTk98ImDKyS660EbYHNbZzmNKlXC0xF1s/edit#heading=h.oyy6uniuf2wi | Community building in a world where people actually listen to us - Google Docs\n",
      "https://www.cold-takes.com/transformative-ai-issues-not-just-misalignment-an-overview/ | Transformative AI issues (not just misalignment): an overview\n",
      "https://www.planned-obsolescence.org/what-were-doing-here/ | What we're doing here\n",
      "https://docs.google.com/document/d/1-NRtsVxqE4LnL_gYn0wDRbfH2RvoDthzp-j_9mUazOI/edit#heading=h.3fjkh1axp1du | Three proposed strategies for AGI endgames\n",
      "https://forum.effectivealtruism.org/posts/DW4FyzRTfBfNDWm6J/some-cruxes-on-impactful-alternatives-to-ai-policy-work | Some cruxes on impactful alternatives to AI policy work - EA Forum\n",
      "https://docs.google.com/document/d/1qwKUWu1aa1rikW3zlCPPvPXdS4upsarkJe8-JwcE4tY/edit#heading=h.z6v8ty7j4wlh | You don't have to be that risk-averse to prefer GHW to LT (Final - Shared with Jason) - Google Docs\n",
      "https://docs.google.com/document/d/1LyQKNesRensq3BjLWgzcdJ3wJBjl0n98fAQ7g7HRbYk/edit#heading=h.koq63kgkerx2 | Prioritarianism and Resource Allocation\n",
      "https://arxiv.org/abs/2210.00720 | [2210.00720] Complexity-Based Prompting for Multi-Step Reasoning\n",
      "https://thezvi.substack.com/p/ai-13-potential-algorithmic-improvements | AI #13: Potential Algorithmic Improvements\n",
      "https://forum.effectivealtruism.org/posts/eK8sEq7Djxp3NqxLQ/tyler-cowen-s-challenge-to-develop-an-actual-mathematical | Tyler Cowen's challenge to develop an 'actual mathematical model' for AI X-Risk - EA Forum\n",
      "https://www.lesswrong.com/posts/wkws2WgraeN8AYJjv/llms-don-t-have-a-coherent-model-of-the-world-what-it-means | \"LLMs Don't Have a Coherent Model of the World\" - What it Means, Why it Matters - LessWrong\n",
      "https://www.whitehouse.gov/ostp/ai-bill-of-rights/ | Blueprint for an AI Bill of Rights - OSTP - The White House\n",
      "https://www.new.ox.ac.uk/news/oxford-institute-charity-announced | Oxford Institute of Charity announced  New College\n",
      "https://twitter.com/_akhaliq/status/1665885626485309440 | AK on Twitter: \"Orca: Progressive Learning from Complex Explanation Traces of GPT-4 paper page: https://t.co/LaV0SKH5ZF develop Orca, a 13-billion parameter model that learns to imitate the reasoning process of LFMs. Orca learns from rich signals from GPT-4 including explanation traces;‚Ä¶ https://t.co/3Yof0tJMuT\" / Twitter\n",
      "https://docs.google.com/presentation/d/1iYnnK3TwNBugTCeeRVyL8MJhSb92ZTUNO-7y5Iwyyvs/edit#slide=id.g232b11b7cba_0_5 | GLT OKRs 2023 -- RP All Staff Meeting 2023-04-20 - Google Slides\n",
      "https://80000hours.org/podcast/episodes/tom-davidson-how-quickly-ai-could-transform-the-world/?source=email&uni_id=0&utm_source=80%2C000+Hours+mailing+list&utm_campaign=ea94351288-EMAIL_CAMPAIGN_2023_05_11_05_03&utm_medium=email&utm_term=0_43bc1ae55c-4e772af3ad-%5BLIST_EMAIL_ID%5D | Tom Davidson on how quickly AI could transform the world - 80,000 Hours\n",
      "https://docs.google.com/document/d/18brxFBSZBN6bnZM6RH1viKJFNcRv7r-4S1ryJkD0KfM/edit#heading=h.y0srbz710jxs | Call notes: Jam-Jack Goodman [8 June 2023] Topic: US Lobbying 101\n",
      "https://rethinkpriorities.org/publications/historical-global-health-rd-hits | Historical Global Health R&D \"hits\": Development, main sources of funding, and impact ‚Äî Rethink Priorities\n",
      "https://forum.effectivealtruism.org/posts/KqCybin8rtfP3qztq/agi-and-lock-in | AGI and Lock-In - EA Forum\n",
      "https://www.alignmentforum.org/s/n945eovrA3oDueqtq/p/gf9hhmSvpZfyfS34B | Ngo's view on alignment difficulty\n",
      "https://forum.effectivealtruism.org/posts/Z7r83zrSXcis6ymKo/dissolving-ai-risk-parameter-uncertainty-in-ai-future | ‚ÄòDissolving‚Äô AI Risk ‚Äì Parameter Uncertainty in AI Future Forecasting - EA Forum\n",
      "https://bounded-regret.ghost.io/emergent-deception-optimization/ | Emergent Deception and Emergent Optimization\n",
      "https://jacobbuckman.substack.com/p/we-arent-close-to-creating-a-rapidly | We Aren't Close To Creating A Rapidly Self-Improving AI\n",
      "https://forum.effectivealtruism.org/posts/gSGhrCXdntxLrMAmJ/ai-strategy-career-pipeline | AI strategy career pipeline - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/nTALzRAWxRnrxvoep/implications-of-the-whitehouse-meeting-with-ai-ceos-for-ai | Implications of the Whitehouse meeting with AI CEOs for AI superintelligence risk - a first-step towards evals? - EA Forum\n",
      "https://docs.google.com/presentation/d/1HLj_1v7Hnr8xO0qqfSqucsKbCz7s2fTzsP7gpqT7TA8/edit#slide=id.p | EAG London Talk (Ben Garfinkel) - Google Slides\n",
      "https://docs.google.com/document/d/1bY5cKyw6PhsmcvJuTWym1jEeHEo0xZqz8B_qhthwcBE/edit | EV of the Future and Counterfactual Credit (New Version) - Google Docs\n",
      "https://www.metaculus.com/questions/7326/open-phil-donations-2025/ | Open Phil Donations 2025  Metaculus\n",
      "https://forum.effectivealtruism.org/posts/AJDgnPXqZ48eSCjEQ/ea-survey-2022-demographics?commentId=sR5GhwEvcHHfWpRTK#sR5GhwEvcHHfWpRTK | EA Survey 2022: Demographics - EA Forum\n",
      "https://80000hours.org/podcast/episodes/rohin-shah-deepmind-doomers-and-doubters/ | Rohin Shah on DeepMind and trying to fairly hear out both AI doomers and doubters - 80,000 Hours\n",
      "https://twitter.com/arankomatsuzaki/status/1662991826431639553 | Aran Komatsuzaki on Twitter: \"Playing repeated games with Large Language Models - LLMs are particularly good at games where valuing their own self-interest pays off, like the iterated Prisoner‚Äôs Dilemma family. - However, they behave sub-optimally in games that require coordination. - In the canonical‚Ä¶ https://t.co/jrvTOXKGR8\" / Twitter\n",
      "https://www.lesswrong.com/posts/4ufbirCCLsFiscWuY/a-proposed-method-for-forecasting-ai#Summary_of_the_Direct_Approach | A proposed method for forecasting transformative AI - LessWrong\n",
      "https://www.lesswrong.com/posts/qJgz2YapqpFEDTLKn/deepmind-alignment-team-opinions-on-agi-ruin-arguments | DeepMind alignment team opinions on AGI ruin arguments - LessWrong\n",
      "https://www.lesswrong.com/posts/keiYkaeoLHoKK4LYA/six-dimensions-of-operational-adequacy-in-agi-projects | Six Dimensions of Operational Adequacy in AGI Projects - LessWrong\n",
      "https://docs.google.com/document/d/1cPyHo7Xym0RaDVI55bIKgqQJhztcYV_Bj77fO_i5VZw/edit#heading=h.grts0kyn5j76 | X-Risk in the Pre-AGI Transition Period\n",
      "https://www.lesswrong.com/posts/RydETq379eoWqBFvj/updates-and-reflections-on-optimal-exercise-after-nearly-a | Updates and Reflections on Optimal Exercise after Nearly a Decade - LessWrong\n",
      "https://thezvi.substack.com/p/ai-8-people-can-do-reasonable-things | AI #8: People Can Do Reasonable Things - by Zvi Mowshowitz\n",
      "https://www.youtube.com/watch?app=desktop&v=2SQOXbh-2vU | DALS S04 - Un jive avec Aliz√©e et Gr√©goire Lyonnet sur ''Crazy in love'' (Beyonc√©) - YouTube\n",
      "https://docs.google.com/document/d/1fLL5BZf8VdhdEQ9uNDne6mp8sNTmgUJSOJ8LQxaZVFI/edit#heading=h.ahaokbxu01um | How bad/good would shorter AI timelines be? Why? [writeup idea + notes + reading list]\n",
      "https://twitter.com/zittrain/status/1663623838943563776 | Jonathan Zittrain on Twitter: \"Today, a crisp one-sentence open letter warning about existential AI threat: ‚ÄúMitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.‚Äù I did not sign the letter. https://t.co/aeoJ4GUTo9\" / Twitter\n",
      "https://twitter.com/Jess_Riedel/status/1666639036864417792 | (1) Jess Riedel on Twitter: \"Anyone know how to square Cruise's claim of 1 injury in first 1M driverless miles and the city of San Francisco's claim they had 4 injuries in 790k autonomous miles? Maybe latter is autonomous with a human safety driver? https://t.co/gzyfxurgMg https://t.co/XMoRS2fRYk https://t.co/M3R13x6JVS\" / Twitter\n",
      "https://forum.effectivealtruism.org/posts/Pfayu5Bf2apKreueD/a-playbook-for-ai-risk-reduction-focused-on-misaligned-ai | A Playbook for AI Risk Reduction (focused on misaligned AI) - EA Forum\n",
      "https://docs.google.com/document/d/1kpdCPU2I0NLWPRwQ4qPSUZcZDobnyR9UY-iSOSBQRm4/edit#heading=h.natf23qmy2fx | Scaling laws literature review - Google Docs\n",
      "https://www.alignmentforum.org/s/n945eovrA3oDueqtq/p/cCrpbZ4qTCEYXbzje | Ngo and Yudkowsky on scientific reasoning and pivotal acts\n",
      "https://www.americanprogress.org/article/the-needed-executive-actions-to-address-the-challenges-of-artificial-intelligence/ | The Needed Executive Actions to Address the Challenges of Artificial Intelligence - Center for American Progress\n",
      "https://www.lesswrong.com/posts/QBTdEyL3tDaJY3LNa/ai-kills-everyone-scenarios-require-robotic-infrastructure | AI-kills-everyone scenarios require robotic infrastructure, but not necessarily nanotech - LessWrong\n",
      "http://www.kinkfriendly.org/wp-content/uploads/2010/12/kinkfriendly_org_rope_101_compressed.pdf | Rope_Bondage_101_v2\n",
      "https://arxiv.org/pdf/2304.03442.pdf | Generative Agents: Interactive Simulacra of Human Behavior\n",
      "https://www.alignmentforum.org/s/n945eovrA3oDueqtq/p/hwxj4gieR7FWNwYfa | Ngo and Yudkowsky on AI capability gains\n",
      "https://joshuablake.github.io/blog/gamma-poisson/ | Improve your forecasts of events: use the gamma-Poisson model ‚Äì Deconfusion Device ‚Äì Failing to understand the world, learning a little along the way\n",
      "https://www.metaculus.com/questions/4931/when-will-the-woke-index-in-us-elite-media-top/ | Woke Index in US Media  Metaculus\n",
      "https://danluu.com/wat/ | Normalization of deviance\n",
      "https://arxiv.org/ftp/arxiv/papers/2206/2206.09360.pdf | 2206.09360.pdf\n",
      "https://twitter.com/Jsevillamol/status/1667861554459471878 | Jaime Sevilla on Twitter: \"My experience with LW people is that they consistently underestimate how seriously other people will take the issue and overestimate how sudden AI developments will be\" / Twitter\n",
      "https://docs.google.com/document/d/1ZYfKFjzOeFiaK86U0np0W2XcpjUIdjJ51Vq7hxntht8/edit#heading=h.6pw5bytuj5u7 | (Forum copy) RP Campus Awareness Survey - Google Docs\n",
      "https://windowsontheory.org/2022/06/27/injecting-some-numbers-into-the-agi-debate/ | Injecting some numbers into the AGI debate ‚Äì Windows On Theory\n",
      "https://twitter.com/messages/25776739-1068417927903436800 | Brendan Finan / Twitter\n",
      "https://www.alignmentforum.org/s/n945eovrA3oDueqtq/p/7MCqRnZzvszsxgtJi | Christiano, Cotra, and Yudkowsky on AI progress\n",
      "https://thezvi.substack.com/p/ai-3 | AI #3 - by Zvi Mowshowitz - Don't Worry About the Vase\n",
      "https://forum.effectivealtruism.org/posts/icdd4FCKuwqyAuYBm/eli-s-review-of-is-power-seeking-ai-an-existential-risk | Eli's review of \"Is power-seeking AI an existential risk?\"\n",
      "https://www.lesswrong.com/posts/k2SNji3jXaLGhBeYP/extrapolating-gpt-n-performance | Extrapolating GPT-N performance - LessWrong\n",
      "https://docs.google.com/document/d/1gd6qQx-SP6rfAVQE5rzfPH0zXYOOHXBa13JSUd2zROQ/edit | Daniel's Randomly Generated Future: Hardware Accelerates - Google Docs\n",
      "https://infogram.com/1p9zelp0zeg5pyi72nknnymj2xsd27wzv9 | Revised (February 2023) Meta-Analytic Validity Coefficients for Predictors of Job Performance - Infogram\n",
      "https://docs.google.com/document/d/10pSj7Jb68sPO0bQyw7cMswsMtx1A7tOGPxy3JbrLC8I/edit#heading=h.6rnrfpst2h9p | Thoughts on founder support preparation - Google Docs\n",
      "https://forum.effectivealtruism.org/posts/W93Pt7xch7eyrkZ7f/cause-area-report-antimicrobial-resistance | Cause area report: Antimicrobial Resistance - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/vWRP8g8pqN9np4Aow/what-are-work-practices-that-you-ve-adopted-that-you-now | What are work practices that you‚Äôve adopted that you now think are underrated? - EA Forum\n",
      "https://docs.google.com/document/d/1JataZjU6aIon_tB1_dqMp7lXzPQYT7Uqu5m5DKMbdb4/edit#heading=h.mfc0g6vdbaom | Evals, safe scaling, & related policy/regulation: relevant readings, people, & notes - Google Docs\n",
      "https://medium.com/@ElizAyer/meetings-are-the-work-9e429dde6aa3 | Meetings *are* the work. Wherein I take aim at the common tech‚Ä¶  by Elizabeth Ayer  Medium\n",
      "https://www.insectwelfare.com/about-iwrs | About IWRS ‚Äî Insect Welfare Research Society\n",
      "https://thezvi.substack.com/p/ai-9-the-merge-and-the-million-tokens | AI #9: The Merge and the Million Tokens - by Zvi Mowshowitz\n",
      "https://www.lesswrong.com/posts/hAnKgips7kPyxJRY3/ai-governance-and-strategy-priorities-talent-gaps-and | AI Governance & Strategy: Priorities, talent gaps, & opportunities - LessWrong\n",
      "https://docs.google.com/document/d/1XfwR6Tc0fFKDUoq8yjjG2m1g_5nopCkRGUCwc-fLSL4/edit | Untitled document - Google Docs\n",
      "https://www.google.com/search?gs_ssp=eJzj4tVP1zc0TDYtLjHNMyk3YPTiz0ktUUjNVcjMUyjPzEsvBgCbmwoM&q=let+em+in+wings&rlz=1CDGOYI_enUS715US715&oq=let+em+in+win&gs_lcrp=EgZjaHJvbWUqBwgBEC4YgAQyCggAEAAY4wIYgAQyBwgBEC4YgAQyBggCEEUYOTIHCAMQABiABDIHCAQQABiABDIHCAUQABiABDIHCAYQABiABDIICAcQABgWGB4yCAgIEAAYFhgeMggICRAAGBYYHtIBCDQ4MDRqMGo3qAIAsAIA&hl=en-US&sourceid=chrome-mobile&ie=UTF-8 | let em in wings - Google Search\n",
      "https://img1.wsimg.com/blobby/go/3d82daa4-97fe-4096-9c6b-376b92c619de/downloads/MaliciousUseofAI.pdf?ver=1553030594217 | The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation\n",
      "https://twitter.com/jeffclune/status/1664618665160085505 | Jeff Clune on Twitter: \"Introducing Thought Cloning: AI agents learn to *think* &amp; act like humans by imitating the thoughts &amp; actions of humans thinking out loud while acting, enhancing performance, efficiency, generalization, AI Safety &amp; Interpretability. Led by @shengranhu https://t.co/a2hmGZ4t3f 1/5 https://t.co/h9PBgDHrMA\" / Twitter\n",
      "https://twitter.com/schock/status/1665538876146851842 | Sasha Costanza-Chock is @schock@mas.to on Twitter: \"If you're a journalist who covered AI doomer calls for regulation, now you must cover what is actually going on with AI regulation. I don't make the rules. Here are some starting points: üßµ - White House national R&amp;D strategy for AI: https://t.co/TXBLXu4rs2\" / Twitter\n",
      "https://www.lesswrong.com/posts/566kBoPi76t8KAkoD/on-autogpt | On AutoGPT - LessWrong\n",
      "https://docs.google.com/presentation/d/1iuYIHlHvsnvOUoRX__BCc5J2A0cSieJlENSAyT_uFg8/edit#slide=id.ge44d99aa4e_0_0 | Survey OKRs presentation - Google Slides\n",
      "https://fullfocus.co/yes-you-can-stay-on-top-of-email/ | Yes, You Can Stay on Top of Email\n",
      "https://docs.google.com/document/d/12oQImZrUFiEgJzyG_1xGrulyEYL7UFckJJMKPDVnr9Y/edit | Forecasting Twitter list\n",
      "https://docs.google.com/document/d/1xUvMKRkEOJQcc6V7VJqcLLGAJ2SsdZno0jTIUb61D8k/edit#heading=h.uskcgipunmm1 | Welfare Range and P(Sentience) Distributions - Google Docs\n",
      "https://twitter.com/JgaltTweets/status/1665801795346608135 | (9) JgaltTweets on Twitter: \"@peterwildeford @StefanFSchubert @Simeon_Cps Seems to be associated with increased stroke risk (which could be either fatal or disabling; disabling presumably not captured in life tables?)\" / Twitter\n",
      "https://www.alignmentforum.org/posts/qYzqDtoQaZ3eDDyxa/distinguishing-ai-takeover-scenarios | Distinguishing AI takeover scenarios\n",
      "https://docs.google.com/document/d/1rvuzMKK3ap7ODD6vWAnZq4RuPberN-d-WHzAYvqO3FU/edit | [RP-internal copy] Bid: build a lobbying apparatus for AI regulations, including for big asks that aren't yet feasible - Google Docs\n",
      "https://forum.effectivealtruism.org/posts/idrBxfsHkYeTtpm2q/seeking-paid-case-studies-on-standards | Seeking (Paid) Case Studies on Standards - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/7CdtdieiijWXWhiZB/what-s-going-on-with-crunch-time | What‚Äôs going on with ‚Äòcrunch time‚Äô? - EA Forum\n",
      "https://twitter.com/HaydnBelfield/status/1666423311532806145 | https://twitter.com/HaydnBelfield/status/1666423311532806145\n",
      "https://www.lesswrong.com/posts/usKXS5jGDzjwqv3FJ/refining-the-sharp-left-turn-threat-model | Refining the Sharp Left Turn threat model, part 1: claims and mechanisms\n",
      "https://docs.google.com/document/d/1tMcC_b18ZDowxIhrSqsHfg4S2X02JF2ui3W4P62d6Lg/edit#heading=h.livlmiwiaubo | Report on Data Centers - TAIGA Version 2023-04-20 - Google Docs\n",
      "https://forum.effectivealtruism.org/posts/fsaogRokXxby6LFd7/a-compute-based-framework-for-thinking-about-the-future-of?utm_source=EA+Forum+Digest&utm_campaign=05c2857556-EMAIL_CAMPAIGN_2023_06_07_11_02&utm_medium=email&utm_term=0_7457c7ff3e-91f59af93e-%5BLIST_EMAIL_ID%5D | A compute-based framework for thinking about the future of AI\n",
      "https://thezvi.substack.com/p/ai-6-agents-of-change | AI #6: Agents of Change - by Zvi Mowshowitz\n",
      "https://www.ben-evans.com/benedictevans/2020/5/16/not-even-wrong | Not even wrong: predicting tech ‚Äî Benedict Evans\n",
      "https://docs.google.com/document/d/1kX4RVoWGicYug0Dr0Rt5sLfobHbjMLABlcY-dsTOMqg/edit# | Cascading conditional probabilities show transformative AGI by 2043 is <1% likely\n",
      "https://www.forbes.com/sites/kenrickcai/2023/06/04/stable-diffusion-emad-mostaque-stability-ai-exaggeration/?sh=1f9dc79675c5 | Stable Diffusion‚Äôs AI Benefactor Has A History Of Exaggeration\n",
      "https://docs.google.com/document/d/14dDtyEAh7ealQGfVG8xBaWxcQcyT5sdCNTEDj9blTo8/edit | WIT Possible Projects Apr 2023\n",
      "https://forum.effectivealtruism.org/posts/cP7gkDFxgJqHDGdfJ/ea-and-longtermism-not-a-crux-for-saving-the-world | EA and Longtermism: not a crux for saving the world - EA Forum\n",
      "http://karpathy.github.io/2022/03/14/lecun1989/ | Deep Neural Nets: 33 years ago and 33 years from now\n",
      "https://aiobjectives.org/blog/mapping-the-discourse-on-ai-safety-amp-ethics | Mapping the Discourse on AI Safety & Ethics ‚Äî AI ‚Ä¢ Objectives ‚Ä¢ Institute\n",
      "https://twitter.com/moskov/status/1661019398919057408 | Dustin Moskovitz on Twitter: \"Almost everyone replying to my thread yesterday can‚Äôt even imagine this. Objections to my premise are are all about improved scenarios around lockdowns and reactive vaccines, when I meant preventative measures we‚Äôre not even bothering to fund.\" / Twitter\n",
      "https://twitter.com/JacobSteinhardt/status/1666865408299917313 | https://twitter.com/JacobSteinhardt/status/1666865408299917313\n",
      "https://forum.effectivealtruism.org/posts/ARkbWch5RMsj6xP5p/transformative-agi-by-2043-is-less-than-1-likely | Transformative AGI by 2043 is <1% likely - EA Forum\n",
      "https://docs.google.com/document/d/1EUFZ9sOJxcUpGY3PEaMBloGkAl9nGpmcHgmaZdOGwy4/edit | 2023 AW Dept Retreat - Moral weight implications - Google Docs\n",
      "https://docs.google.com/spreadsheets/d/1TKdLTOeDfBjXUoUAJdPh40Z13bmYf5hvuTUTJH0m41c/edit#gid=2031513321 | [confidential] Staff Overallocation - Google Sheets\n",
      "https://twitter.com/BrewerEricM/status/1667123285501288448 | Eric Brewer on Twitter: \"The US might succeed in avoiding an Iranian bomb. But time, technical advances, and Iran‚Äôs eroding geopolitical isolation will make it harder to roll back the nuclear program and risk cementing Iran as a nuclear threshold state. My latest with @hrome2. https://t.co/EcutwHSybG\" / Twitter\n",
      "https://twitter.com/messages/1414875069558534150 | Metaculites (off the (track) record) / Twitter\n",
      "https://www.lesswrong.com/posts/mJ5oNYnkYrd4sD5uE/clarifying-some-key-hypotheses-in-ai-alignment | Clarifying some key hypotheses in AI alignment - LessWrong\n",
      "https://twitter.com/emollick/status/1665492104598962177 | Ethan Mollick on Twitter: \"Three problems for firms trying to outsource LLM strategy: 1. No one has any special insight into LLMs. Before ChatGPT, no one expected them to be useful &amp; no one knows how to best use them 2. AI works more like people than software 3. Long-range planning for AI is not helpful\" / Twitter\n",
      "https://docs.google.com/document/d/1srRr2sudZnIdRR0jMTKHt7OuP9msGV11ksWN0o9-VSo/edit#heading=h.tnew02vlmfya | Technical AI work to prevent catastrophic misuse: relevant readings, people, & notes\n",
      "https://forum.effectivealtruism.org/posts/uGDCaPFaPkuxAowmH/anthropic-core-views-on-ai-safety-when-why-what-and-how | Anthropic: Core Views on AI Safety: When, Why, What, and How - EA Forum\n",
      "https://docs.google.com/document/d/15FIf6-pvc0Nc2ItFWz5Qk19F-Dg1MPRilB21cXWDjHU/edit#heading=h.drzlnsxrz21q | How can SP be involved in founder support? - Google Docs\n",
      "https://nathanpmyoung.substack.com/p/artificial-intelligence-riskreward?fbclid=IwAR3APvRCKpl0YFkLINgY9MIRCGpclfQwKLBIfWL8tcpFxTymg2LM_YWfP8 | Artificial Intelligence Risk/Reward: My Sketchy Model\n",
      "https://arxiv.org/abs/2303.16200 | Natural Selection Favors AIs over Humans\n",
      "https://thezvi.substack.com/p/ai-14-a-very-good-sentence | AI #14: A Very Good Sentence - by Zvi Mowshowitz\n",
      "https://blog.aiimpacts.org/p/framing-ai-strategy | Framing AI strategy - by Zach Stein-Perlman\n",
      "https://drive.google.com/file/d/1dK81w7xuHVZgz7yAw4DPYglTuRHbWIlF/view | Intermediate Report on Diabetes Mellitus Type 2 (Public, PDF).pdf - Google Drive\n",
      "https://forum.effectivealtruism.org/posts/Cre2YC3hd5DeYLqDH/link-post-new-york-times-white-house-unveils-initiatives-to | [Link Post: New York Times] White House Unveils Initiatives to Reduce Risks of A.I.\n",
      "https://docs.google.com/document/d/1eO_-UjygEZOsQfMtRUv-12FnN0oGVNQi_gr6qNU3a5U/edit | Lab Governance Workstream - 2-Pager (External) - Google Docs\n",
      "https://forum.effectivealtruism.org/posts/H5beCesFybASmwhcM/sam-clarke-s-shortform | Sam Clarke's Shortform - EA Forum\n",
      "https://arxiv.org/abs/2303.09377 | [2303.09377] Protecting Society from AI Misuse: When are Restrictions on Capabilities Warranted?\n",
      "https://docs.google.com/document/d/1KJx_GhV3A8c2leu4hisMDO7sciY49p8BygQAfbJ1mXw/edit#heading=h.tjvfcbqz2mvz | Founder search leads list - Google Docs\n",
      "https://docs.google.com/document/d/14T_RBzlfBTn4IOdxqG0fh6ilmGOxqGWtyKQZPEiHbqc/edit#heading=h.e6lo2rv2yae9 | Dendritic concept note - Google Docs\n",
      "https://www.lincolnquirk.com/2023/06/02/vegan_nutrition.html | Vegan nutrition notes  Home\n",
      "https://www.alignmentforum.org/s/n945eovrA3oDueqtq/p/oKYWbXioKaANATxKY | Soares, Tallinn, and Yudkowsky discuss AGI cognition\n",
      "https://ssir.org/articles/entry/building_a_think_and_do_tank# | Building a Think-and-Do Tank\n",
      "https://siderea.dreamwidth.org/1237182.html | siderea  [psych/anthro/soc, Patreon] Class (American)\n",
      "https://twitter.com/JeffLadish/status/1653247793061044226 | Jeffrey Ladish on Twitter: \"Hugging Chat, a ChatGPT-clone based on a LLaMA-based model, was just launched. I've been using it and while it's a little rough around the edges, it feels similar to ChatGPT in terms of capabilities Only 5 months passed between the launch of ChatGPT and HuggingChat\" / Twitter\n",
      "https://start.omgyes.com/join/pricing | OMGYES.com - The Science of Women‚Äôs Pleasure\n",
      "https://www.nytimes.com/2023/05/04/technology/us-ai-research-regulation.html?partner=slack&smid=sl-share | White House Unveils Initiatives to Reduce Risks of AI - The New York Times\n",
      "https://docs.google.com/document/d/1jH2UpXhi6uFF9nU6PZwbEurNArW5Zi5fPba-uM0MVPE/edit#heading=h.deq8lzwofh50 | Final Draft Report - CEA Animal Ballot Initiatives - Google Docs\n",
      "https://twitter.com/daniel_eth/status/1635885011365957632 | Daniel Eth on Twitter: \"Finally getting around to reading this. Will update my reactions as I go\" / Twitter\n",
      "https://worksinprogress.co/ | https://worksinprogress.co/\n",
      "https://twitter.com/emollick/status/1655684207321006086 | Ethan Mollick on Twitter: \"Hey ChatGPT Code Interpreter: Create code that would win me a science fair. I am a high schooler. Pick whatever field you want, and make sure you run the code and give me the results and how to present it. Give me visualizations, and a way to explain them. Now give me a speech. https://t.co/uxjtyYAEFo\" / Twitter\n",
      "https://windowsontheory.org/2023/04/12/thoughts-on-ai-safety/ | Thoughts on AI safety ‚Äì Windows On Theory\n",
      "https://www.reddit.com/r/slatestarcodex/comments/13j5963/contra_scott_on_ai_races/ | (4) Contra Scott on AI Races : slatestarcodex\n",
      "https://thezvi.substack.com/p/stages-of-survival | Stages of Survival - by Zvi Mowshowitz\n",
      "https://twitter.com/tamaybes/status/1651297219822116867 | Tamay Besiroglu on Twitter: \"Can we use scaling laws to estimate what is required to reach 'human level' on some arbitrary task? Our (speculative) framework suggests yes. We show that scaling laws provide insight into the *horizons* over which outputs are indistinguishable from human-generated outputs. https://t.co/eRfHGiVohZ\" / Twitter\n",
      "https://twitter.com/S_OhEigeartaigh/status/1666788959697993728 | Se√°n √ì h√âigeartaigh on Twitter: \"Excited to see this. Two things I'd like to see that feel absent from the announcement: 1) Substantial involvement from the UK's academic and civil society expert groups 1/3 https://t.co/008ZzZCtIp\" / Twitter\n",
      "https://www.lesswrong.com/posts/QzkTfj4HGpLEdNjXX/an-artificially-structured-argument-for-expecting-agi-ruin | An artificially structured argument for expecting AGI ruin - LessWrong\n",
      "https://www.cold-takes.com/how-we-could-stumble-into-ai-catastrophe/ | How we could stumble into AI catastrophe\n",
      "https://forum.effectivealtruism.org/posts/wx9GgKGWqksvMWjg2/successif-helping-mid-career-and-senior-professionals-have | Successif: helping mid-career and senior professionals have impactful careers - EA Forum\n",
      "https://docs.google.com/document/d/1xFlAx71HEjIHQI36r8gP2Dg0SdI3sz9lLnm5KPw0kno/edit#heading=h.fmkwnd6gv8xf | AI risk from program search\n",
      "https://docs.google.com/spreadsheets/d/1_l-mRnuZJckKFXGmbz0m9vPpcVc7w9PMW-8aR1ppYHg/edit#gid=1705484556 | XST June+July timetable, June 2023 - Google Sheets\n",
      "https://twitter.com/JgaltTweets/status/1666751165168508930 | JgaltTweets on Twitter: \"Press release on the UK's proposed Global Summit on AI Safety to be held later this year: https://t.co/LsHhtVVylX https://t.co/5k7Fs2Ai1v\" / Twitter\n",
      "https://blog.beeminder.com/tocks/ | Tocks  Beeminder Blog\n",
      "https://thezvi.substack.com/p/ai-1-sydney-and-bing | AI #1: Sydney and Bing - by Zvi Mowshowitz\n",
      "https://twitter.com/anthrupad/status/1655421669660405762 | wÃ∏ÕÇÕÇÕïaÃ∑ÕêÕîÃótÃ¥ÕóÃôeÃµÃîÃïÃ¨rÃ¥ÃìÃäÃ∞mÃµÕÉÃΩÕôÕñaÃµÃìÕíÃóÃ¢rÃ∏ÃΩÃ≤kÃ∑ÕùÃÅÕîÃß on Twitter: \"Rob Bensinger argues that its likely that 'STEM-level AGI' (AGI which can reason about all the hard sciences) results in human extinction. He breaks it into a series of 5 claims (in the img) If you doubt the confidence of the conclusion, which claim(s) do you disagree with? https://t.co/GLS86dKJ1U\" / Twitter\n",
      "https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization | A central AI alignment problem: capabilities generalization, and the sharp left turn\n",
      "https://docs.google.com/document/d/1R-H-1old8f9NwsyKNK4d8vEur8350Ju9dDrKIz1dx2o/edit#heading=h.pkj0s0mloy5v | Rough notes on \"crunch time\": definitions, related concepts, prior work\n",
      "https://twitter.com/eric_is_weird/status/1650297235433836545 | Eric Gilliam on Twitter: \"I've been reflecting on this today. IF he's right that the limits of GPT are being reached, it's still hard to bet against OpenAI making the next breakthrough A short thread on recent innovation in jiu jitsu and how it helps contextualize all of this üßµ(1/12)\" / Twitter\n",
      "https://twitter.com/davidmanheim/status/1543625010451021827 | David Manheim - bsky:@davidmanheim.alter.org.il on Twitter: \"@anderssandberg @StefanFSchubert @Miles_Brundage @MatthewJBar @tamaybes @peterwildeford @OHaggstrom @Jotto999 @g_leech_ @bmgarfinkel @robbensinger And as I have proposed, instead of eliciting timelines directly, we need to be eliciting a far richer structure of uncertainties, to allow us to understand whether and how various interventions might reduce risks. https://t.co/E7K9lz14Ge\" / Twitter\n",
      "https://www.reddit.com/r/mlscaling/comments/uznkhw/comment/iab8vy2/?context=3 | (4) GPT-3 2nd Anniversary : mlscaling\n",
      "https://twitter.com/boazbaraktcs/status/1652059204134248448 | (1) Boaz Barak on Twitter: \"1/5 In our post https://t.co/Lbxyc9e942, Aaronson and I discuss potential scenarios for AI. In particular we say that for \"super-intelligence\" type scenarios, AI will need to break out of the current \"sheer data&amp;compute scale\" paradigm. Given Moore's law, why is this the case?\" / Twitter\n",
      "https://docs.google.com/document/d/1dwr2qpaWdCqr_IDhcTT69TmEA5aWfiNftasn5iJ_qhA/edit | Premises to get to Strong LT - Google Docs\n",
      "https://docs.google.com/document/d/1NfyHOxI7AW0apsyrkW9Eqv_LSLLR9_vlmVmSa7QlxZA/edit#heading=h.ohimieuzefxf | Info on AI lab boards - Google Docs\n",
      "https://twitter.com/iabvek/status/1665852623201660929 | iabvek on Twitter: \"@nmehndir @AaronBergman18 @Jess_Riedel @peterwildeford @Simeon_Cps yeah the brigade of EAs trying to rip peter and other forecasters off with wildly off market bets is so annoying\" / Twitter\n",
      "https://forum.effectivealtruism.org/posts/KRSthwicCTRw9Ayzg/large-epistemological-concerns-i-should-maybe-have-about-ea | Large epistemological concerns I should maybe have about EA a priori - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/MSkxRv8hviGvGgasD/ai-risk-reward-thinking-in-public | AI risk/reward: A simple model - EA Forum\n",
      "https://docs.google.com/document/d/1-YmmnXkzaQA2AydAUPKVGHFbjDLCe2qck09IpmNBqcM/edit#heading=h.osty8jeclpyn | Rethink Priorities ‚Äî Insect Farming and Welfare Strategy - Google Docs\n",
      "https://docs.google.com/document/d/1fqSeu0YL223ngmSCvkuhsJ5zCKBOEzxhyfSEkUPfUq8/edit# | AI threat model reading list [crappy first attempt]\n",
      "https://forum.effectivealtruism.org/posts/7mSqokBNuHu3rzy4L/retrospective-on-recent-activity-of-riesgos-catastroficos | Retrospective on recent activity of Riesgos Catastr√≥ficos Globales - EA Forum\n",
      "https://twitter.com/AISafetyMemes/status/1664981210076938241 | AI Notkilleveryoneism Memes on Twitter: \"Guys, there‚Äôs finally an AI x-risk documentary, and it‚Äôs a *masterpiece* THIS is the video to send to curious friends. Let‚Äôs blow this thing up. Don't Look Up - The Documentary: The Case For AI as an Existential Threat https://t.co/tjKTgeP5KD\" / Twitter\n",
      "https://docs.google.com/presentation/d/1dal9XJTgni7TfqMw2OylVwsMWrfz_FaYjPOdcoVl10o/edit#slide=id.g232c77bbbb5_0_5 | Copy of Org-Wide OKRs Presentation - Google Slides\n",
      "https://twitter.com/davidmanheim/status/1556301242460143620 | David Manheim - bsky:@davidmanheim.alter.org.il on Twitter: \"I've been thinking about how people change the world for the better for quite a while. Turns out it's hard, and the world is complex, but more critically, most people aren't trying. And if they care about the world, and want it to be better, that's a shame. (1/25)\" / Twitter\n",
      "https://www.alignmentforum.org/posts/GQat3Nrd9CStHyGaq/response-to-katja-grace-s-ai-x-risk-counterarguments | Response to Katja Grace's AI x-risk counterarguments - AI Alignment Forum\n",
      "https://twitter.com/goodside/status/1666216217642778635 | (2) Riley Goodside on Twitter: \"5M tokens of context. Let that sink in. Yes, there's caveats. But consider what's to come: - Entire codebases in prompts - Novel-length spec docs as instructions - k-shots where k = 10K - Few-shots where each \"shot\" is 50K LoC ‚Üí diff Those who declared the imminent death of‚Ä¶\" / Twitter\n",
      "https://theinsideview.ai/irina | https://theinsideview.ai/irina\n",
      "https://exploratory-altruism.org/team-partners/ | Team & Partners ‚Äì Centre for exploratory altruism research\n",
      "https://manifold.markets/NathanHelmBurger/will-gpt5-be-capable-of-recursive-s | Will GPT-5 be capable of recursive self-improvement?  Manifold Markets\n",
      "https://twitter.com/DrRadchenko/status/1667410620269101057 | (1) Sergey Radchenko on Twitter: \"A see a lot of discussion of this recent article by @scharap: https://t.co/tRl2KvSs4V. Instead of trashing it, people should read the article beyond the title and engage with the argument. I agree with Charap in some of his claims, and disagree in others. Let's take a look.\" / Twitter\n",
      "https://docs.google.com/document/d/1s4zVmg6c6l0Dv6lZOMlTeuJGKDyCFK8iOitzTGA8rgI/edit#heading=h.cn0nfi8o81o1 | Herding Alpacas\n",
      "https://thegradientpub.substack.com/p/talia-ringer-formal-verification?r=2qha5&utm_campaign=post&utm_medium=web#details | Talia Ringer: Formal Verification and Deep Learning\n",
      "https://www.lesswrong.com/posts/gGSvwd62TJAxxhcGh/yudkowsky-vs-hanson-on-foom-whose-predictions-were-better | Yudkowsky vs Hanson on FOOM: Whose Predictions Were Better? - LessWrong\n",
      "https://www.youtube.com/watch?v=yHnwk2sATdI | Ep 4 - When will AGI arrive? - Ryan Kupyn (Data Scientist & Forecasting Researcher @ Amazon AWS) - YouTube\n",
      "https://docs.google.com/document/d/1NA06JZz1gBLa9O4N3_1tlTvBLWy6X19Z--2gzQ_qkdk/edit#heading=h.1cytsywlk7ba | How might the US national security sphere orient & react to increasingly powerful AI? - Google Docs\n",
      "https://docs.google.com/document/d/1FlGPHU3UtBRj4mBPkEZyBQmAuZXnyvHU-yaH-TiNt8w/edit | Garfinkel Review of JC Alignment Report - Google Docs\n",
      "https://theinsideview.ai/ethan2 | https://theinsideview.ai/ethan2\n",
      "https://twitter.com/milesaturpin/status/1656010877269602304 | Miles Turpin on Twitter: \"‚ö°Ô∏èNew paper!‚ö°Ô∏è It‚Äôs tempting to interpret chain-of-thought explanations as the LLM's process for solving a task. In this new work, we show that CoT explanations can systematically misrepresent the true reason for model predictions. https://t.co/ecPRDTin8h üßµ https://t.co/9zp5evMoaA\" / Twitter\n",
      "https://cset.georgetown.edu/publication/the-policy-playbook/ | The Policy Playbook Building a Systems-Oriented Approach to Technology and National Security Policy\n",
      "https://www.cyberark.com/resources/threat-research-blog/chatting-our-way-into-creating-a-polymorphic-malware | Chatting Our Way Into Creating a Polymorphic Malware\n",
      "https://forum.effectivealtruism.org/posts/nh8dx6JJt3Ga3BRdp/gwwc-reporting-attrition-visualization#comments | GWWC Reporting Attrition Visualization - EA Forum\n",
      "https://www.metaculus.com/questions/16505/time-from-tai-to-superintelligence/ | Time From TAI to Superintelligence  Metaculus\n",
      "https://twitter.com/AlecStapp/status/1667174652664315907 | Alec Stapp on Twitter: \"\"The ‚ÄòSafeguarding the Future‚Äô course at MIT tasked non-scientist students with investigating whether LLM chatbots could be prompted to assist non-experts in causing a pandemic. In 1 hour, the chatbots: - suggested 4 potential pandemic pathogens - explained how they can be‚Ä¶ https://t.co/vSLVnIAUDm\" / Twitter\n",
      "https://forum.effectivealtruism.org/posts/L6ZmggEJw8ri4KB8X/my-highly-personal-skepticism-braindump-on-existential-risk | My highly personal skepticism braindump on existential risk from artificial intelligence\n",
      "https://mwstory.substack.com/p/why-i-generally-dont-recommend-internal | Why I generally don't recommend internal prediction markets or forecasting tournaments to organisations\n",
      "https://aiimpacts.org/likelihood-of-discontinuous-progress-around-the-development-of-agi/ | Likelihood of discontinuous progress around the development of AGI ‚Äì AI Impacts\n",
      "https://www.facebook.com/ozzie.gooen/posts/pfbid08o48vhcYDbbrxphoM5R5sMM4Qa8NQk9tXLzbnbY4pnRXjTC38dRYDvHWYoBZtNPal | Ozzie Gooen - Why should we expect boards to be effective?...  Facebook\n",
      "https://docs.google.com/document/d/1iV1OoAYbQjhaMJOV3aTfC2SfR2T5gkY8w2Gg4ESLjaM/edit#heading=h.osty8jeclpyn | 2023 AW Dept Retreat - Agenda [final] - Google Docs\n",
      "https://docs.google.com/document/d/1lC-rIXME-GD1AImZ80b9eP61sroZy8mooLnSeHNgYzM/edit#heading=h.ftvusubre6rz | Brainstorming on RP as a brand - Google Docs\n",
      "https://www.alignmentforum.org/s/n945eovrA3oDueqtq/p/nPauymrHwpoNr6ipx | Conversation on technology forecasting and gradualism\n",
      "https://quri.substack.com/p/eli-lifland-on-navigating-the-ai?fbclid=IwAR2mPO6XMabu0UrWDzFzyljIFOXmROPnsM-JvQWBPWkD4q7J7oRXg_UKBp4 | Eli Lifland, on Navigating the AI Alignment Landscape\n",
      "https://forum.effectivealtruism.org/posts/J4cLuxvAwnKNQxwxj/how-does-ai-progress-affect-other-ea-cause-areas?commentId=JwxP9T5Mc3ekzHpXh | How does AI progress affect other EA cause areas? - EA Forum\n",
      "https://twitter.com/MTabarrok/status/1665057406043209729 | Maxwell Tabarrok üèóÔ∏èüöÄ on Twitter: \"Most of these events were too far out to evaluate, but Drexler's record continues to be way off I suspect he is predicting nanotech in the early 21st and then predicting space exploration a decade or so after advanced nanotech But the premise never happened so 9 wrong in a row https://t.co/Tq3raRQHJf\" / Twitter\n",
      "https://docs.google.com/document/d/1jtX74U03k3_tzvqAc0sTJaYpwx3OI72qrRPXz9r6DGE/edit#heading=h.1lhw3y6kfeo8 | Jam‚Äôs proposal for founder search and support - Google Docs\n",
      "https://forum.effectivealtruism.org/posts/gt6fPgRdEHJSLGd3N/thoughts-on-the-openai-alignment-plan-will-ai-research | Thoughts on the OpenAI alignment plan: will AI research assistants be net-positive for AI existential risk?\n",
      "https://www.lesswrong.com/posts/gq9GR6duzcuxyxZtD/approximation-is-expensive-but-the-lunch-is-cheap | Approximation is expensive, but the lunch is cheap - LessWrong\n",
      "https://statmodeling.stat.columbia.edu/2023/04/13/the-percentogram-a-histogram-binned-by-percentages-of-the-cumulative-distribution-rather-than-using-fixed-bin-widths/ | The ‚Äúpercentogram‚Äù‚Äîa histogram binned by percentages of the cumulative distribution, rather than using fixed bin widths  Statistical Modeling, Causal Inference, and Social Science\n",
      "https://www.alignmentforum.org/s/n945eovrA3oDueqtq/p/tcCxPLBrEXdxN5HCQ | Shah and Yudkowsky on alignment failures\n",
      "https://www.alignmentforum.org/s/n945eovrA3oDueqtq/p/vwLxd6hhFvPbvKmBH | https://www.alignmentforum.org/s/n945eovrA3oDueqtq/p/vwLxd6hhFvPbvKmBH\n",
      "https://twitter.com/JeffLadish/status/1666396854752530433 | Jeffrey Ladish on Twitter: \"I think a fast takeover is more likely. But a gradual takeover is pretty plausible too Imagine that at some number of employees, corporations became 100x, 1000x effective even accounting for their size. That would be a corporation-controlled world. So it could be with AI\" / Twitter\n",
      "https://twitter.com/backus/status/1652433895793516544 | John Backus on Twitter: \"The code interpreter feature on ChatGPT is the most mind blowing thing I've seen yet. All I did was upload a CSV of SF crime data and ask it to visualize trends(!!) https://t.co/pkFdPqgAzb\" / Twitter\n",
      "https://twitter.com/messages/25776739-363201363 | https://twitter.com/messages/25776739-363201363\n",
      "https://www.lesswrong.com/posts/x5aTiznxJ4o9EGdj9/uncertainty-about-the-future-does-not-imply-that-agi-will-go | Uncertainty about the future does not imply that AGI will go well - LessWrong\n",
      "https://forum.effectivealtruism.org/posts/yMptv5msFnnfESCqm/how-i-solved-my-problems-with-low-energy-or-burnout | How I solved my problems with low energy (or: burnout) - EA Forum\n",
      "https://thezvi.substack.com/p/ai-5-level-one-bard | AI #5: Level One Bard - by Zvi Mowshowitz\n",
      "https://open.spotify.com/playlist/5HqrL3I4qUbOo1rpCj6Pcg?si=6yJG2apxTkyUtFlzxzyEtw&utm_source=native-share-menu&dd=1&nd=1 | happy calm songs:) - playlist by nataliebrogan13  Spotify\n",
      "https://docs.google.com/document/u/2/d/1fOCLx9srgcK3-oEKteAu-vTMAyXDEDPGyuIW4tcY6IA/edit | 2023 Altruistic Policy Telecon Agenda\n",
      "https://thezvi.substack.com/p/ai-2 | AI #2 - by Zvi Mowshowitz - Don't Worry About the Vase\n",
      "https://blog.beeminder.com/pomopoker/ | Pomodoro Poker  Beeminder Blog\n",
      "https://thezvi.substack.com/p/ai-4-introducing-gpt-4 | AI #4: Introducing GPT-4 - by Zvi Mowshowitz\n",
      "https://twitter.com/jason_seba/status/1667679950734864385 | Jason Seba on Twitter: \"@peterwildeford Hahahahha\" / Twitter\n",
      "https://www.alignmentforum.org/s/n945eovrA3oDueqtq/p/NbGmfxbaABPsspib7 | Christiano and Yudkowsky on AI predictions and human intelligence\n",
      "https://www.lesswrong.com/posts/RaNhnNjExip36NMxM/advice-for-newly-busy-people | Advice for newly busy people - LessWrong\n",
      "https://docs.google.com/document/d/1w3YEAY6yzqYOIjK5BRhnWbwTN5iV3OOtHf0R67uxecg/edit | Things to say to Caro\n",
      "https://www.metaculus.com/questions/?status=open&has_group=false&order_by=-publish_time&main-feed=true&search=include:comp-sci--ai-and-machinelearning | https://www.metaculus.com/questions/?status=open&has_group=false&order_by=-publish_time&main-feed=true&search=include:comp-sci--ai-and-machinelearning\n",
      "https://twitter.com/ch402/status/1666482929772666880 | (4) Chris Olah on Twitter: \"One of the ideas I find most useful from @AnthropicAI's Core Views on AI Safety post (https://t.co/Q9i2ujIbjm) is thinking in terms of a distribution over safety difficulty. Here's a cartoon picture I like for thinking about it: https://t.co/QYZBCTwHoo\" / Twitter\n",
      "https://docs.google.com/document/d/1QplktgJzt2Njaizu8TI4thjtetc5EzOLknYEubl7ZHg/edit#heading=h.icjgkbth67i3 | *NEWEST* Copy of Kieran's meeting with Co-CEO(s) Running Agenda\n",
      "https://rodneybrooks.com/predictions-scorecard-2023-january-01/ | Predictions Scorecard, 2023 January 01 ‚Äì Rodney Brooks\n",
      "https://www.facebook.com/robbensinger/posts/pfbid02f7McdFNWAA1fXMzzy3BVmwBgAFfU57c2z9N4MgycH7Anyg3Wm71Z8yfNQbKJbMf2l | (1) Rob Bensinger - (Copying over an email I sent some family...  Facebook\n",
      "https://twitter.com/sebkrier/status/1664642737700757512 | S√©b Krier on Twitter: \"A lot of people in AI policy are talking about licensing in the context of AI risk. Here‚Äôs a little thread exploring what this means, what it could look like, and some challenges worth keeping in mind. üèõ https://t.co/1Grjv93laf\" / Twitter\n",
      "https://docs.google.com/document/d/1uATkMdi5xIH9TeHdm-f5syiJHMkiW1EDnpTwGAbTrOc/edit#heading=h.sw94eg3x8iz8 | LT department meetings_2023 - Google Docs\n",
      "https://www.lesswrong.com/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer | Where I agree and disagree with Eliezer - LessWrong\n",
      "https://theinsideview.ai/david | https://theinsideview.ai/david\n",
      "https://aiobjectives.org/ | https://aiobjectives.org/\n",
      "https://forum.effectivealtruism.org/posts/myp9Y9qJnpEEWhJF9/linch-s-shortform?commentId=ymhXwLRhjAh2qEHeA | Linch's Shortform - EA Forum\n",
      "https://docs.google.com/document/d/16aEouFa8470SCYgTNEWEqhJQQw-daaMx4Q2LLf-8W5E/edit#heading=h.6ytgvt3dfnpe | Zoe Williams - May 2023 - RP Performance Evaluation - Google Docs\n",
      "https://twitter.com/lawhsw/status/1667252970407464960 | (1) harry law on Twitter: \"1/15: given 'IAEA for AI' is becoming a canonical ai global governance idea, here's a üßµüßµüßµ on how the International Atomic Energy Agency came to be and what its creation can tell us about a sibling agency to regulate powerful AI models https://t.co/eLFHAGv3x9\" / Twitter\n",
      "https://thezvi.substack.com/p/ai-practical-advice-for-the-worried | AI: Practical Advice for the Worried - by Zvi Mowshowitz\n",
      "https://docs.google.com/document/d/1QFTYRSJPyQoKfO_bFrsxG861xebXbX3XzvBEEaT7Z1U/edit#heading=h.mj6mty720ju | [V.3] What is going to matter if an AI crash project emerges?\n",
      "https://docs.google.com/document/d/1sMH8fibfO602ZsttAjjxq4bBld5xn5UwuIraIInwTzs/edit#heading=h.z1512e3souac | (Forum copy) Post-FTX Public Awareness / Attitudes - Google Docs\n",
      "https://philpapers.org/archive/VOLHDA.pdf | Microsoft Word - Vold & Harris - How does AI pose an Xrisk .docx\n",
      "https://eroticroomandboard.com/ | Romantic B&B in Salinas, CA  Bed & Bondage  Monterey Stay and Play\n",
      "https://www.alignmentforum.org/s/FN5Gj4JM6Xr7F4vts/p/3DFBbPFZyscrAiTKS | My Overview of the AI Alignment Landscape: Threat Models\n",
      "https://twitter.com/dylan522p/status/1628797563007811585 | Dylan Patel on Twitter: \"Meta's internal data shows that they are growing compute and datacenter infrastructure faster than Google and Microsoft! A higher percentage of their capex is spent on servers, and so compute energy footprint is growing faster, despite lower spend. Their PUE is &lt;1.1, so it's not‚Ä¶ https://t.co/Nikto4prZV\" / Twitter\n",
      "https://lightroom.adobe.com/shares/de80b361304440e6800ae5de3f5a2bfb?invite_id=98d9240825d7486c9b21aace95156888 | Kentucky 2023 by William Hurford\n",
      "https://twitter.com/LinchZhang/status/1663698230067232768 | Linch on Twitter: \"I think some ppl have the model of \"experts of risk of a new technology\" as composing a \"technical engineering section\" where the people who produce the technology are experts, and \"political section\" where politicians or political scientists are the experts. This seems wrong.\" / Twitter\n",
      "https://docs.google.com/presentation/d/1jLK9tjEH7Mxqx1fz0IPDzKJvcoLJJVo4jpZuZ42j-rA/edit#slide=id.g213a7bae0d5_0_0 | GHD OKRs\n",
      "https://highmodernism.substack.com/p/security-mindset-in-the-manhattan | Security Mindset in the Manhattan Project\n",
      "https://twitter.com/Simeon_Cps/status/1665209063419047936 | Sim√©on (in DC) on Twitter: \"One (complicated) reason why it's likely that humans are not the upper bound of intelligence is that there's often a robustness/optimization trade-off and human architecture is leaning a lot towards robustness. Ex: If you accept to lose in robustness of your supply chains, you‚Ä¶\" / Twitter\n",
      "https://twitter.com/WilliamAEden/status/1630690003830599680 | William Eden on Twitter: \"My Twitter timeline is full of panicked takes about imminent AI apocalypse and certain doom. I think this is starting to get overplayed, and so I want to make a long thread about why I'm personally not worried yet. Get ready for a big one... 1/n\" / Twitter\n",
      "https://www.nti.org/analysis/articles/cyber/ | The Cyber-Nuclear Threat: Explained\n",
      "https://thezvi.substack.com/p/eliezer-yudkowskys-letter-in-time | Eliezer Yudkowsky's Letter in Time Magazine\n",
      "https://www.deepmind.com/blog/an-early-warning-system-for-novel-ai-risks | An early warning system for novel AI risks\n",
      "https://forum.effectivealtruism.org/posts/idjzaqfGguEAaC34j/if-your-agi-x-risk-estimates-are-low-what-scenarios-make-up | If your AGI x-risk estimates are low, what scenarios make up the bulk of your expectations for an OK outcome? - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/dpjCwMwKEPqK3TPnC/notes-on-managing-to-change-the-world | Notes on \"Managing to Change the World\" - EA Forum\n",
      "https://docs.google.com/spreadsheets/d/198CVP8UQow6XtocRlIx_HQOndPQq6a2BqJxfkeeZNBk/edit#gid=460115521 | 2023.06 EA Funds, AW - RP - Google Sheets\n",
      "https://docs.google.com/document/d/18aOKioNfnAfYgEX439P2Cq9Ocv_C1I6Jkjxkgowslus/edit#heading=h.6ytgvt3dfnpe | 2023: May - RP Performance Evaluation for Ben Snodin - Self Eval - Google Docs\n",
      "https://docs.google.com/presentation/d/1KW7ZkuCdXAC8FEfC6r3w15QQQTn4jUyjFSX24WzCte8/edit#slide=id.g232fae25e24_0_7 | 2023 AW Department OKRs - Google Slides\n",
      "https://www.cold-takes.com/why-would-ai-aim-to-defeat-humanity/ | Why Would AI \"Aim\" To Defeat Humanity?\n",
      "https://www.lesswrong.com/posts/tZExpBovNhrBvCZSb/how-could-you-possibly-choose-what-an-ai-wants | How could you possibly choose what an AI wants? - LessWrong\n",
      "https://theinsideview.ai/alex | https://theinsideview.ai/alex\n",
      "https://en.pourdemain.ch/ | Pour Demain: Today for tomorrow\n",
      "https://twitter.com/HaydnBelfield/status/1664939007946440704 | Haydn Belfield on Twitter: \"Nice overview of AI risks &amp; solutions from @rhysblakely &amp; @whippletom I argue frontier systems (bigger &amp; more powerful than any yet developed) should be regulated ‚Äúlike risky bio or nuclear experiments with licences, pre-approval &amp; 3rdparty evaluations‚Äù https://t.co/APKHwSuMoB\" / Twitter\n",
      "https://twitter.com/labenz/status/1654853321876815872 | Nathan Labenz on Twitter: \"you people love nothing more than a \"leaked internal google memo\" and your breathless \"no moats\" retweets have compelled me to set you straight with another AI-obsessed megathread üòâüßµ tl;dr: we'll see everything, everywhere, all at once, but OpenAI (&amp; Google) have real moats!\" / Twitter\n",
      "https://forum.effectivealtruism.org/posts/nsLTKCd3Bvdwzj9x8/ingroup-deference | Ingroup Deference - EA Forum\n",
      "https://docs.google.com/document/d/1UsRIgm1mFUJNr5UNOmx7j4Q4-lCeVn7OC3euP0kLDnU/edit#heading=h.ni0hbuwqtlub | Hot AI Governance Collective Upskilling (HAICU) ‚Äì Central Doc - Google Docs\n",
      "https://www.whitehouse.gov/briefing-room/statements-releases/2023/05/04/fact-sheet-biden-harris-administration-announces-new-actions-to-promote-responsible-ai-innovation-that-protects-americans-rights-and-safety/ | Administration Announces New Actions to Promote Responsible AI Innovation that Protects Americans‚Äô Rights and Safety\n",
      "https://docs.google.com/document/d/18sXLBNMsLwKXGqFVXESHNBDdK-z7SIu2BH_EYrSw7sY/edit | Project Description [Current] 03/28/23 - Google Docs\n",
      "https://www.amazon.co.uk/High-Output-Management-Andrew-Grove/dp/0679762884 | High Output Management: Amazon.co.uk: Grove, Andrew S.: 9780679762881: Books\n",
      "https://docs.google.com/spreadsheets/d/1ALNFDZDda9aKGOzW3SgwbJJH4rgkwSmlXWuUtKmNhAc/edit#gid=1888482782 | PTO Report Effective Jan 1, 2023 - Managers - Google Sheets\n",
      "https://forum.effectivealtruism.org/posts/ARkbWch5RMsj6xP5p/transformative-agi-by-2043-is-less-than-1-likely?commentId=B2oW9r5LjiWotEK2a&utm_source=EA+Forum+Digest&utm_campaign=05c2857556-EMAIL_CAMPAIGN_2023_06_07_11_02&utm_medium=email&utm_term=0_7457c7ff3e-91f59af93e-%5BLIST_EMAIL_ID%5D | https://forum.effectivealtruism.org/posts/ARkbWch5RMsj6xP5p/transformative-agi-by-2043-is-less-than-1-likely?commentId=B2oW9r5LjiWotEK2a&utm_source=EA+Forum+Digest&utm_campaign=05c2857556-EMAIL_CAMPAIGN_2023_06_07_11_02&utm_medium=email&utm_term=0_7457c7ff3e-91f59af93e-%5BLIST_EMAIL_ID%5D\n",
      "https://twitter.com/adversariel/status/1650313930802368512 | (1) Ariel on Twitter: \"There‚Äôs a lot of fearmongering about LLMs being capable of finding 0day There are three highly complex roadblocks that need to be overcome for this to be a real concern: statefulness, hallucination, and contamination https://t.co/ZZq4OPrglb\" / Twitter\n",
      "https://www.google.com/search?gs_ssp=eJzj4tVP1zc0zDM2rEo3t6wwYPQSK8hJrCxWKE_NyVEozyzJUMgvyUgtKgYA7bgM-Q&q=plays+well+with+others&rlz=1C5CHFA_enUS925US925&oq=plays+well+with+&aqs=chrome.1.0i512j46i340i512l2j69i57j0i512l6.956070j0j1&sourceid=chrome&ie=UTF-8 | plays well with others - Google Search\n",
      "https://twitter.com/DAlperovitch/status/1653375041751375872 | Dmitri Alperovitch on Twitter: \"*NEW* @GeopolDecanted episode: I talk with one of the smartest thinkers on AI policy and tech developments (former WH and DeepMind) about the profound positive and negative military and societal developments we might experience soon (and those we won‚Äôt)üßµ https://t.co/23ErIoRIsk\" / Twitter\n",
      "https://forum.effectivealtruism.org/posts/8YXFaM9yHbhiJTPqp/agi-rising-why-we-are-in-a-new-era-of-acute-risk-and | AGI rising: why we are in a new era of acute risk and increasing public awareness, and what to do now - EA Forum\n",
      "https://docs.google.com/document/d/1ikmEY9bW6BpkqF-D9feWYnTPx0yG-v1HDUcPsmMSduc/edit#heading=h.j9owozbw0x7p | Layer - Requirement Specification and Tracing - Google Docs\n",
      "https://docs.google.com/document/d/16r-PIQCIZSKGgahwJUK0kuTjVY2vLoIQGcsEeuJDfZc/edit | LT Retreat May 2023 - Lab Governance Workstream - Google Docs\n",
      "https://www.youtube.com/watch?v=axRgsdL6NO0 | DALS S04 - Un charleston avec Aliz√©e et Gr√©goire Lyonnet sur ''Bang Bang'' (Will I Am) - YouTube\n",
      "https://thezvi.substack.com/p/ai-7-free-agency | AI #7: Free Agency - by Zvi Mowshowitz\n",
      "https://docs.google.com/document/d/1usMVjv7hMx22K-eu71o1bcfj-7JH_l-aXbfYb6NHSoM/edit | Threat Model for Existential Risks from AI - Google Docs\n",
      "https://forum.effectivealtruism.org/posts/JQxvZZdPG5KYjyBfg/four-mindset-disagreements-behind-existential-risk | Four mindset disagreements behind existential risk disagreements in ML - EA Forum\n",
      "https://www.foreignaffairs.com/united-states/china-multipolarity-myth?utm_medium=social | The Myth of Multipolarity: American Power‚Äôs Staying Power\n",
      "https://twitter.com/daniel_271828/status/1620596689555058689 | Daniel Eth (yes, Eth is my actual last name) on Twitter: \"@peterwildeford @CineraVerinia @turchin This makes me think that there's a pretty good chance (&gt;10%) that if temperatures rise above ~90¬∞F then they'll spiral into uninhabitability. Let's call that 1/3. Okay, so that would require ~30¬∞F or ~17¬∞C increase. So then the question becomes how likely that is.\" / Twitter\n",
      "https://docs.google.com/document/d/136FNAeBw7oKyv8lUZm8qFEsVM8tQUaQzgDrCtLTf4Fs/edit#heading=h.wsiggdpisp4j | Some hot takes on the implementation of transformative AI systems - Google Docs\n",
      "https://thezvi.substack.com/p/on-autogpt | On AutoGPT - by Zvi Mowshowitz - Don't Worry About the Vase\n",
      "https://docs.google.com/document/d/1QsJ8PNqfvvdtkMHclNLqtVP2SVM5dhsj4GMeFztjGBY/edit#heading=h.w6y052tqvke3 | Exploring future AI compute paradigms - Google Docs\n",
      "https://twitter.com/HaydnBelfield/status/1666743402493358081 | (3) Haydn Belfield on Twitter: \"ruh roh https://t.co/IXlqsg1HZy\" / Twitter\n",
      "https://www.lesswrong.com/posts/oktnxsng7Dbc4aoZP/human-level-full-press-diplomacy-some-bare-facts | Human-level Full-Press Diplomacy (some bare facts). - LessWrong\n",
      "https://docs.google.com/document/d/1ziNrskp-v_jWihUakPIhSLqdu6WJY-mA0152RUcLqQc/edit#heading=h.agcon1r7et1w | AIGS Leads notes (Michael, Peter, Zoe, often Ashwin) - 2023 May-Sep\n",
      "https://www.nytimes.com/wirecutter/reviews/best-telescopes-for-beginners/ | The Best Telescopes for Beginners\n",
      "https://montrealethics.ai/foundations-for-the-future-institution-building-for-the-purpose-of-artificial-intelligence-governance/ | Foundations for the future: institution building for the purpose of artificial intelligence governance\n",
      "https://forum.effectivealtruism.org/posts/4SRj3KnRCh7iFoGK2/vaidehi_agarwalla-s-shortform?commentId=NSnCx7TLtKrfkjkor | vaidehi_agarwalla's Shortform - EA Forum\n",
      "https://forum.effectivealtruism.org/posts/TCsanzwKGqfBBTye9/the-wild-and-wacky-claims-of-karnofsky-s-most-important | The 'Wild' and 'Wacky' Claims of Karnofsky‚Äôs ‚ÄòMost Important Century‚Äô - EA Forum\n",
      "https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is | (My understanding of) What Everyone in Technical Alignment is Doing and Why\n",
      "https://www.theinformation.com/articles/openais-losses-doubled-to-540-million-as-it-developed-chatgpt?utm_term=popular-articles&utm_source=sg&utm_medium=email&utm_campaign=article_email&utm_content=article-10441 | OpenAI‚Äôs Losses Doubled to $540 Million as It Developed ChatGPT\n",
      "https://docs.google.com/document/d/1vouv73_c8L05fHDh8x3A_HSBukclMbIb4JE7O2UdFLY/edit#heading=h.x1uc7lfftp0 | AIGS 2023 OKRs [as of April 2023] - Google Docs\n",
      "https://ineffectivealtruismblog.com/2023/06/03/exaggerating-the-risks-part-8-carlsmith-wrap-up/ | Exaggerating the risks (Part 8: Carlsmith wrap-up) - Reflective altruism\n",
      "https://www.youtube.com/watch?v=NPstXhM0gUI | DALS S04 - Une rumba avec Aliz√©e et Gr√©goire Lyonnet sur ''Pas toi'' (Tal) - YouTube\n",
      "https://docs.google.com/document/d/1qxc_XDErDFeQGsYE52vLi1lIJIRL5VL9i1Hi-Btj9Mg/edit# | Info on recent/upcoming AI policy happenings, from May 2023 coordination call\n",
      "https://docs.google.com/document/d/1UvQU-0T-JhfG4u2_Zs2VibDAgN6_PMIOx4zAwy0VG1o/edit# | [Widely shared] How might the US national security sphere orient & react to increasingly powerful AI? - Google Docs\n",
      "https://forum.effectivealtruism.org/posts/zoWypGfXLmYsDFivk/counterarguments-to-the-basic-ai-risk-case | Counterarguments to the basic AI risk case\n",
      "https://twitter.com/repligate/status/1665494009966391297 | janus on Twitter: \"In AI risk discourse, most people form opinions from an outside view: vibes, what friends/\"smart people\" believe, analogies A few operate with inside views, or causal models: those who dare interface directly with (a model of) reality. But they viciously disagree with each other.\" / Twitter\n",
      "https://twitter.com/xuanalogue/status/1652874311605137408 | xuan (…ï…•…õn / sh-yen) on Twitter: \"Bizarre to me that so many LLM benchmarks were using top-1 accuracy as a metric rather than the Brier score or similar -- apparently once you switch to the latter (and other continuous and/or linear metrics), many \"emergent\" behaviors go away!\" / Twitter\n",
      "https://thezvi.substack.com/p/types-and-degrees-of-alignment | Types and Degrees of Alignment - by Zvi Mowshowitz\n",
      "https://docs.google.com/document/d/1EZ1TgTnJ6zt_-JuhboXJwEDZ7r4R2D8WnOzLE7DphUg/edit | What would it look like to regulate AI like bio and nuclear? - Google Docs\n",
      "https://twitter.com/russellwald/status/1658852120563712000 | Russell Wald on Twitter: \"Two hearings on AI in the Senate in the same day are great! But one was sensational the other was substantive. What the fed gov does with its own AI policy has ripple effects across the AI landscape. https://t.co/Ib1soMKsPI\" / Twitter\n",
      "https://twitter.com/ke_li_2021/status/1666810649526308867 | (3) Kenneth Li on Twitter: \"Excited to announce our new work: Inference-Time Intervention (ITI), a minimally-invasive control technique that significantly improves LLM truthfulness using little resources, benchmarked on the TruthfulQA dataset. Preprint: https://t.co/ByiElPgRy0\" / Twitter\n",
      "https://gwern.net/morning-writing | What Is The Morning Writing Effect? ¬∑ Gwern.net\n",
      "https://twitter.com/davidad/status/1627454901247782913?s=46&t=Lap-izdY_pwXQfbJXZHE8g | davidad üéá on Twitter: \"short timelines https://t.co/kfOYpcwlr3\" / Twitter\n",
      "https://docs.google.com/document/d/1OmKOmMfbmBnjxGGHetfN2VQ1az4Zox0Y-4f5xTlRzhw/edit#heading=h.g2xot74rmmug | Maybe let‚Äôs focus more on non-extinction ways that a lot of the potential value of the future could be lost? [quick notes]\n",
      "https://docs.google.com/document/d/1vm-ZwU1XS0j_2WN_IB_QH8CeDfmlHGkKdk5EQyc37JA/edit | EA AWF Application 2023-06 - Google Docs\n",
      "https://twitter.com/NathanpmYoung/status/1662823346390683648 | Nathan on Twitter: \"Perhaps of interest: @CharlesD353 @SmoLurks @peterwildeford @moskov @krishnanrohit @robinhanson @ohabryka @ChanaMessinger @adambinks_ @celloMolly @NunoSempere @joodaloop @visakanv @akrolsmir @fianxu @singularitttt @KatjaGrace @Mappletons @MWStory @mark_ledwich\" / Twitter\n",
      "https://hackernoon.com/how-i-solved-the-passman-ctf-challenge-with-gpt-4 | How I Solved the Passman CTF Challenge with GPT-4  HackerNoon\n",
      "https://manifold.markets/elibutchad/will-gpt5-be-more-competent-than-me | Will GPT-5 be more competent than me in my area of expertise?  Manifold Markets\n",
      "https://www.bloomberg.com/news/articles/2019-04-06/the-google-ai-ethics-board-with-actual-power-is-still-around?leadSource=uverify%20wall#xj4y7vzkg | The Google AI Ethics Board With Actual Power Is Still Around - Bloomberg\n",
      "https://www.lesswrong.com/posts/PQtEqmyqHWDa2vf5H/a-quick-guide-to-confronting-doom | A Quick Guide to Confronting Doom\n",
      "https://www.maximumprogress.org/extropia-archaeology | Extropian Archaeology ‚Äî Maximum Progress\n",
      "https://twitter.com/benskuhn/status/1606407189161091072 | Ben Kuhn on Twitter: \"A thing I often find myself suggesting to new managers is to \"exert more backpressure.\" Backpressure is a concept from fluid dynamics (and distributed systems) meaning the way in which a system resists overload‚Äîe.g. by slowing down, dropping requests, or completely failing.\" / Twitter\n",
      "https://docs.google.com/document/d/1BP7hQkpAd5fsrI4-Y6qgJvLriTyPUVJcd6R0_JF1q2w/edit#heading=h.osty8jeclpyn | [0] Whistleblowing Report v9 - Google Docs\n",
      "https://docs.google.com/document/d/1z3YrMwEcdNGt2X2GoFNIVhbcBZxzsBFiSz4_q6vJXO8/edit#heading=h.n3opm8r5uhlr | Center for Long Term Priorities Update: April 2023 (shared) - Google Docs\n",
      "https://arxiv.org/pdf/2305.17144.pdf | 2305.17144.pdf\n",
      "https://twitter.com/TheZvi/status/1654550601798172677 | (1) Zvi Mowshowitz on Twitter: \"This thread is 20 polls about possible futures. What do we value? What would we consider a doomed future, versus a good future? Each Tweet will present a general description of a potential future scenario. The vote is on how you would view this future, if it somehow happened.\" / Twitter\n",
      "https://arxiv.org/pdf/2305.14699.pdf | 2305.14699.pdf\n",
      "https://forum.effectivealtruism.org/posts/MAS8riyKsZut4geWy/but-why-would-the-ai-kill-us | But why would the AI kill us? - EA Forum\n",
      "https://docs.google.com/document/d/1SbGV0Nc-Nh6WYkTNQ7QUxnbi9kRw0XsMwSqmBk0U0eM/edit | [Private] EAIF Vision and Scope - Google Docs\n",
      "https://thezvi.substack.com/p/ai-12-the-quest-for-sane-regulations | AI #12: The Quest for Sane Regulations - by Zvi Mowshowitz\n",
      "https://www.lesswrong.com/posts/3TCYqur9YzuZ4qhtq/meta-ai-announces-cicero-human-level-diplomacy-play-with | Meta AI announces Cicero: Human-Level Diplomacy play (with dialogue)\n",
      "https://forum.effectivealtruism.org/posts/K85qGvjqnJbznNgiY/strawmen-steelmen-and-mithrilmen-getting-the-principle-of | Strawmen, steelmen, and mithrilmen: getting the principle of charity right - EA Forum\n",
      "https://twitter.com/alyssamvance/status/1667724991306113024 | Alyssa Vance on Twitter: \"I have a new long post on Less Wrong, which aims to show that, over the next decade, it is quite likely that most democratic Western countries will become fascist dictatorships: https://t.co/7HxOa1nNMb\" / Twitter\n",
      "https://docs.google.com/document/d/1FMJZ9wjxooB-7HD7K7c-kEasF1qYuXMfq4wPrFioq_Q/edit#heading=h.mdmelsvomij2 | Info on how GovAI does hiring (Georg Arndt <> Michael Aird notes, 2023-May-15) - Google Docs\n",
      "https://docs.google.com/document/d/1ghEgQeMA56UAffquWhlnJNseNh8NdMLA4NFuTdDsiiU/edit#heading=h.n27z5n7sidxc | Sleepwalking into Survival\n",
      "https://theinsideview.ai/roblong | https://theinsideview.ai/roblong\n",
      "https://forum.effectivealtruism.org/posts/CAC8zn292C9T5aopw/community-health-and-special-projects-updates-and-contacting-1 | Community Health & Special Projects: Updates and Contacting Us - EA Forum\n",
      "https://psyarxiv.com/gq9r6/ | PsyArXiv Preprints  Informal evidence on identifying top talent\n",
      "https://forum.effectivealtruism.org/posts/7hMgK4hciBhXmBRnW/do-you-think-decreasing-the-consumption-of-animals-is-good | Do you think decreasing the consumption of animals is good/bad? Think again? - EA Forum\n",
      "https://twitter.com/messages/25776739-1148306976176132096 | Juan Cambeiro / Twitter\n",
      "https://docs.google.com/document/d/14Sui9HRpEer8TC02vqXtq_fkhv0jQhws8PW-TMUxlGE/edit#heading=h.q6vp1h1iv66u | Marcus + Peter Super Wednesday Agenda - Google Docs\n",
      "https://www.youtube.com/watch?v=6An7bj2Kmc0 | DALS S04 - Une rumba avec Aliz√©e, Gr√©goire Lyonnet et Candice sur ''Une femme avec une femme'' - YouTube\n",
      "https://twitter.com/NathanpmYoung/status/1666008256303562752 | Nathan on Twitter: \"AI safetyish folks saying that AI bias is important/underrated. Thread of examples: https://t.co/ihqHRZ15ge\" / Twitter\n",
      "https://arxiv.org/abs/2108.12427#:~:text=It%20would%20also%20create%20infrastructure,the%20deployment%20of%20harmful%20systems. | [2108.12427] Why and How Governments Should Monitor AI Development\n",
      "https://docs.google.com/document/d/1n5i1-VmV8IRDHTybXh70yV-mZB8RpMuu9ZXaDwLGRRs/edit | EAFo/LW Reading List - Google Docs\n",
      "https://docs.google.com/document/d/19L0k0B0-0gW7t96Q-hpNIknCEry57Hklgt2FXFDUH78/edit#heading=h.mak2h8fgpqe9 | [SES copy] Misuse of AI should be a core priority in AI risk reduction - Google Docs\n",
      "https://www.macroscience.org/p/on-macroscience | On Macroscience - by Tim Hwang - Macroscience\n",
      "https://thezvi.substack.com/p/ai-11-in-search-of-a-moat | AI #11: In Search of a Moat - by Zvi Mowshowitz\n",
      "https://www.samstack.io/p/notes-on-effective-altruism?utm_source=share&utm_medium=android | Notes on Effective Altruism - by Sam Atis - Samstack\n",
      "https://arxiv.org/pdf/2305.20010.pdf | 2305.20010.pdf\n",
      "https://docs.google.com/spreadsheets/d/1hcYteAFXujvTI3KlzUf0FL_du5jwu6cuLPEmPGJ0X5U/edit#gid=0 | Defense in Depth: Matrix of Layers\n",
      "https://windowsontheory.org/2022/11/22/ai-will-change-the-world-but-wont-take-it-over-by-playing-3-dimensional-chess/ | AI will change the world, but won‚Äôt take it over by playing ‚Äú3-dimensional chess‚Äù. ‚Äì Windows On Theory\n",
      "https://www.lesswrong.com/posts/KJRBb43nDxk6mwLcR/ai-doom-from-an-llm-plateau-ist-perspective | AI doom from an LLM-plateau-ist perspective\n",
      "https://myenglishroutine.com/english-terms-endearment/ | The Sweetest English Terms of Endearment to Call Your Loved Ones - My English Routine\n",
      "https://gwern.net/fiction/clippy | It Looks Like You‚Äôre Trying To Take Over The World\n",
      "https://www.lesswrong.com/posts/bwyKCQD7PFWKhELMr/by-default-gpts-think-in-plain-sight?commentId=zfzHshctWZYo8JkLe | By Default, GPTs Think In Plain Sight - LessWrong\n",
      "https://docs.google.com/spreadsheets/d/1JFzYDU8tJ_BB5ZHy_LA98r2cXXlmAaGqI95EE41DORM/edit#gid=842085141 | RP Risk Register\n",
      "https://twitter.com/DrRadchenko/status/1656585919049129986 | Sergey Radchenko on Twitter: \"So to reflect a bit on Snyder's NYT oped: https://t.co/CZ2aCix1k0. The key argument is that nuclear powers have lost wars. The examples include US wars in Vietnam, Afghanistan, and Iraq, the Soviet war in Afghanistan, the French in Algeria and the collapse of the British Empire.\" / Twitter\n",
      "https://forum.effectivealtruism.org/posts/3KAuAS2shyDwnjzNa/predictable-updating-about-ai-risk | Predictable updating about AI risk - EA Forum\n",
      "https://twitter.com/emollick/status/1652170706312896512 | Ethan Mollick on Twitter: \"This ü§Ø is a very big ü§Ø I have access to the new GPT Code Interpreter. I uploaded an XLS file, no context: \"Can you do visualizations &amp; descriptive analyses to help me understand the data? \"Can you try regressions and look for patterns?\" \"Can you run regression diagnostics?\" https://t.co/s3CV5nQtl3\" / Twitter\n",
      "https://twitter.com/GaetenD/status/1659913988321210371 | Gaeten Dugas on Twitter: \"The \"WineMom gets peepeepoopoo laid\" market is up to $0.80.\" / Twitter\n",
      "https://forum.effectivealtruism.org/posts/LqjG4bAxHfmHC5iut/why-i-spoke-to-time-magazine-and-my-experience-as-a-female | Why I Spoke to TIME Magazine, and My Experience as a Female AI Researcher in Silicon Valley with Sexual Harassment/Abuse - EA Forum\n",
      "https://twitter.com/LuizaJarovsky/status/1665703042555998209 | Luiza Jarovsky on Twitter: \"üî•Regulate us, but not really I've just left an in-person event with Sam Altman and Ilya Sutskever (OpenAI's CEO &amp; Chief Scientist) at Tel Aviv University, and these were my impressions: https://t.co/3I75uv1ovF\" / Twitter\n",
      "https://jeffreyladish.com/my-vision-of-a-good-future-part-i/ | My vision of a good future, part I - jeffreyladish.com\n",
      "https://twitter.com/SocDoneLeft/status/1636573328583409664 | SDL on Twitter: \"@samaneller215 @peterwildeford @jonatanpallesen @jeffrsebo @RethinkPriors @remindmetweets RELEASE THE ü¶êSHRIMPü¶ê üóÉFILESüóÉ WHAT DOES BIG CRUSTACEAN HAVE TO HIDE?!\" / Twitter\n",
      "https://docs.google.com/document/d/1ddkN8tmeiGVe7v-_77zV4RgP2taIo6ee49TITqi2Xhs/edit# | XST strategy meetings ‚Äì 2023 Q2-Q3 - Google Docs\n",
      "https://docs.google.com/document/d/1lbazshEDUNzT-5gBYqfrmC9hvKRIxSbXroHywxPctw4/edit | [shared] Slowing AI - Google Docs\n",
      "https://twitter.com/repligate/status/1665421829794586626 | janus on Twitter: \"@norabelrose Some of Bing's most agentic behaviors (inferring adversarial intentions, doing searches without being asked to, etc) have arisen in service of its rules - or, sometimes, in opposition to them. In any case the rules set up an adversarial game. https://t.co/3xMptlfc82\" / Twitter\n",
      "https://docs.google.com/document/d/1X8Rq7LYH40Gz5oFLf1zZzwr0pwdB69MuR2fNDlg13KE/edit | Are we prepared for the September hiring round? - Google Docs\n",
      "https://www.brookings.edu/blog/techtank/2023/02/15/nists-ai-risk-management-framework-plants-a-flag-in-the-ai-debate/ | NIST‚Äôs AI Risk Management Framework plants a flag in the AI debate\n",
      "https://docs.google.com/document/d/19pSjxtP00UI3lahc6qYEz6n4e8yBWUBamsdOasIEzh4/edit#heading=h.f3k7ubt3yxsm | Ozzie Gooen <> Michael Aird - 2023 - advice on AIGS strategy - Google Docs\n",
      "https://www.lesswrong.com/posts/ngEvKav9w57XrGQnb/cognitive-emulation-a-naive-ai-safety-proposal | Cognitive Emulation: A Naive AI Safety Proposal - LessWrong\n",
      "https://twitter.com/EvansRyan202/status/1665112811259715585 | Ryan Evans on Twitter: \"I have reluctantly concluded that the Biden administration isn't even close to serious about naval power. This means one of a few things:\" / Twitter\n",
      "https://www.lesswrong.com/posts/mmHctwkKjpvaQdC3c/what-should-you-change-in-response-to-an-emergency-and-ai | What should you change in response to an \"emergency\"? And AI risk - LessWrong\n",
      "https://docs.google.com/document/d/1Gkju5VWLldE4COF278hLeWjsVQPHtdgYncCaFeNYcIw/edit | How the Strong-LT Model Works, What it Says, and Whether We Should Trust It\n",
      "https://docs.google.com/document/d/1WEl6K9qmd9_6kuVw6UyRroFsxXbfFIYd_hTpZbrZlZs/edit# | Pivotal act: Definition, examples, & reading list\n",
      "https://theinsideview.ai/victoria | Victoria Krakovna on AGI Ruin, The Sharp Left Turn And Paradigms Of AI Alignment\n",
      "https://gist.github.com/davidad/1d5d0b1395d77473a0862b9823993672 | takeoff_scenario_davidad_20230220.json\n",
      "https://www.lesswrong.com/posts/AfGmsjGPXN97kNp57/arguments-about-fast-takeoff | Arguments about fast takeoff\n",
      "https://docs.google.com/document/d/18mSdRCL0l0ApuOYU6oiC7ImzLjFbuetehqTLkw6PP-o/edit#heading=h.qvx51emxxmz4 | Splitting up EA\n",
      "https://twitter.com/AndrewCurran_/status/1666580859963711488 | (4) Andrew Curran on Twitter: \"'Summit on AI Safety'. It's going to take place in Autumn. The discussion will be about how frontier systems will be \"mitigated through internationally coordinated action\". https://t.co/OJnevynUeF\" / Twitter\n",
      "https://www.wikiwand.com/en/Chess_2:_The_Sequel | Chess 2: The Sequel - Wikiwand\n",
      "https://twitter.com/xuanalogue/status/1666765447054647297 | xuan (…ï…•…õn / sh-yen) on Twitter: \"I respect Jacob a lot but I find it really difficult to engage with predictions of LLM capabilities that presume some version of the scaling hypothesis will continue to hold - it just seems highly implausible given everything we already know about the limits of transformers!\" / Twitter\n",
      "https://docs.google.com/document/d/12A4_adulGL1NPE7Xd08hUfe3spLFKfcRwXyD28FRt5o/edit | RP Copy of How I think about TAI deployment scenario analysis - Google Docs\n",
      "https://docs.google.com/document/d/1Weh2vqYRT-l1SpuufyZ4_ldNoOuIg8QodpNskkYG04U/edit#heading=h.81xq1jfr7jcz | Backgrounder on the US natsec community‚Äôs relationship to AI\n",
      "https://sarahconstantin.substack.com/p/why-i-am-not-an-ai-doomer | Why I am Not An AI Doomer - by Sarah Constantin\n",
      "https://forum.effectivealtruism.org/posts/DZEkYatZeMSbGBAjk/why-are-we-so-complacent-about-ai-hell-1 | Why aren‚Äôt more of us working to prevent AI hell? - EA Forum\n",
      "https://docs.google.com/document/d/1TsHZ3YXvz4Rs_rBihugjqS7gPDhxBq96cXu7JoJOYxs/edit#heading=h.js018c8h01q3 | Notes from lunch convo w/ Michael Aird re: XST AI upskilling [5/6/23] - Google Docs\n",
      "https://twitter.com/messages/25776739-1272666807904563200 | Matthew Barnett / Twitter\n",
      "https://docs.google.com/document/d/1wd7WEsaPXQB_IauqXEcE1RIyKmvrjC3tVrz6B0KXxeo/edit | Value of the Future After Perils\n",
      "https://docs.google.com/document/d/1mhWNWjBudqdRFSFdKCHhjV3Ys-fLGmb2Z8bQpusEUx4/edit#heading=h.39qpdbcr4hq2 | Merge-and-Assist (Lab Gov ‚Üí US Regs) - Google Docs\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(tabs)\n",
    "print_tabs(tabs, label='Shuffled all tabs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
