{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40568205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded\n",
      "405\n",
      "405\n",
      "364\n",
      "345\n",
      "315\n",
      "Loaded\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import webbrowser\n",
    "from copy import copy\n",
    "\n",
    "\n",
    "def print_tabs(tabs, label=None, shuffled=True):\n",
    "    if shuffled:\n",
    "        tabs = random.sample(tabs, len(tabs))\n",
    "    if label:\n",
    "        print('## {} ## ({} tabs)'.format(label, len(tabs)))\n",
    "    else:\n",
    "        print('({} tabs)'.format(len(tabs)))\n",
    "    print('')\n",
    "    for tab in tabs:\n",
    "        print(tab.replace('\\n', ''))\n",
    "    return None\n",
    "\n",
    "\n",
    "def open_tab(tab):\n",
    "    url = tab.split('|')[0].replace(' ', '')\n",
    "    webbrowser.open(url, new=2, autoraise=False)\n",
    "    \n",
    "    \n",
    "def open_tabs_from_text(tab_text):\n",
    "    tabs = tab_text.split('\\n')\n",
    "    print('{} tabs opened!'.format(len(tabs) - 2))\n",
    "    for t in tabs:\n",
    "        open_tab(t.split('|')[0].strip())\n",
    "        \n",
    "print('Loaded')\n",
    "\n",
    "tab_file = open('/Users/peterhurford/Documents/alltabs.txt', 'r')\n",
    "tabs = tab_file.readlines()\n",
    "print(len(tabs))\n",
    "\n",
    "tabs = [t for t in tabs if t != '\\n']\n",
    "print(len(tabs))\n",
    "\n",
    "tabs = sorted(list(set(tabs)))\n",
    "print(len(tabs))\n",
    "\n",
    "tabs = ['{} | {}'.format(k, v) for k, v in dict([(t.split('|')[0].strip(), ''.join(t.split('|')[1:]).strip()) for t in tabs]).items()]\n",
    "print(len(tabs))\n",
    "\n",
    "tabs = ['{} | {}'.format(v, k) for k, v in dict([(''.join(t.split('|')[1:]).strip(), t.split('|')[0].strip()) for t in tabs]).items()]\n",
    "print(len(tabs))\n",
    "\n",
    "print('Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d5fddf1-e942-4059-b414-48e28bdc58f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted tabs! (315)\n",
      "-\n",
      "https://ai.gov/apply/ | Join the National AI Talent Surge - AI.gov\n",
      "https://ailabwatch.org/ | Lab Watch\n",
      "https://aisafetysummit.gov.uk/#company-policies | AI Safety Summit AISS 2023\n",
      "https://aiscc.org/2023/11/01/yougov-poll-83-of-brits-demand-companies-prove-ai-systems-are-safe-before-release/ | YouGov poll: 83% of Brits demand companies prove AI systems are safe before release ‚Äì AI Safety Communications Centre\n",
      "https://aisnakeoil.com/p/evaluating-llms-is-a-minefield | Evaluating LLMs is a minefield\n",
      "https://aitreaty.org/ | Urging an International AI Treaty: An Open Letter  aitreaty.org\n",
      "https://anthropic.com/uk-government-internal-ai-safety-policy-response | Our response to the UK Government‚Äôs internal AI safety‚Ä¶ \\ Anthropic\n",
      "https://apnews.com/article/openai-chatgpt-investigation-federal-ftc-76c6218c506996942282d7f5d608088e | FTC investigating ChatGPT creator OpenAI over consumer protection issues  AP News\n",
      "https://arxiv.org/abs/2303.11341 | [2303.11341] What does it take to catch a Chinchilla? Verifying Rules on Large-Scale Neural Network Training via Compute Monitoring\n",
      "https://arxiv.org/abs/2305.15324 | [2305.15324] Model evaluation for extreme risks\n",
      "https://arxiv.org/abs/2307.03718 | [2307.03718] Frontier AI Regulation: Managing Emerging Risks to Public Safety\n",
      "https://arxiv.org/abs/2310.19736 | [2310.19736] Evaluating Large Language Models: A Comprehensive Survey\n",
      "https://arxiv.org/abs/2310.19737 | [2310.19737] Adversarial Attacks and Defenses in Large Language Models: Old and New Threats\n",
      "https://arxiv.org/abs/2310.20563 | [2310.20563] Taking control: Policies to address extinction risks from advanced AI\n",
      "https://arxiv.org/pdf/2306.06924.pdf | TASRA: a Taxonomy and Analysis of Societal-Scale Risks from AI\n",
      "https://assets.publishing.service.gov.uk/media/653aabbd80884d000df71bdc/emerging-processes-frontier-ai-safety.pdf | Emerging processes for frontier AI safety\n",
      "https://avidml.org/arva/ | AI Risk and Vulnerability Alliance  AVID\n",
      "https://aws.amazon.com/uki/cloud-services/uk-gov-ai-safety-summit/ | AI Safety Summit - Enhancing Frontier AI Safety\n",
      "https://bbc.com/news/technology-67172230 | Can Rishi Sunak‚Äôs big summit save us from AI nightmare? - BBC News\n",
      "https://benchmarks.llmonitor.com/ | LLMonitor Benchmarks\n",
      "https://blogs.microsoft.com/on-the-issues/2023/10/26/microsofts-ai-safety-policies/ | Microsoft‚Äôs AI Safety Policies - Microsoft On the Issues\n",
      "https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/ | What will GPT-2030 look like?\n",
      "https://builtin.com/artificial-intelligence/ftc-openai | FTC OpenAI Investigation  Built In\n",
      "https://cnas.org/press/in-the-news/can-america-handle-two-wars-and-maybe-a-third | Can America handle two wars, and maybe a third?  Center for a New American Security (en-US)\n",
      "https://cnas.org/press/noteworthy/noteworthy-executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence | NOTEWORTHY: Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence  Center for a New American Security (en-US)\n",
      "https://cnas.org/press/press-note/cnas-responds-white-house-executive-order-on-artificial-intelligence | CNAS Responds: White House Executive Order on Artificial Intelligence  Center for a New American Security (en-US)\n",
      "https://cnas.org/publications/reports/strengthening-the-shield | Strengthening the Shield  Center for a New American Security (en-US)\n",
      "https://concordia-consulting.com/wp-content/uploads/2023/10/State-of-AI-Safety-in-China.pdf | State of AI Safety in China\n",
      "https://constellation.org/programs/astra-fellowship | Astra Fellowship\n",
      "https://constellation.org/programs/researcher-program | Constellation Visiting Researcher Program\n",
      "https://cs.princeton.edu/~arvindn/talks/insight_forum_statement.pdf | Copy of The urgent need for accountability in predictive AI\n",
      "https://dam.gcsp.ch/files/doc/gcsp-geneva-paper-29-22 | gcsp-geneva-paper-29-22\n",
      "https://datasociety.net/library/ai-red-teaming-is-not-a-one-stop-solution-to-ai-harms-recommendations-for-using-red-teaming-for-ai-accountability/ | Data & Society ‚Äî AI Red-Teaming Is Not a One-Stop Solution to AI Harms: Recommendations for Using Red-Teaming for AI Accountability\n",
      "https://deepmind.google/public-policy/ai-summit-policies/ | AI Safety Summit: An update on our approach to safety and responsibility - Google DeepMind\n",
      "https://docs.google.com/document/d/10Ru6IjJrjWZy_gk5cfnoqCh653luZ19Ee0KE1IkuXuU/edit#heading=h.4h9qdgodgcz5 | [Working draft] Safeguarding the safeguards - Google Docs\n",
      "https://docs.google.com/document/d/15IRosp-aUqJ8dvMp9Zz1HNI3jjlX-l54iYynrKdbOdI/edit | ZW Thoughts - Responsibility Splits - Oct 2023 - Google Docs\n",
      "https://docs.google.com/document/d/15OerqofHlipOYys0h8ls2YqjLNCk2BwXKsETGlXINSo/edit | Surveys Squad Meeting Agendas - Google Docs\n",
      "https://docs.google.com/document/d/164f49D-XkH8XyEF0howZQx0HWPFUESB1X3st5hgnRQs/edit | Brainstorming & prioritizing potential uses of AIGS team budget - Google Docs\n",
      "https://docs.google.com/document/d/16sbBh7MRmXKrsD_oOLNfbMgB4Yn8jqoUwMq9IM0_dWs/edit | Manifund Regranting for Surveys (Branding EA vs Longtermism vs X-risk...) 2023-07 - Google Docs\n",
      "https://docs.google.com/document/d/19rKkgNqaMPRISPqjJGU48GfXxNzZYjrtds-FTI52Dlc/edit | Peter <> Michael - 1-1s - 2023 Q3 - Google Docs\n",
      "https://docs.google.com/document/d/1AyANUnE-yZl9y1qUDz2cJdoBsG8zzeM4Kr7QFC8NmBA/edit#heading=h.xxfi8rk39qa7 | IAPS all-hands meetings 2023 Oct - Google Docs\n",
      "https://docs.google.com/document/d/1Gc09Bhn73KvHuIvrpDQNj7jit1xnq1UlGSPxeGzqmLY/edit#heading=h.jxumr5giwybo | Mechanisms for making consumer GPUs less useful for AI training - Google Docs\n",
      "https://docs.google.com/document/d/1JKY9I1H3d9FJIEKOAjAklbNW0sXW54x-FJPDxU43tgQ/edit | Current USG Authority to Influence AI Development - Google Docs\n",
      "https://docs.google.com/document/d/1NHFNXfzMnKtaFUxByCXns27syAlhjjMqAP4co8y8Ue0/edit?userstoinvite=adam@bluedotimpact.org&sharingaction=manageaccess&role=writer | Peter's Reflections on Summit on Existential Security - Google Docs\n",
      "https://docs.google.com/document/d/1OnhAKi9kBtxL_8yd4PT5NN9oN8I3QxIC79uhd-yUtSs/edit#heading=h.62zbnk202lxw | Nov 2023 US EO on AI - Google Docs\n",
      "https://docs.google.com/document/d/1PE8eMfwTHehf56w1aFChv31Uw-4_rYKc7S_TfgCH0TQ/edit | Ben, Peter, Renan - Meeting Notes - Google Docs\n",
      "https://docs.google.com/document/d/1QufMubGOVuoOhY-WoNfNB0wRuxoKmlO70gksrUDbVaY/edit | Further responses to OP - Google Docs\n",
      "https://docs.google.com/document/d/1W_Ezc0-xj1_GanHVCHyTe6RvmlahzUMb1N5MlrGo9mw/edit#heading=h.qvxqhft5lddc | IAPS comments on Anth RSPs - Google Docs\n",
      "https://docs.google.com/document/d/1XVslWOt0wNVeEAhEe0e1PG9k6GWLMXckBi2NRHrWEBA/edit#heading=h.osty8jeclpyn | WAW risk aversion - Google Docs\n",
      "https://docs.google.com/document/d/1Ys2WVoiskLlzMyILJxir-1FZwd2Ffmb60FggX8FGXOU/edit#heading=h.7nuantk77hs | Marcus / Peter Post - Draft 1 - How Rethink Priorities is Addressing Risk and Uncertainty - Google Docs\n",
      "https://docs.google.com/document/d/1_qZEVs4SE-F2uWBIb0cxynjceO0LkhjrlXssJd-eM34/edit#heading=h.xhhs1xccdr60 | [WIP] Late-stage IR - Google Docs\n",
      "https://docs.google.com/document/d/1bhl9kVF1_LkLzOnEaI8MPrUnRsegI0jittLmc5tZZQI/edit#heading=h.v9pzfmksvnbf | AI Regs / Lab Gov Sync Up Call - Oct 2023 - Google Docs\n",
      "https://docs.google.com/document/d/1czt0fKR6S-JWw5_11TquXAAkE-hMu28Sl-4by-dCtsM/edit | Caro-Peter: Oxford, The Magical Retreat II - Google Docs\n",
      "https://docs.google.com/document/d/1e8-P-DI_9su6fTT7U8_L4Ku-PewxcveQFQOLGzbCKso/edit | Caro and Peter's Chocolatey Adventure in Brussels: The AI and Waffle Chronicles - Google Docs\n",
      "https://docs.google.com/document/d/1eq2BPX9MSL-onZEuzD39wK4B6cb3VH6ffw_ixN05upo/edit#heading=h.lbj5xicd7zt7 | AISCC Comms Tactics: UK Summit - Google Docs\n",
      "https://docs.google.com/document/d/1fqmabo6ANKocghsmJ4bicXys2TsbAHgiRwwMTRkoe2k/edit | Peter / Amanda / Zoe Responsibilities Call - Oct 2023 - Google Docs\n",
      "https://docs.google.com/document/d/1gNHs1tEw6btzxYqFZIyDVyVlWa-L4rmdaHUG7-TNl0I/edit | Close Read of \"Keynote Remarks by U/S Jenkins (T) to the Summit on Responsible Artificial Intelligence in the Military Domain (REAIM) Ministerial Segment\" - Google Docs\n",
      "https://docs.google.com/document/d/1hR-LNaK54Rz94Xtu5chrHvRptM7UUMux_eYEu3oC7io/edit | WIT Feedback - Google Docs\n",
      "https://docs.google.com/document/d/1i1KAfwLU5G1fyjO82DJyRVrtTJHR5jPcCKjwjQ0scC4/edit#heading=h.72mwfkgg6lcn | *XST strategy meetings ‚Äì 2023 Q3 - Google Docs\n",
      "https://docs.google.com/document/d/1lKzgAC7VQu1mVC7DAZACFU4VVsYjRjAc2rSFuCHW_A0/edit | Untitled document - Google Docs\n",
      "https://docs.google.com/document/d/1lP0XF_ncfIF5_hBitLW5wlfCV0W_xhYgeNzZdD2ZkVg/edit#heading=h.v21njykqjfyn | Collection: All FRAP conversation notes - Google Docs\n",
      "https://docs.google.com/document/d/1m_-XgZgBs0LZHplodgBbGcpiGtNWalPxPBjGOwF6rig/edit | Macrocalendar - Google Docs\n",
      "https://docs.google.com/document/d/1oD0tQuVkj0lWWVjbCy27tLfC2TjDJfRTUloh4sFSwQQ/edit | Executive Research Assistant - Interview - Google Docs\n",
      "https://docs.google.com/document/d/1qjoLVsStP4iktkXcYP5oKSIM20FEWDcS/edit | 360 Reflection_Peter Wildeford.docx - Google Docs\n",
      "https://docs.google.com/document/d/1rwM5tY8yS1YIhhfVOqUg868aZDWuHD-SOld4AMqCF4s/edit | Quarterly Review, Plan for 2023 Sep-Nov - Google Docs\n",
      "https://docs.google.com/document/d/1vMSoRvzgF2VLcBcjqHTvQ2NXHRqVXsNyT4XI54wk79Q/edit | Peter/Ben 1-1s - Google Docs\n",
      "https://docs.google.com/document/d/1zBS3J3R4sKrgLynZDbCIn2u5AdvOr3wcGcnglZ6HSSY/edit#heading=h.fi10dekku7ku | Evidence on the impact/quality of AIGS's work so far - Google Docs\n",
      "https://docs.google.com/document/d/1ziNrskp-v_jWihUakPIhSLqdu6WJY-mA0152RUcLqQc/edit | IAPS Leads notes (Michael, Peter, Zoe, sometimes Amanda/Ashwin) - 2023 May-Oct - Google Docs\n",
      "https://docs.google.com/document/d/1zm4-Uga394tfrkX5Xj2oO6K2vhL_Nxj-7YyiL7Zixj4/edit | Potential AI Forecasting Qs - Google Docs\n",
      "https://docs.google.com/document/u/0/d/1os_4YOw6Xv33KjX-kR76D3kW1drkWRHKG2caeiEWzNs/mobilebasic | Part 0 - What a compute-centric framework says about takeoff speeds: short summary and long summary\n",
      "https://docs.google.com/presentation/d/1NjE49tDpSbE0FiXwroxpW_spY3TkKxkNDknxBvsndO8/edit#slide=id.g208ea025851_0_304 | [latest] 2023 Nov RP Fundraising Forecast - Google Slides\n",
      "https://docs.google.com/spreadsheets/d/1-iyQ2q3vH6eVQnTh7BnSM9l01VFCWNFnr4TTs658RUg/edit#gid=804926468 | IAPS budget [Aug 2023 OP application] - Google Sheets\n",
      "https://docs.google.com/spreadsheets/d/1AAIebjNsnJj_uKALHbXNfn3_YsT6sHXtCU0q7OIPuc4/edit#gid=0 | Parameter, Compute and Data Trends in Machine Learning - Google Sheets\n",
      "https://docs.google.com/spreadsheets/d/1FROfNCchRS8nYmPSkwsBF6V5ncQTaeMvjbB_5tkkde8/edit#gid=1708690655 | Program Service Revenue Tracking - Overall - Google Sheets\n",
      "https://docs.google.com/spreadsheets/d/1RFlb6tuqYH71ljpmqq1Y7BhbU6Cfy9CrMLAE-gXEhug/edit#gid=0 | Big rocks weekly planning - Google Sheets\n",
      "https://docs.google.com/spreadsheets/d/1TlWcxy-fuzXd93DEJ_sj8R6Ikm4HJMZd92sXULbnZvM/edit#gid=0 | [very confidential] Staff Risk - Google Sheets\n",
      "https://docs.google.com/spreadsheets/d/1VgX8Tq6VIwpbEUHHSem97enDeyUll2Fbd3uJu107hts/edit#gid=0 | SFF + Lightspeed details - Google Sheets\n",
      "https://docs.google.com/spreadsheets/d/1_S_OPoTpFB07sjPDEhZ-g3kGj7wE4nfflNFTculL_BE/edit#gid=1551801213 | RP Fundraising Forecast [2023 + 2024 predictions] - Google Sheets\n",
      "https://docs.google.com/spreadsheets/d/1b8TfmWMxoHnOTFAlEfqJQZlvJo0487lPqqb-Z4NOoys/edit#gid=0 | [master copy] IAPS budget [for Aug 2023 fundraising] - Google Sheets\n",
      "https://docs.google.com/spreadsheets/d/1fk6jAn4rpbx1T9kKYl-3C9TJHQA8ZC6lFNpT5QawNbs/edit#gid=1158312132 | 2023 Stats - Google Sheets\n",
      "https://docs.google.com/spreadsheets/d/1hCOQy0D0xTCQeXYG_E6QbiHnQTU5wD_K4lDi9PzikCs/edit#gid=0 | [shared] Comparing \"Emerging Processes for Frontier AI Safety\" to companies' AI Safety Policies - Google Sheets\n",
      "https://docs.google.com/spreadsheets/d/1kxEwC9-ZGQjEa1qrMuc1RIjTKs8w0Jdg5l3Rf5LxELw/edit#gid=0 | First common budget! - Google Sheets\n",
      "https://docs.google.com/spreadsheets/d/1nZu1kxcmkiQ22tjx9pFwW8fOcuXsHBklBDVHBU4wDjw/edit#gid=539134426 | RP Revenue by Dept - Google Sheets\n",
      "https://docs.google.com/spreadsheets/d/1oWHhbn19dom2vW6tx8fCSfLjSAyQHIv1daI4wKDYv90/edit#gid=0 | Accomodation in Brussels - Google Sheets\n",
      "https://docs.google.com/spreadsheets/d/1p2kKN_7CxVDJlosPQTWwCePNpZzKlG15N1ZJWlOyehQ/edit#gid=0 | 2023-08 EAIF for Extra EA Survey budget - Google Sheets\n",
      "https://docs.google.com/spreadsheets/d/1qEsXG6I6hR2rhYxnuAtpHI-YcCAyPNt1uNYK5LT7CPc/edit#gid=791158494 | Peter's ERA - Rubric - 2023 - Google Sheets\n",
      "https://docs.google.com/spreadsheets/d/1sJlC9PLWGxQ6XQsvPVybnJtuKjNW7iKmYv_WqZ3cjXQ/edit#gid=1605902912 | DRAFTING RP 2024 Draft Budget - Google Sheets\n",
      "https://docs.google.com/spreadsheets/d/1vS1D40zwEsA6oBtnXnxWd0juDbDV7D5k4edJRd0tsEY/edit#gid=875232347 | RP Net Assets - Google Sheets\n",
      "https://doingwestminsterbetter.substack.com/p/will-ai-safety-policy-survive-rishi | Will AI safety policy survive Rishi Sunak?\n",
      "https://evals.alignment.org/blog/2023-08-01-new-report/ | New report: Evaluating Language-Model Agents on Realistic Autonomous Tasks - ARC Evals\n",
      "https://facebook.com/groups/1062957250383195/?multi_permalinks=7276436365701888&hoisted_section_header_type=recently_seen | Effective Altruism Job Postings  Facebook\n",
      "https://facebook.com/spencer.greenberg/posts/pfbid0n55WBJXUipuDyuFu1BDzYxKoEx44wGnb7qi68D5UghKK6KoYADoCMc12Y9fAecw5l | People often talk about how women are... - Spencer Greenberg  Facebook\n",
      "https://flf.org/ | The Future of Life Foundation\n",
      "https://fmprc.gov.cn/mfa_eng/wjdt_665385/2649_665393/202310/t20231020_11164834.html | Global AI Governance Initiative\n",
      "https://foreignaffairs.com/united-states/henry-kissinger-path-artificial-intelligence-arms-control | Henry Kissinger: The Path to AI Arms Control\n",
      "https://foreignpolicy.com/2023/10/07/cloud-computing-artificial-intelligence-chips-sanctions-us-china/ | The Cloud Can Solve America's AI Problems\n",
      "https://forum.effectivealtruism.org/ | Effective Altruism Forum\n",
      "https://forum.effectivealtruism.org/editPost?postId=unFycWDoyDHdHQGT5 | How Rethink Priorities is Addressing Risk and Uncertainty ‚Äî EA Forum\n",
      "https://forum.effectivealtruism.org/giving-portal | Giving portal ‚Äî EA Forum\n",
      "https://forum.effectivealtruism.org/posts/3qpaRKe8R4ptiqSkr/uk-prime-minister-rishi-sunak-s-speech-on-ai | UK Prime Minister Rishi Sunak's Speech on AI ‚Äî EA Forum\n",
      "https://forum.effectivealtruism.org/posts/7SjtFYo6sCe3588Tx/why-scale-is-overrated-the-case-for-increasing-ea-policy | Why scale is overrated: The case for increasing EA policy efforts in smaller countries ‚Äî EA Forum\n",
      "https://forum.effectivealtruism.org/posts/GW3cxBurTNKHs352S/controlling-for-a-thinker-s-big-idea | Controlling for a thinker‚Äôs big idea ‚Äî EA Forum\n",
      "https://forum.effectivealtruism.org/posts/NrqGyXzvwB2Gqu6XW/state-of-the-east-and-southeast-asian-eacosystem | State of the East and Southeast Asian EAcosystem ‚Äî EA Forum\n",
      "https://forum.effectivealtruism.org/posts/S9H86osFKhfFBCday/how-bad-would-human-extinction-be | How bad would human extinction be? ‚Äî EA Forum\n",
      "https://forum.effectivealtruism.org/posts/TurdGigfQKwxBkKnb/estimating-ea-growth-rates-mcf-memo-1 | Estimating¬†EA Growth Rates¬†(MCF memo) ‚Äî EA Forum\n",
      "https://forum.effectivealtruism.org/posts/WfodoyjePTTuaTjLe/efficacy-of-ai-activism-have-we-ever-said-no?utm_source=EA+Forum+Digest&utm_campaign=1771e9b7a6-EMAIL_CAMPAIGN_2023_11_01_08_45&utm_medium=email&utm_term=0_-1771e9b7a6-%5BLIST_EMAIL_ID%5D | Efficacy of AI Activism: Have We Ever Said No? ‚Äî EA Forum\n",
      "https://forum.effectivealtruism.org/posts/ZuzK2s4JsJcexBJxy/will-releasing-the-weights-of-large-language-models-grant | Will releasing the weights of large language models grant widespread access to pandemic agents? ‚Äî EA Forum\n",
      "https://forum.effectivealtruism.org/posts/cSTxmWadcdFayEhAE/tom-barnes-s-quick-takes?commentId=zXsgvqzXAGmrsXmLG | Tom Barnes's Quick takes ‚Äî EA Forum\n",
      "https://forum.effectivealtruism.org/posts/dpjCwMwKEPqK3TPnC/notes-on-managing-to-change-the-world | Notes on \"Managing to Change the World\" ‚Äî EA Forum\n",
      "https://forum.effectivealtruism.org/posts/g4fXhiJyj6tdBhuBK/survey-on-intermediate-goals-in-ai-governance | Survey on intermediate goals in AI governance ‚Äî EA Forum\n",
      "https://forum.effectivealtruism.org/posts/hAzhyikPnLnMXweXG/participate-in-the-donation-election-and-the-first-weekly | Participate in the Donation Election and the first weekly theme (starting 7 November) ‚Äî EA Forum\n",
      "https://forum.effectivealtruism.org/posts/hDNpHEA2Kn4xBoS8r/impact-evaluation-in-ea | Impact Evaluation in EA ‚Äî EA Forum\n",
      "https://forum.effectivealtruism.org/posts/hFPbe2ZwmB9athsXT/clean-water-the-incredible-30-mortality-reducer-we-can-t | Clean Water - the incredible 30% mortality reducer we can‚Äôt explain ‚Äî EA Forum\n",
      "https://forum.effectivealtruism.org/posts/j2TreuRZT9mBFEMEs/the-bletchley-declaration-on-ai-safety | The Bletchley Declaration on AI Safety ‚Äî EA Forum\n",
      "https://forum.effectivealtruism.org/posts/jCwuozHHjeoLPLemB/how-long-do-policy-changes-matter-new-paper | How Long Do Policy Changes Matter? New Paper ‚Äî EA Forum\n",
      "https://forum.effectivealtruism.org/posts/pniDWyjc9vY5sjGre/rethink-priorities-cross-cause-cost-effectiveness-model?commentId=N98gLkDi87yJZpdaf | Rethink Priorities‚Äô Cross-Cause Cost-Effectiveness Model: Introduction and Overview ‚Äî EA Forum\n",
      "https://forum.effectivealtruism.org/posts/t5x9xLew2ZeENAzzT/let-s-celebrate-some-wins | Let's celebrate some wins ‚Äî EA Forum\n",
      "https://forum.effectivealtruism.org/posts/vQFBtHqgcJAwPpwEu/improving-the-welfare-of-ais-a-nearcasted-proposal | Improving the Welfare of AIs: A Nearcasted Proposal ‚Äî EA Forum\n",
      "https://forum.effectivealtruism.org/posts/vq5pHzrxLgABAwkhD/shrimp-paste-might-consume-more-animal-lives-than-any-other | Shrimp paste might cause more animal deaths than any other food product. Who‚Äôs working on this? ‚Äî EA Forum\n",
      "https://forum.effectivealtruism.org/posts/vwJcuyb8wCwXFktJS/ea-infrastructure-fund-june-2023-grant-recommendations | EA Infrastructure Fund: June 2023 grant recommendations ‚Äî EA Forum\n",
      "https://forum.effectivealtruism.org/posts/zaSagsDjRmStfJ7MW/responsible-scaling-policies-are-risk-management-done-wrong | Responsible Scaling Policies Are Risk Management Done Wrong ‚Äî EA Forum\n",
      "https://forum.effectivealtruism.org/s/WdL3LE5LHvTwWmyqj/p/LCfd56cBeRzrMiAhw | Is x-risk the most cost-effective if we count only the next few generations? ‚Äî EA Forum\n",
      "https://forum.effectivealtruism.org/s/WdL3LE5LHvTwWmyqj/p/i5cuLZH3SQJigiHMs | Charting the precipice: The time of perils and prioritizing x-risk ‚Äî EA Forum\n",
      "https://futureoflife.org/our-work/grantmaking-work/ | Grantmaking work - Future of Life Institute\n",
      "https://futureoflife.org/our-work/policy-work/ | Policy work - Future of Life Institute\n",
      "https://futureoflife.org/project/artificial-escalation/ | Artificial Escalation - Future of Life Institute\n",
      "https://futureoflife.org/project/eu-ai-act/ | Strengthening the European AI Act - Future of Life Institute\n",
      "https://futureoflife.org/wp-content/uploads/2023/04/FLI_Policymaking_In_The_Pause.pdf | FLI_Policymaking_In_The_Pause.pdf\n",
      "https://google.com/search?q=bayh+dole+act&rlz=1CDGOYI_enUS715US715&oq=bay+doyle+act&gs_lcrp=EgZjaHJvbWUqCQgBEAAYChiABDIGCAAQRRg5MgkIARAAGAoYgAQyCQgCEAAYChiABDIJCAMQABgKGIAEMgkIBBAAGAoYgAQyCQgFEAAYChiABDIJCAYQABgKGIAEMgkIBxAAGAoYgAQyCQgIEAAYChiABDIJCAkQABgKGIAE0gEIMzA4M2oxajeoAgCwAgA&hl=en-US&sourceid=chrome-mobile&ie=UTF-8 | bayh dole act - Google Search\n",
      "https://google.com/search?q=hiroshima+g7+process&rlz=1CDGOYI_enUS715US715&oq=hiroshima+g7+process&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIICAEQABgWGB4yCAgCEAAYFhgeMgoIAxAAGIYDGIoFMgoIBBAAGIYDGIoFMgoIBRAAGIYDGIoFMgoIBhAAGIYDGIoF0gEINjcwMmowajSoAgCwAgA&hl=en-US&sourceid=chrome-mobile&ie=UTF-8 | hiroshima g7 process - Google Search\n",
      "https://google.com/search?q=pentagon+fire+deepfake&rlz=1CDGOYI_enUS715US715&oq=pentagon+fire+deepfake&gs_lcrp=EgZjaHJvbWUyBggAEEUYOdIBCDg0NTlqMGo3qAIAsAIA&hl=en-US&sourceid=chrome-mobile&ie=UTF-8 | pentagon fire deepfake - Google Search\n",
      "https://google.com/search?q=tent+pole+advocacy&rlz=1CDGOYI_enUS715US715&oq=tent+pole+advocacy&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIHCAEQABiiBDIHCAIQABiiBDIHCAMQABiiBDIHCAQQABiiBNIBCTIxODI2ajBqN6gCALACAA&hl=en-US&sourceid=chrome-mobile&ie=UTF-8 | tent pole advocacy - Google Search\n",
      "https://gov.uk/government/news/leading-frontier-ai-companies-publish-safety-policies | Leading frontier AI companies publish safety policies - GOV.UK\n",
      "https://gov.uk/government/publications/emerging-processes-for-frontier-ai-safety/emerging-processes-for-frontier-ai-safety | Emerging processes for frontier AI safety - GOV.UK\n",
      "https://gov.uk/government/publications/frontier-ai-capabilities-and-risks-discussion-paper | Future Risks of Frontier AI [UK gov]\n",
      "https://gov.uk/government/publications/frontier-ai-taskforce-second-progress-report/frontier-ai-taskforce-second-progress-report | Frontier AI Taskforce: second progress report - GOV.UK\n",
      "https://gov.uk/government/publications/international-survey-of-public-opinion-on-ai-safety | International survey of public opinion on AI safety - GOV.UK\n",
      "https://gov.uk/search/all?order=updated-newest&topical_events%5B%5D=ai-safety-summit-2023 | gov.uk/search/all?order=updated-newest&topical_events%5B%5D=ai-safety-summit-2023\n",
      "https://guarded-everglades-89687.herokuapp.com/?url=&title=&aggregator=-Custom&before=&after=&page=1&sort=&starred= | Upcoming Links\n",
      "https://gufaculty360.georgetown.edu/s/contact/00336000014U0TKAA0/ben-buchanan | Ben Buchanan: Georgetown University\n",
      "https://jefftk.com/p/computational-approaches-to-pathogen-detection | Computational Approaches to Pathogen Detection\n",
      "https://jefftk.com/p/examples-of-superintelligence-risk | Examples of Superintelligence Risk\n",
      "https://jefftk.com/p/public-weights | Public Weights?\n",
      "https://lcfi.ac.uk/news-and-events/news/2023/oct/31/ai-safety-policies/ | Site is not secure\n",
      "https://lesswrong.com/posts/6dn6hnFRgqqWJbwk9/deception-chess-game-1 | Deception Chess: Game #1 ‚Äî LessWrong\n",
      "https://lesswrong.com/posts/9Fdd9N7Escg3tcymb/preventing-language-models-from-hiding-their-reasoning | Preventing Language Models from hiding their reasoning ‚Äî LessWrong\n",
      "https://lesswrong.com/posts/AocXh6gJ9tJC2WyCL/book-review-going-infinite#Where_Was_This_Guy_ | Book Review: Going Infinite ‚Äî LessWrong\n",
      "https://lesswrong.com/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1 | Model Organisms of Misalignment: The Case for a New Pillar of Alignment Research ‚Äî LessWrong\n",
      "https://lesswrong.com/posts/EaZghEwcCJRAuee66/my-thoughts-on-the-social-response-to-ai-risk | My thoughts on the social response to AI risk ‚Äî LessWrong\n",
      "https://lesswrong.com/posts/J9eF4nA6wJW6hPueN/the-6d-effect-when-companies-take-risks-one-email-can-be | The 6D effect: When companies take risks, one email can be very powerful. ‚Äî LessWrong\n",
      "https://lesswrong.com/posts/JcLhYQQADzTsAEaXd/ai-as-a-science-and-three-obstacles-to-alignment-strategies | AI as a science, and three obstacles to alignment strategies ‚Äî LessWrong\n",
      "https://lesswrong.com/posts/LhxHcASQwpNa3mRNk/untrusted-smart-models-and-trusted-dumb-models | Untrusted smart models and trusted dumb models ‚Äî LessWrong\n",
      "https://lesswrong.com/posts/Nwgdq6kHke5LY692J/alignment-by-default | Alignment By Default ‚Äî LessWrong\n",
      "https://lesswrong.com/posts/PvBpRu354uG7ypwRP/on-the-executive-order | On the Executive Order ‚Äî LessWrong\n",
      "https://lesswrong.com/posts/SseoT9mKDTL3RCbE9/vaniver-s-thoughts-on-anthropic-s-rsp | Vaniver's thoughts on Anthropic's RSP ‚Äî LessWrong\n",
      "https://lesswrong.com/posts/a3RjXa2dryoH6Xgij/managing-ai-risks-in-an-era-of-rapid-progress | Managing AI Risks in an Era of Rapid Progress ‚Äî LessWrong\n",
      "https://lesswrong.com/posts/cCbybRT8bgiMbEHEv/a-list-of-all-the-deadlines-in-biden-s-executive-order-on-ai#comments | A list of all the deadlines in Biden's Executive Order on AI ‚Äî LessWrong\n",
      "https://lesswrong.com/posts/eJ7pm7LahehddYxNw/eli5-why-isn-t-alignment-easier-as-models-get-stronger | ELI5 Why isn't alignment *easier* as models get stronger? ‚Äî LessWrong\n",
      "https://lesswrong.com/posts/gQyphPbaLHBMJoghD/comp-sci-in-2027-short-story-by-eliezer-yudkowsky | Comp Sci in 2027 (Short story by Eliezer Yudkowsky) ‚Äî LessWrong\n",
      "https://lesswrong.com/posts/jyM7MSTvy8Qs6aZcz/what-s-up-with-responsible-scaling-policies | What's up with \"Responsible Scaling Policies\"? ‚Äî LessWrong\n",
      "https://lesswrong.com/posts/ms3x8ngwTfep7jBue/thoughts-on-the-ai-safety-summit-company-policy-requests-and | Thoughts on the AI Safety Summit company policy requests and responses ‚Äî LessWrong\n",
      "https://lesswrong.com/posts/tyE4orCtR8H9eTiEr/stuxnet-not-skynet-humanity-s-disempowerment-by-ai | Stuxnet, not Skynet: Humanity's disempowerment by AI ‚Äî LessWrong\n",
      "https://lesswrong.com/posts/vFqa8DZCuhyrbSnyx/integrity-in-ai-governance-and-advocacy | Integrity in AI Governance and Advocacy ‚Äî LessWrong\n",
      "https://lesswrong.com/posts/vm7FRyPWGCqDHy6LF/dario-amodei-s-prepared-remarks-from-the-uk-ai-safety-summit | Dario Amodei‚Äôs prepared remarks from the UK AI Safety Summit, on Anthropic‚Äôs Responsible Scaling Policy ‚Äî LessWrong\n",
      "https://linkedin.com/in/will-poff-webster-a235964b/?original_referer=https%3A%2F%2Fgoogle.com%2F | (86) Will Poff-Webster  LinkedIn\n",
      "https://listennotes.com/podcasts/the-dynamist/episode-41-chip-wars-china-BWDRwPH1SG3/ | Episode 41: Chip Wars, China, & Compute Governance w/ Onni Aarne & Erich Grunewald  Listen Notes\n",
      "https://localhost:8888/lab/tree/Tab%20sorts.ipynb | JupyterLab\n",
      "https://localhost:8889/lab/tree/Fundraising%20Forecast%20Lite.ipynb | Fundraising ‚Ä¶ - JupyterLab\n",
      "https://localhost:8890/lab/tree/(3C)%20When%20TAI%3F.ipynb | (3C) When TA‚Ä¶ (2) - JupyterLab\n",
      "https://mail.google.com/mail/u/0/#inbox | Inbox (46) - peter@peterhurford.com - Peter Hurford Mail\n",
      "https://mail.google.com/mail/u/1/#inbox | Inbox - peter@rethinkpriorities.org - Rethink Priorities Mail\n",
      "https://mail.google.com/mail/u/2/#inbox | Inbox - peter@iaps.ai - Institute for AI Policy and Strategy Mail\n",
      "https://managing-ai-risks.com/ | Managing AI Risks in an Era of Rapid Progress\n",
      "https://meritalk.com/articles/fcc-prepares-for-splash-into-ai-regulatory-waters/ | FCC Prepares for Splash Into AI Regulatory Waters ‚Äì MeriTalk\n",
      "https://metaculus.com/ai/ | The Metaculus Lens on AI\n",
      "https://metaculus.com/questions/11608/self-driving-taxis-available-to-metaculites/?sub-question=5306 | Self-Driving Taxis Available to Metaculites  Metaculus\n",
      "https://metaculus.com/questions/12539/pierre-poilievre-pm-of-canada-before-2026/ | Pierre Poilievre PM of Canada before 2026?  Metaculus\n",
      "https://metaculus.com/questions/12937/greatest-computation-used-in-ai-training/ | Greatest Computation Used in AI Training  Metaculus\n",
      "https://metaculus.com/questions/16639/non-democratic-brontier-ai-lab-before-2026/ | Non-Democratic Frontier AI Lab before 2026?  Metaculus\n",
      "https://metaculus.com/questions/18825/us-2024-election-balance-of-power/ | US 2024 election balance of power  Metaculus\n",
      "https://metaculus.com/questions/19492/israel-offensive-in-gaza-by-november-1-2023/ | Israel Offensive in Gaza by November 1, 2023?  Metaculus\n",
      "https://metaculus.com/questions/19643/number-of-hostages-freed-by-2024/ | Number of Hostages Freed by 2024?  Metaculus\n",
      "https://metaculus.com/questions/19700/additional-russian-ipo-in-2023/ | Additional Russian IPO in 2023?  Metaculus\n",
      "https://metaculus.com/questions/19702/foreign-intervention-in-the-israel-gaza-war/ | Foreign intervention in the Israel-Gaza War  Metaculus\n",
      "https://metaculus.com/questions/19764/second-starship-integrated-test-before-2024/ | Second Starship Integrated Test Before 2024?  Metaculus\n",
      "https://metaculus.com/questions/19765/donald-trump-jailed-before-2024/ | Donald Trump Jailed Before 2024?  Metaculus\n",
      "https://metaculus.com/tournament/quarterly-cup/ | üèÜ Quarterly Cup üèÜ  Metaculus\n",
      "https://nature.com/articles/d41586-023-03272-3 | AI ‚Äòbreakthrough‚Äô: neural net has human-like ability to generalize language\n",
      "https://oecd.ai/en/wonk/athens-roundtable-2023?ct=t(EMAIL_CAMPAIGN_1NOV23) | Bridging the gap in AI governance and the rule of law: The Athens Roundtable 2023 - OECD.AI\n",
      "https://omidyar.com/omidyar_team/troy-perry/ | Troy Perry - Omidyar Network\n",
      "https://omidyar.com/wp-content/uploads/2020/09/ON_POV_PDF-2020.pdf | ON_POV_PDF-2020.pdf\n",
      "https://omidyar.com/wp-content/uploads/2020/09/Omidyar-Network-POV_Platforms-and-Power_2.7.2020.pdf | Omidyar-Network-POV_Platforms-and-Power_2.7.2020.pdf\n",
      "https://onlinelibrary.wiley.com/doi/full/10.1111/phpr.13006 | Rational risk‚Äêaversion: Good things come to those who weight - Bottomley - Philosophy and Phenomenological Research - Wiley Online Library\n",
      "https://openai.com/global-affairs/our-approach-to-frontier-risk | OpenAI‚Äôs Approach to Frontier Risk\n",
      "https://openphilanthropy.org/about/team/benjamin-tereick/ | Benjamin Tereick  Open Philanthropy\n",
      "https://openphilanthropy.org/research/europes-animal-welfare-reforms-are-under-threat/ | Europe‚Äôs animal welfare reforms are under threat  Open Philanthropy\n",
      "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4579773 | International AI Institutions: A Literature Review of Models, Examples, and Proposals by Matthijs M. Maas, Jos√© Jaime Villalobos :: SSRN\n",
      "https://pbs.twimg.com/media/F97ifwdW0AAzRZw?format=jpg&name=4096x4096 | F97ifwdW0AAzRZw (2366√ó1660)\n",
      "https://politico.com/news/magazine/2023/11/02/bruce-reed-ai-biden-tech-00124375 | The Man Behind Biden‚Äôs Sweeping AI Executive Order - POLITICO\n",
      "https://punchbowl.news/wp-content/uploads/Canvass-AI.pdf | punchbowl.news/wp-content/uploads/Canvass-AI.pdf\n",
      "https://rand.org/pubs/testimonies/CTA2654-1.html | Challenges to U.S. National Security and Competitiveness Posed by AI  RAND\n",
      "https://rand.org/pubs/testimonies/CTA2723-1.html | Artificial Intelligence: Challenges and Opportunities for the Department of Defense  RAND\n",
      "https://rand.org/pubs/testimonies/CTA2824-1.html | Advancing Trustworthy Artificial Intelligence  RAND\n",
      "https://rand.org/pubs/testimonies/CTA2953-1.html | Preparing the Federal Response to Advanced Technologies  RAND\n",
      "https://rand.org/pubs/working_papers/WRA2849-1.html | Securing Artificial Intelligence Model Weights: Interim Report  RAND\n",
      "https://reddit.com/r/ChatGPT/comments/17hbx8f/prompt_challenge_can_you_get_chatgpt_to_generate/ | reddit.com/r/ChatGPT/comments/17hbx8f/prompt_challenge_can_you_get_chatgpt_to_generate/\n",
      "https://redfin.com/news/november-layoff/#:~:text=To%20every%20departing%20employee%20who,More%20information%20is%20on%20Fin. | All-Hands Email on November Layoff - Redfin Real Estate News\n",
      "https://rethinkpriorities.org/research | Research ‚Äî Rethink Priorities\n",
      "https://safer-ai.org/ | SaferAI\n",
      "https://saidtwice.substack.com/ | Said Twice  Eirin Evjen  Substack\n",
      "https://simonwillison.net/2023/Oct/17/open-questions/#open-questions.036.jpeg | Open questions for AI engineering\n",
      "https://simonwillison.net/2023/Oct/26/add-a-walrus/ | Now add a walrus: Prompt engineering in DALL-E 3\n",
      "https://sites.google.com/existential-security.com/2024summit/home | 2024 Summit\n",
      "https://stripe.com/newsroom/news/ceo-patrick-collisons-email-to-stripe-employees | CEO Patrick Collison's email to Stripe employees\n",
      "https://taigarchive.com/documents/a-blueprint-for-the-european-ai-office | A Blueprint for the European AI Office - TAIGA\n",
      "https://taigarchive.com/documents/estimating-the-implications-of-advanced-ai-for-automating-chip-design | Estimating the implications of advanced AI for automating chip design - TAIGA\n",
      "https://taigarchive.com/documents/navigating-ai-the-big-picture-(may-2023-slides) | Navigating AI ‚Äî the big picture (May 2023 slides) - TAIGA\n",
      "https://taigarchive.com/documents/on-the-future-of-language-models | On the future of language models (predictions / strategy memo) - TAIGA\n",
      "https://taigarchive.com/documents/takeover-cost | Takeover Cost - TAIGA\n",
      "https://taigarchive.com/documents/the-history-of-sematech-and-lessons-for-state-involvement-in-emerging-technologies | The history of Sematech and lessons for state involvement in emerging technologies - TAIGA\n",
      "https://taisc.org/report | A 30% Chance of AI Catastrophe: Samotsvety's Forecasts on AI Risks and the Impact of a Strong AI Treaty  taisc.org\n",
      "https://takeoffspeeds.com/ | Playground\n",
      "https://technologyreview.com/2022/11/18/1063487/meta-large-language-model-ai-only-survived-three-days-gpt-3-science/ | Why Meta‚Äôs latest large language model only survived three days online  MIT Technology Review\n",
      "https://techpolicy.press/us-senate-ai-insight-forum-tracker/ | US Senate AI ‚ÄòInsight Forum‚Äô Tracker\n",
      "https://theaidigest.org/progress-and-dangers | How fast is AI improving? - AI Digest\n",
      "https://thefuturesociety.org/a-blueprint-for-the-european-ai-office/?ct=t(EMAIL_CAMPAIGN_1NOV23) | A Blueprint for the European AI Office - The Future Society\n",
      "https://thefuturesociety.org/giving-agency-to-the-ai-act/?ct=t(EMAIL_CAMPAIGN_1NOV23) | Giving Agency to the AI Act - The Future Society\n",
      "https://thefuturesociety.org/heavy-is-the-head-that-wears-the-crown/?ct=t(EMAIL_CAMPAIGN_1NOV23) | Heavy is the Head that Wears the Crown - The Future Society\n",
      "https://thefuturesociety.org/response-to-u-s-ntia-ai-accountability-policy-request-for-comment/?ct=t(EMAIL_CAMPAIGN_1NOV23) | Response to U.S. NTIA AI Accountability Policy Request for Comment - The Future Society\n",
      "https://thefuturesociety.org/response-to-u-s-ostp-request-for-information/?ct=t(EMAIL_CAMPAIGN_1NOV23) | Response to U.S. OSTP Request for Information on National Priorities for AI - The Future Society\n",
      "https://theorgplumber.com/posts/statement/ | A Post Mortem on the Gino Case  The Organizational Plumber\n",
      "https://time.com/6327635/ai-needs-to-be-regulated-like-nuclear-weapons/ | How AI Needs to Be Regulated like Nuclear Energy  TIME\n",
      "https://tosummarise.com/four-thousand-weeks-10-practical-tools-to-help-embrace-your-finitude/ | Four Thousand Weeks - 10 Practical Tools to Help Embrace Your Finitude - To Summarise\n",
      "https://transformative.org/wp-content/uploads/2023/10/AI_Consortium_v2.01arXiv.pdf | Working‚ÄîAI Consortium Draft #2 (arXiv)\n",
      "https://transparency.fb.com/en-gb/policies/ai-safety-policies-for-safety-summit/ | Overview of Meta AI safety policies prepared for the UK AI Safety Summit  Transparency Center\n",
      "https://twitter.com/AaronBergman18/status/1719066808127242410 | Aaron Bergman üîç ‚è∏Ô∏è (in that order) on X: \"Based and correct understanding of risk aversion-pilled @Laura_k_Duffy I am begging people to understand that ‚Äúrisk aversion‚Äù doesn‚Äôt just mean ‚Äúhave narrow confidence intervals on the magnitude of one of the first-order effects of an intervention‚Äù (link below) t.co/3pp6EpfEeX\" / X\n",
      "https://twitter.com/AkashWasil/status/1720061569478873442 | Akash ‚è∏Ô∏è Wasil on X: \"**Summary of various AI governance proposals** Excellent work by @FLIxrisk that scores several AI governance on some relevant axes. IMO the most important axes are burden of proof, quantitative risk bounds, and compute limits. See üëá for a link to the FLI report. t.co/zV9KSgUvGm\" / X\n",
      "https://twitter.com/AkashWasil/status/1720061569478873442/photo/1 | (1) Akash ‚è∏Ô∏è Wasil on X: \"**Summary of various AI governance proposals** Excellent work by @FLIxrisk that scores several AI governance on some relevant axes. IMO the most important axes are burden of proof, quantitative risk bounds, and compute limits. See üëá for a link to the FLI report. t.co/zV9KSgUvGm\" / X\n",
      "https://twitter.com/CFGeek/status/1720514327990694129 | Charles Foster on X: \"Worried about the future of openness in AI? Here is a way to help: We're putting together a public list of all the good work that's been enabled by open-weight foundation models, to show why transparency &amp; public scrutiny is worth protecting. ‚¨áÔ∏è Links below ‚¨áÔ∏è t.co/eSMumNhtA5\" / X\n",
      "https://twitter.com/ChanaMessinger/status/1719099354827374891 | (23) Chana on X: \"@peterwildeford What would a bad version of the executive order looked like to you? (I'm trying to figure out how to evaluate this kind of thing)\" / X\n",
      "https://twitter.com/CharlotteSiegm/status/1720110570832281623 | Charlotte Siegmann on X: \"Yesterday, I was fooled by this fake LLM-generated website. Took me more than 10 minutes to figure out this was fake. Why did it take me so long? The women in the photos looked real and trustworthy. My brain still needs to fully update that models can generate that.‚Ä¶\" / X\n",
      "https://twitter.com/Chris_Said/status/1718983908866380227 | (6) Chris Said on X: \"For $200 you can fine tune an open-source LLM to provide nearly all key information needed to obtain the 1918 pandemic influenza virus.\" / X\n",
      "https://twitter.com/DanielColson6/status/1719503338184802441 | Daniel Colson on X: \"‚ÄúBiden‚Äôs AI Executive Order Isn‚Äôt Nearly Enough‚ÄîBut It‚Äôs a Good Start‚Äù says the Daily Beast in an article today covering AIPI‚Äôs polling on the executive order. Says @TonyHoWasHere: ‚ÄúIt‚Äôs more clear than ever that Americans are hungry for AI regulation. Not only does the‚Ä¶ t.co/lYY115V2LG\" / X\n",
      "https://twitter.com/DrNikkiTeran/status/1719048031549505974 | Nikki Teran on X: \"Will releasing the weights of large language models grant widespread access to pandemic agents? Turns out, yes, probably. 1/5 t.co/nl5rawvToc\" / X\n",
      "https://twitter.com/HaydnBelfield/status/1719326991780913484 | Haydn Belfield on X: \"Meta 48% t.co/wH80BdW6re\" / X\n",
      "https://twitter.com/JacquesThibs/status/1719116575221977505 | Jacques on X: \"@MatthewJBar @peterwildeford I think, for the most part, it was that they expected there would be AI regulation but that it would be basically irrelevant to their main concerns regarding agentic AGI systems. At most, it would cover misuse, and you‚Äôd likely get AI ethics stuff covered.\" / X\n",
      "https://twitter.com/JeffLadish/status/1720871947528220917 | twitter.com/JeffLadish/status/1720871947528220917\n",
      "https://twitter.com/JgaltTweets/status/1718440984596422762 | (8) JgaltTweets on X: \"Fwiw I think his approval rating will probably improve a bit by the time of the election; but mostly because of polarization (i.e. voters comparing him to Trump) rather than actually evaluating him more positively. Will probably still be quite underwater then, though. t.co/dqJarRur4M\" / X\n",
      "https://twitter.com/JgaltTweets/status/1719313280630501592 | (22) JgaltTweets on X: \"Partial guest list for the UK AI Summit released (organizations only) t.co/BCY4YBBb0D t.co/aq2fHJKNPE\" / X\n",
      "https://twitter.com/Kirsten3531/status/1721471962852016281 | Kirsten on X: \"So the best thing about having a cleaner, which I absolutely did not expect, is how much less anxiety I feel There's a part of my brain that was like \"if we don't clean the crumbs off the toaster today maybe it'll never ever ever happen\" And now I know it'll happen in 2 weeks\" / X\n",
      "https://twitter.com/Kirsten3531/status/1721474597709611226 | Kirsten on X: \"okay I want to have this for every part of my life. please tell me your backup safety mechanisms, the default bare minimum that just happens automatically so your basic needs get met\" / X\n",
      "https://twitter.com/MatthewJBar/status/1719244420929991165 | Matthew Barnett on X: \"@scholl_adam @peterwildeford If this executive order is in the 99th percentile of what people expected, it's a reasonable inference that the 50th percentile was something like \"almost no serious regulation\". And also more than a few people told me they didn't expect much AI regulation.\" / X\n",
      "https://twitter.com/Mod_Infequency/status/1719177246009864637 | Modicum_Of_Infrequency on X: \"Criminally under-read book Mathsemantics by Edward MacNeal elucidates this, among many other problems of thinking MacNeal ran an air-travel consulting company. The book is structured around questions given to job applicants. The question anchoring ch 19 is relevant in this caseüßµ\" / X\n",
      "https://twitter.com/ModerateMarcel/status/1720818453127258188 | MetaSci/Forecasts/AI Guy üîçü¶äü¶î on X: \"@peterwildeford I‚Äôd like to see a meme like this but with chimps and humans, and stats like ‚Äúnumber of nuclear weapons‚Ä¶ moon landings‚Ä¶ natural predators‚Ä¶‚Äù\" / X\n",
      "https://twitter.com/PradyuPrasad/status/1719186813489910046 | Pradyumna on X: \"t.co/7RndPRMkBr\" / X\n",
      "https://twitter.com/RichardSSutton/status/1700315838468043015 | (1) X\n",
      "https://twitter.com/Simeon_Cps/status/1717558074523934977 | twitter.com/Simeon_Cps/status/1717558074523934977\n",
      "https://twitter.com/Simeon_Cps/status/1719056018359906396 | Sim√©on on X: \"AI world experts including Yoshua Bengio, Yi Zeng, @GaryMarcus and Viktoria Krakovna are asking the UK international AI summit to launch a working group on an international AI Safety Treaty that would include: 1) Compute thresholds 2) CERN for AI 3) Safe APIs Very exciting‚Ä¶\" / X\n",
      "https://twitter.com/Simeon_Cps/status/1719549343244378296 | Sim√©on on X: \"Could GPT hide information in its words that it understands but that humans don't? Yes! And it can allow it to perform better!\" / X\n",
      "https://twitter.com/Simeon_Cps/status/1719935861314228591 | Sim√©on on X: \"Why is the Responsible Scaling Policies (RSPs) framework inadequate for risk management? 1) It's missing core components of basic risk management procedures. i) It's not measuring an aggregate level risk level ii) it's not specifying what the aggregate risk limit‚Ä¶ t.co/rPC00dEyTS\" / X\n",
      "https://twitter.com/Simeon_Cps/status/1720513387304517816 | Sim√©on on X: \"I've been chatting to many of you about alternative architectures as a more credible path to safety than making transformers safe. If you want to dig deeper into one of the most exciting one out there, the Open Agency Architecture, check the bibliography of this announcement.\" / X\n",
      "https://twitter.com/Simeon_Cps/status/1721191615430025426 | twitter.com/Simeon_Cps/status/1721191615430025426\n",
      "https://twitter.com/StefanFSchubert/status/1719102746815508796 | twitter.com/StefanFSchubert/status/1719102746815508796\n",
      "https://twitter.com/StefanFSchubert/status/1719338046057837012 | (21) Stefan Schubert on X: \"Yes, people giving explanations too frequently fail to make the obvious test, \"what about other countries?\" A bit embarrassing that this simple error is so common. Rationality is to an underappreciated extent about paying attention to these very basic things.\" / X\n",
      "https://twitter.com/StephenLCasper/status/1720896730483548289 | twitter.com/StephenLCasper/status/1720896730483548289\n",
      "https://twitter.com/StephenLCasper/status/1720910484441014525 | Stephen Casper ‚è∏Ô∏è on X: \"I like this! \"The goal of task blocking is to create models that increase the costs of fine-tuning on harmful downstream tasks such that an adversary would rather start from scratch than use the pretrained model, while remaining useful for desired tasks\" t.co/4cB6IrWaFK t.co/PTvTDI18TY\" / X\n",
      "https://twitter.com/TheDevanshMehta/status/1719987035598151781 | (1) Devansh Mehta on X: \"A üßµ on how to conduct PRODUCTIVE group meetings of over 10 people 1. Create prompt questions in a figma jam 2. Have all participants submit ideas to the prompts via sticky notes 3. Give a thumbs up on sticky notes you like, briefly discuss the most upvoted ideas t.co/lRsbhtt4Ma\" / X\n",
      "https://twitter.com/TolgaBilge_/status/1719024093284995205 | Tolga Bilge on X: \"Top AI and Policy Experts Call for an International AI Safety Treaty In an open letter we just published, top experts including Yoshua Bengio, and over 100 others urge AI treaty developent to begin. We encourage all members of the public to sign below: t.co/J6qo7nK3Us\" / X\n",
      "https://twitter.com/acesounderglass/status/1721024661646152154 | Elizabeth Van Nostrand on X: \"This is your call to test the potato + watermelon diet\" / X\n",
      "https://twitter.com/alxndrdavies/status/1720435535855149513 | Xander Davies on X: \"Proud to be a researcher at the UK's AI Safety Institute‚Äîwhat is it? üßµ based on yesterday's introduction: t.co/MmnsxQvZZK\" / X\n",
      "https://twitter.com/andyzou_jiaming/status/1709365304789238201 | (1) Andy Zou on X: \"LLMs can hallucinate and lie. They can be jailbroken by weird suffixes. They memorize training data and exhibit biases. üß† We shed light on all of these phenomena with a new approach to AI transparency. üßµ Website: t.co/J7ikNYmZP9 Paper: t.co/b84PXQOOsc t.co/uwO7tzd2Ie\" / X\n",
      "https://twitter.com/archanaahlawat/status/1719473234200957256 | Archana Ahlawat on X: \"Excited to share our paper on how the field of AI safety has developed! We focus on the epistemic culture of AI safety: not only the ideas, but also the intertwined social, intellectual, and institutional practices that have shaped and propelled them.\" / X\n",
      "https://twitter.com/davidmanheim/status/1719243636817248766 | David Manheim (Follow me on $otherplatform!) on X: \"@daniel_271828 \"Use AI to advance cybersecurity\" seems decidedly net-negative to me. Why is that not a dangerous domain for escalation? Instead, we should be rebuilding a secure computing stack, without AI - per the below. t.co/k3YvOphhiJ\" / X\n",
      "https://twitter.com/davidmanheim/status/1719687126923506017 | David Manheim (Follow me on $otherplatform!) on X: \"College is supposed to be full-time. Evidently, the expectation in elite schools is that a full course load should make sure you still have time for a part-time consulting project, protests, and the extra-curricular goals students have. It seems @Princeton doesn't do that - yet. t.co/RpoBIXnAO9\" / X\n",
      "https://twitter.com/farairesearch/status/1719409979088884186 | FAR AI on X: \"Codebook Features make language models more interpretable and controllable, with minimal performance loss! Our method turns complex vectors into discrete codes, providing a potential path toward safer and more reliable machine learning systems. t.co/L24zY9AtrC\" / X\n",
      "https://twitter.com/farairesearch/status/1719723764047315032 | FAR AI on X: \"Prominent AI researchers from West and East including Turing recipients Yoshua Bengio üá®üá¶ &amp; Andrew Yao üá®üá≥called for global action on AI safety and governance to prevent uncontrolled frontier model development posing unacceptable risks to humanity. üßµ\" / X\n",
      "https://twitter.com/felix_red_panda/status/1718916631512949248 | (6) Felix on X: \"Microsoft paper claims ChatGPT 3.5 has ~20 billion parameters t.co/gZxh0l2VqX t.co/EDCWbLdYEz\" / X\n",
      "https://twitter.com/fiiiiiist/status/1717546875849437542 | Tim Fist on X: \"Which frontier AI regulatory measures are actually shovel-ready today? @hlntnr and I ran a workshop series with ppl across academia, civil society, and industry, with voices both supportive and skeptical of frontier AI regs. Here's what we found: t.co/L8szoFyMNn\" / X\n",
      "https://twitter.com/freed_dfilan/status/1720700332479787458 | Daniel Filan üîé on X: \"i solved the problem of whether you should open-source LLMs t.co/enoTgnpUG2\" / X\n",
      "https://twitter.com/iScienceLuvr/status/1719550186505302408 | Tanishq Mathew Abraham, PhD on X: \"Does GPT-4 Pass the Turing Test? abs: t.co/kaYovepPYP &gt; The best-performing GPT-4 prompt passed in 41% of games, outperforming baselines set by ELIZA (27%) and GPT-3.5 (14%), but falling short of chance and the baseline set by human participants (63%). t.co/XhXe76MKde\" / X\n",
      "https://twitter.com/james_acton32/status/1717978150355783772 | (1) (((James Acton))) on X: \"üßµThoughts on the U.S. decision to develop the B61-13 nuclear warhead. I think this has to be seen in the context of Congressional efforts to stop the administration from withdrawing the old megaton-class B83. The fact sheet hints at this. (1/n) t.co/JcHQGAZe2q t.co/xzi7swkZu0\" / X\n",
      "https://twitter.com/jesswhittles/status/1720009847830163878 | Jess Whittlestone on X: \"1/ I was pleased to have an opportunity to provide some brief closing remarks at the UK's #AISafetySummit yesterday (I start speaking around 52:00) A (very slightly edited) summary of my remarks below, including what I hope to see by the next summit in 6 months' time:\" / X\n",
      "https://twitter.com/kanjun/status/1720502401067811242 | Kanjun üêôüè° on X: \"Reflections on UK AI Safety Summitüëá 1/ People agree way more than expected. National ministers, AI lab leaders, safety researchers all rallied on infrastructure hardening, continuous evals, global coordination. Views were nuanced; Twitter is a disservice to complex discussion.\" / X\n",
      "https://twitter.com/kesvelt/status/1718976444175425796 | (6) Kevin Esvelt on X: \"Will sharing the weights of future foundation models negate safeguards? To find out, we ran a hackathon at MIT in which participants playing compulsively honest bioterrorists asked Base and Spicy versions of Llama-2 how to obtain 1918 influenza virus. 1/7 t.co/qVxUc21sRr t.co/34IzMipTYL\" / X\n",
      "https://twitter.com/kesvelt/status/1718977801431155173 | Kevin Esvelt on X: \"Examining worst-case pandemic scenarios, we identified two that could collapse civilization. We‚Äôve outlined defenses against both types of pandemic and are working on them, but for now we're extremely vulnerable. 4/7 t.co/rYzRiN4Tpm t.co/w1IraUdxaZ\" / X\n",
      "https://twitter.com/kesvelt/status/1720440451059335520 | Kevin Esvelt on X: \"MWP v2: Can biology kill &gt;100m? Yes: smallpox. Can biology do worse? Yes: myxoma killed &gt;90% of rabbits. Could a biotech expert match this within 10y? Surprising if not. Would sharing future model weights give everyone an amoral biotech-expert tutor? Yes. Therefore, let‚Äôs not. t.co/nXleUJJbXv\" / X\n",
      "https://twitter.com/lxrjl/status/1717885547606118863 | alex lawsen on X: \"Very exciting to see that, ahead of the summit, Amazon, Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI have all been asked to outline their policies on 9 areas of AI safety.\" / X\n",
      "https://twitter.com/messages/25776739-27569373 | twitter.com/messages/25776739-27569373\n",
      "https://twitter.com/norabelrose/status/1718388451584934150 | (1) Nora Belrose on X: \"The kind of AI deception that @dwarkesh_sp and @liron are worried about is less than 1% likely IMO, so I get why @ShaneLegg isn't actively planning for it. That said, I'm training 1000s of NNs with different random seeds right now to empirically test how likely it actually is.\" / X\n",
      "https://twitter.com/ohlennart/status/1717243855781712300 | twitter.com/ohlennart/status/1717243855781712300\n",
      "https://twitter.com/ohlennart/status/1719776488839356761 | Lennart Heim on X: \"Three months ago, @bmgarfinkel and I convened a workshop &amp; asked: What Should the AI Safety Summit Try to Accomplish? Now, with China there and our outlined outcomes met, it's clear that significant progress has been made. This deserves recognition. t.co/8cZvphPZkV t.co/Y6e9CcBXNJ\" / X\n",
      "https://twitter.com/random_walker/status/1719457323847233596 | Arvind Narayanan on X: \"New on the AI Snake Oil blog: How will the Executive Order impact openness in AI? We did a deep dive. On balance, for now, the EO seems to be good news for those who favor openness in AI. t.co/q4KyHvsiZZ with @sayashk and @RishiBommasani. t.co/uIKAeVNj0c\" / X\n",
      "https://twitter.com/robbensinger/status/1719051718674043090 | Rob Bensinger ‚èπÔ∏è on X: \"Institutions work better when people freely share ass numbers (and subjective probabilities more generally). E.g., a lot of the CDC's COVID errors stem from black-and-white thinking and speaking. But publicly communicating ass numbers is tricky.\" / X\n",
      "https://twitter.com/robbensinger/status/1720516817234620833 | Rob Bensinger ‚èπÔ∏è on X: \"Back in the day, x-risk folks considered the 2014 short film \"The Awareness\" by far the least unrealistic film depiction of smarter-than-human AI risk. (Beating eg Ex Machina.) Are there any better contenders nowadays? (Give a reason if you suggest one.) t.co/J0SnIspqgy\" / X\n",
      "https://twitter.com/saffronhuang/status/1719826694414995687 | (1) Saffron Huang on X: \"Humbled to have presented on ‚ÄòRisks from Frontier Al Integration into Society‚Äô at the #AlSafetySummit. And excited to be advancing sociotechnical evals at the Taskforce-turned-Al Safety Institute w/ @collect_intel :) t.co/qSbjqGcDzC\" / X\n",
      "https://twitter.com/sebkrier/status/1717577361674080711 | S√©b Krier on X: \"The GO Science report on Frontier AI risk is pretty good. It goes through five different possible scenarios for AGI from 'unpredictable advanced AI' to 'AI dissapoints'. Nice clear overview of future capabilities and uncertainties in the field too. t.co/Cvc8Tf44b3 t.co/B62ErRpBml\" / X\n",
      "https://twitter.com/shaunkeee/status/1720409161895473165 | Shaun K.E. Ee on X: \"[1/15] How can we adapt cybersecurity frameworks to address frontier AI risks? My colleagues at @_IAPS_ and I explore this question in our new ‚Äúdefense-in-depth‚Äù paper and provide recs for the Frontier Model Forum, @NIST, @CISAgov, @MITREcorp, and others: t.co/ROUiEd8JI8 t.co/Bwi3FcfiFa\" / X\n",
      "https://twitter.com/simonw/status/1718309019109220791 | (9) Simon Willison on X: \"This summary of the various common definitions looks about right to me\" / X\n",
      "https://twitter.com/simonw/status/1718637168602878169 | Simon Willison on X: \"I'm really surprised to see browsing and code interpreter made available in the same session - feels like a potent vector for creative prompt injection attacks against the combination of the two\" / X\n",
      "https://twitter.com/sorenmind/status/1719005832099045554 | S√∂ren Mindermann on X: \"Our new consensus paper on AI risks &amp; measures, including policy and technical measures, has made the The Guardian's front page! Whereas Biden‚Äôs \"sweeping\" executive order from today holds no-one responsible.. 1/üßµ t.co/m29FyeKNSW\" / X\n",
      "https://twitter.com/soundboy/status/1718925374623519155 | (5) Ian Hogarth on X: \"1/ The Taskforce is a start-up inside government, delivering on the mission given to us by the Prime Minister: to build an AI research team that can evaluate risks at the frontier of AI. We are now 18 weeks old and this is our second progress report: t.co/FdEUYWiY8a\" / X\n",
      "https://twitter.com/timhwang/status/1719548740409983123 | Tim Hwang on X: \"hands down one of my favorite longitudinal datasets of all time\" / X\n",
      "https://twitter.com/typedfemale/status/1718878835910045931 | twitter.com/typedfemale/status/1718878835910045931\n",
      "https://twitter.com/ylecun/status/1721374616906555489 | twitter.com/ylecun/status/1721374616906555489\n",
      "https://washingtonpost.com/technology/2023/07/13/ftc-openai-chatgpt-sam-altman-lina-khan/ | \n",
      "https://whitehouse.gov/briefing-room/statements-releases/2023/11/01/fact-sheet-vice-president-harris-announces-new-u-s-initiatives-to-advance-the-safe-and-responsible-use-of-artificial-intelligence/ | FACT SHEET: Vice President Harris Announces New U.S. Initiatives to Advance the Safe and Responsible Use of Artificial Intelligence  The White House\n",
      "https://wikiwand.com/en/Federal_Trade_Commission | Federal Trade Commission - Wikiwand\n",
      "https://wikiwand.com/en/Madeleine_Albright | Madeleine Albright - Wikiwand\n",
      "https://wired.com/story/ftx-founder-sam-bankman-fried-guilty-fraud-trial/ | Sam Bankman-Fried Has Been Found Guilty of Fraud  WIRED\n",
      "https://wired.com/story/slovakias-election-deepfakes-show-ai-is-a-danger-to-democracy/ | Slovakia‚Äôs Election Deepfakes Show AI Is a Danger to Democracy  WIRED\n",
      "https://workdrive.zohopublic.eu/writer/open/9eo4gf5ed77a14d5f4eeeada1dde9ef3442c1?authId=%7B%22linkId%22%3A%222hi4rY3knV7-lUMadS%22%7D | Summary - Relevant Aspects of Executive Order on AI\n",
      "https://www-p404.intacct.com/ia/acct/frameset.phtml?.sess=G0nS9jvXYvJtycPw8iACc3yH8mxUEhtJiS3eyK-UbcnD8PIgAmMyyc9t&.cc=KF0e7hxs_O9ZI_VPiHeunVSavExwUvROZCF-QUNCVnk. | Rethink Priorities\n",
      "https://youtube.com/watch?v=5nb4vnpiMMI | Improving AI benchmarks by Isabel Juniewicz - YouTube\n",
      "https://youtube.com/watch?v=LCLPHED9zWM | Biggest Questions in AI Forecasting by Matthew Barnett - YouTube\n",
      "https://youtube.com/watch?v=dAwLMS8fgoA | Barbie - Dance The Night Scene  HD - YouTube\n"
     ]
    }
   ],
   "source": [
    "tabs = ['https://' + t for t in sorted([t.replace('http://', '').replace('https://', '').replace('www.', '') for t in tabs])]\n",
    "\n",
    "print('Sorted tabs! ({})'.format(len(tabs)))\n",
    "\n",
    "print('-')\n",
    "for t in tabs:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65df311d-c9c6-4ace-a7c9-7ed21b34d78f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled tabs! (315)\n",
      "-\n",
      "https://benchmarks.llmonitor.com/ | LLMonitor Benchmarks\n",
      "https://metaculus.com/questions/19700/additional-russian-ipo-in-2023/ | Additional Russian IPO in 2023?  Metaculus\n",
      "https://metaculus.com/questions/19764/second-starship-integrated-test-before-2024/ | Second Starship Integrated Test Before 2024?  Metaculus\n",
      "https://metaculus.com/tournament/quarterly-cup/ | üèÜ Quarterly Cup üèÜ  Metaculus\n",
      "https://twitter.com/RichardSSutton/status/1700315838468043015 | (1) X\n",
      "https://theorgplumber.com/posts/statement/ | A Post Mortem on the Gino Case  The Organizational Plumber\n",
      "https://docs.google.com/document/d/1eq2BPX9MSL-onZEuzD39wK4B6cb3VH6ffw_ixN05upo/edit#heading=h.lbj5xicd7zt7 | AISCC Comms Tactics: UK Summit - Google Docs\n",
      "https://metaculus.com/questions/19702/foreign-intervention-in-the-israel-gaza-war/ | Foreign intervention in the Israel-Gaza War  Metaculus\n",
      "https://forum.effectivealtruism.org/posts/WfodoyjePTTuaTjLe/efficacy-of-ai-activism-have-we-ever-said-no?utm_source=EA+Forum+Digest&utm_campaign=1771e9b7a6-EMAIL_CAMPAIGN_2023_11_01_08_45&utm_medium=email&utm_term=0_-1771e9b7a6-%5BLIST_EMAIL_ID%5D | Efficacy of AI Activism: Have We Ever Said No? ‚Äî EA Forum\n",
      "https://docs.google.com/spreadsheets/d/1kxEwC9-ZGQjEa1qrMuc1RIjTKs8w0Jdg5l3Rf5LxELw/edit#gid=0 | First common budget! - Google Sheets\n",
      "https://docs.google.com/document/d/1hR-LNaK54Rz94Xtu5chrHvRptM7UUMux_eYEu3oC7io/edit | WIT Feedback - Google Docs\n",
      "https://takeoffspeeds.com/ | Playground\n",
      "https://thefuturesociety.org/giving-agency-to-the-ai-act/?ct=t(EMAIL_CAMPAIGN_1NOV23) | Giving Agency to the AI Act - The Future Society\n",
      "https://onlinelibrary.wiley.com/doi/full/10.1111/phpr.13006 | Rational risk‚Äêaversion: Good things come to those who weight - Bottomley - Philosophy and Phenomenological Research - Wiley Online Library\n",
      "https://twitter.com/sebkrier/status/1717577361674080711 | S√©b Krier on X: \"The GO Science report on Frontier AI risk is pretty good. It goes through five different possible scenarios for AGI from 'unpredictable advanced AI' to 'AI dissapoints'. Nice clear overview of future capabilities and uncertainties in the field too. t.co/Cvc8Tf44b3 t.co/B62ErRpBml\" / X\n",
      "https://ai.gov/apply/ | Join the National AI Talent Surge - AI.gov\n",
      "https://docs.google.com/document/d/1fqmabo6ANKocghsmJ4bicXys2TsbAHgiRwwMTRkoe2k/edit | Peter / Amanda / Zoe Responsibilities Call - Oct 2023 - Google Docs\n",
      "https://google.com/search?q=hiroshima+g7+process&rlz=1CDGOYI_enUS715US715&oq=hiroshima+g7+process&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIICAEQABgWGB4yCAgCEAAYFhgeMgoIAxAAGIYDGIoFMgoIBBAAGIYDGIoFMgoIBRAAGIYDGIoFMgoIBhAAGIYDGIoF0gEINjcwMmowajSoAgCwAgA&hl=en-US&sourceid=chrome-mobile&ie=UTF-8 | hiroshima g7 process - Google Search\n",
      "https://forum.effectivealtruism.org/posts/NrqGyXzvwB2Gqu6XW/state-of-the-east-and-southeast-asian-eacosystem | State of the East and Southeast Asian EAcosystem ‚Äî EA Forum\n",
      "https://twitter.com/StefanFSchubert/status/1719102746815508796 | twitter.com/StefanFSchubert/status/1719102746815508796\n",
      "https://forum.effectivealtruism.org/ | Effective Altruism Forum\n",
      "https://twitter.com/ohlennart/status/1719776488839356761 | Lennart Heim on X: \"Three months ago, @bmgarfinkel and I convened a workshop &amp; asked: What Should the AI Safety Summit Try to Accomplish? Now, with China there and our outlined outcomes met, it's clear that significant progress has been made. This deserves recognition. t.co/8cZvphPZkV t.co/Y6e9CcBXNJ\" / X\n",
      "https://simonwillison.net/2023/Oct/26/add-a-walrus/ | Now add a walrus: Prompt engineering in DALL-E 3\n",
      "https://concordia-consulting.com/wp-content/uploads/2023/10/State-of-AI-Safety-in-China.pdf | State of AI Safety in China\n",
      "https://metaculus.com/questions/12937/greatest-computation-used-in-ai-training/ | Greatest Computation Used in AI Training  Metaculus\n",
      "https://youtube.com/watch?v=dAwLMS8fgoA | Barbie - Dance The Night Scene  HD - YouTube\n",
      "https://taigarchive.com/documents/takeover-cost | Takeover Cost - TAIGA\n",
      "https://taisc.org/report | A 30% Chance of AI Catastrophe: Samotsvety's Forecasts on AI Risks and the Impact of a Strong AI Treaty  taisc.org\n",
      "https://docs.google.com/document/d/1ziNrskp-v_jWihUakPIhSLqdu6WJY-mA0152RUcLqQc/edit | IAPS Leads notes (Michael, Peter, Zoe, sometimes Amanda/Ashwin) - 2023 May-Oct - Google Docs\n",
      "https://twitter.com/random_walker/status/1719457323847233596 | Arvind Narayanan on X: \"New on the AI Snake Oil blog: How will the Executive Order impact openness in AI? We did a deep dive. On balance, for now, the EO seems to be good news for those who favor openness in AI. t.co/q4KyHvsiZZ with @sayashk and @RishiBommasani. t.co/uIKAeVNj0c\" / X\n",
      "https://arxiv.org/abs/2305.15324 | [2305.15324] Model evaluation for extreme risks\n",
      "https://docs.google.com/spreadsheets/d/1_S_OPoTpFB07sjPDEhZ-g3kGj7wE4nfflNFTculL_BE/edit#gid=1551801213 | RP Fundraising Forecast [2023 + 2024 predictions] - Google Sheets\n",
      "https://gov.uk/government/publications/frontier-ai-capabilities-and-risks-discussion-paper | Future Risks of Frontier AI [UK gov]\n",
      "https://lesswrong.com/posts/ms3x8ngwTfep7jBue/thoughts-on-the-ai-safety-summit-company-policy-requests-and | Thoughts on the AI Safety Summit company policy requests and responses ‚Äî LessWrong\n",
      "https://forum.effectivealtruism.org/posts/cSTxmWadcdFayEhAE/tom-barnes-s-quick-takes?commentId=zXsgvqzXAGmrsXmLG | Tom Barnes's Quick takes ‚Äî EA Forum\n",
      "https://metaculus.com/ai/ | The Metaculus Lens on AI\n",
      "https://twitter.com/ylecun/status/1721374616906555489 | twitter.com/ylecun/status/1721374616906555489\n",
      "https://forum.effectivealtruism.org/posts/hAzhyikPnLnMXweXG/participate-in-the-donation-election-and-the-first-weekly | Participate in the Donation Election and the first weekly theme (starting 7 November) ‚Äî EA Forum\n",
      "https://twitter.com/iScienceLuvr/status/1719550186505302408 | Tanishq Mathew Abraham, PhD on X: \"Does GPT-4 Pass the Turing Test? abs: t.co/kaYovepPYP &gt; The best-performing GPT-4 prompt passed in 41% of games, outperforming baselines set by ELIZA (27%) and GPT-3.5 (14%), but falling short of chance and the baseline set by human participants (63%). t.co/XhXe76MKde\" / X\n",
      "https://twitter.com/PradyuPrasad/status/1719186813489910046 | Pradyumna on X: \"t.co/7RndPRMkBr\" / X\n",
      "https://docs.google.com/document/d/1Gc09Bhn73KvHuIvrpDQNj7jit1xnq1UlGSPxeGzqmLY/edit#heading=h.jxumr5giwybo | Mechanisms for making consumer GPUs less useful for AI training - Google Docs\n",
      "https://twitter.com/sorenmind/status/1719005832099045554 | S√∂ren Mindermann on X: \"Our new consensus paper on AI risks &amp; measures, including policy and technical measures, has made the The Guardian's front page! Whereas Biden‚Äôs \"sweeping\" executive order from today holds no-one responsible.. 1/üßµ t.co/m29FyeKNSW\" / X\n",
      "https://dam.gcsp.ch/files/doc/gcsp-geneva-paper-29-22 | gcsp-geneva-paper-29-22\n",
      "https://listennotes.com/podcasts/the-dynamist/episode-41-chip-wars-china-BWDRwPH1SG3/ | Episode 41: Chip Wars, China, & Compute Governance w/ Onni Aarne & Erich Grunewald  Listen Notes\n",
      "https://foreignpolicy.com/2023/10/07/cloud-computing-artificial-intelligence-chips-sanctions-us-china/ | The Cloud Can Solve America's AI Problems\n",
      "https://twitter.com/JgaltTweets/status/1719313280630501592 | (22) JgaltTweets on X: \"Partial guest list for the UK AI Summit released (organizations only) t.co/BCY4YBBb0D t.co/aq2fHJKNPE\" / X\n",
      "https://oecd.ai/en/wonk/athens-roundtable-2023?ct=t(EMAIL_CAMPAIGN_1NOV23) | Bridging the gap in AI governance and the rule of law: The Athens Roundtable 2023 - OECD.AI\n",
      "https://docs.google.com/document/d/10Ru6IjJrjWZy_gk5cfnoqCh653luZ19Ee0KE1IkuXuU/edit#heading=h.4h9qdgodgcz5 | [Working draft] Safeguarding the safeguards - Google Docs\n",
      "https://twitter.com/Mod_Infequency/status/1719177246009864637 | Modicum_Of_Infrequency on X: \"Criminally under-read book Mathsemantics by Edward MacNeal elucidates this, among many other problems of thinking MacNeal ran an air-travel consulting company. The book is structured around questions given to job applicants. The question anchoring ch 19 is relevant in this caseüßµ\" / X\n",
      "https://cnas.org/press/press-note/cnas-responds-white-house-executive-order-on-artificial-intelligence | CNAS Responds: White House Executive Order on Artificial Intelligence  Center for a New American Security (en-US)\n",
      "https://taigarchive.com/documents/on-the-future-of-language-models | On the future of language models (predictions / strategy memo) - TAIGA\n",
      "https://twitter.com/Simeon_Cps/status/1721191615430025426 | twitter.com/Simeon_Cps/status/1721191615430025426\n",
      "https://washingtonpost.com/technology/2023/07/13/ftc-openai-chatgpt-sam-altman-lina-khan/ | \n",
      "https://facebook.com/groups/1062957250383195/?multi_permalinks=7276436365701888&hoisted_section_header_type=recently_seen | Effective Altruism Job Postings  Facebook\n",
      "https://linkedin.com/in/will-poff-webster-a235964b/?original_referer=https%3A%2F%2Fgoogle.com%2F | (86) Will Poff-Webster  LinkedIn\n",
      "https://aws.amazon.com/uki/cloud-services/uk-gov-ai-safety-summit/ | AI Safety Summit - Enhancing Frontier AI Safety\n",
      "https://forum.effectivealtruism.org/giving-portal | Giving portal ‚Äî EA Forum\n",
      "https://twitter.com/DanielColson6/status/1719503338184802441 | Daniel Colson on X: \"‚ÄúBiden‚Äôs AI Executive Order Isn‚Äôt Nearly Enough‚ÄîBut It‚Äôs a Good Start‚Äù says the Daily Beast in an article today covering AIPI‚Äôs polling on the executive order. Says @TonyHoWasHere: ‚ÄúIt‚Äôs more clear than ever that Americans are hungry for AI regulation. Not only does the‚Ä¶ t.co/lYY115V2LG\" / X\n",
      "https://metaculus.com/questions/12539/pierre-poilievre-pm-of-canada-before-2026/ | Pierre Poilievre PM of Canada before 2026?  Metaculus\n",
      "https://lesswrong.com/posts/Nwgdq6kHke5LY692J/alignment-by-default | Alignment By Default ‚Äî LessWrong\n",
      "https://jefftk.com/p/public-weights | Public Weights?\n",
      "https://gufaculty360.georgetown.edu/s/contact/00336000014U0TKAA0/ben-buchanan | Ben Buchanan: Georgetown University\n",
      "https://forum.effectivealtruism.org/posts/TurdGigfQKwxBkKnb/estimating-ea-growth-rates-mcf-memo-1 | Estimating¬†EA Growth Rates¬†(MCF memo) ‚Äî EA Forum\n",
      "https://sites.google.com/existential-security.com/2024summit/home | 2024 Summit\n",
      "https://twitter.com/AkashWasil/status/1720061569478873442 | Akash ‚è∏Ô∏è Wasil on X: \"**Summary of various AI governance proposals** Excellent work by @FLIxrisk that scores several AI governance on some relevant axes. IMO the most important axes are burden of proof, quantitative risk bounds, and compute limits. See üëá for a link to the FLI report. t.co/zV9KSgUvGm\" / X\n",
      "https://rethinkpriorities.org/research | Research ‚Äî Rethink Priorities\n",
      "https://forum.effectivealtruism.org/posts/dpjCwMwKEPqK3TPnC/notes-on-managing-to-change-the-world | Notes on \"Managing to Change the World\" ‚Äî EA Forum\n",
      "https://jefftk.com/p/examples-of-superintelligence-risk | Examples of Superintelligence Risk\n",
      "https://aisafetysummit.gov.uk/#company-policies | AI Safety Summit AISS 2023\n",
      "https://docs.google.com/document/d/1vMSoRvzgF2VLcBcjqHTvQ2NXHRqVXsNyT4XI54wk79Q/edit | Peter/Ben 1-1s - Google Docs\n",
      "https://twitter.com/timhwang/status/1719548740409983123 | Tim Hwang on X: \"hands down one of my favorite longitudinal datasets of all time\" / X\n",
      "https://localhost:8890/lab/tree/(3C)%20When%20TAI%3F.ipynb | (3C) When TA‚Ä¶ (2) - JupyterLab\n",
      "https://docs.google.com/spreadsheets/d/1qEsXG6I6hR2rhYxnuAtpHI-YcCAyPNt1uNYK5LT7CPc/edit#gid=791158494 | Peter's ERA - Rubric - 2023 - Google Sheets\n",
      "https://docs.google.com/spreadsheets/d/1fk6jAn4rpbx1T9kKYl-3C9TJHQA8ZC6lFNpT5QawNbs/edit#gid=1158312132 | 2023 Stats - Google Sheets\n",
      "https://gov.uk/government/publications/international-survey-of-public-opinion-on-ai-safety | International survey of public opinion on AI safety - GOV.UK\n",
      "https://lesswrong.com/posts/LhxHcASQwpNa3mRNk/untrusted-smart-models-and-trusted-dumb-models | Untrusted smart models and trusted dumb models ‚Äî LessWrong\n",
      "https://lesswrong.com/posts/tyE4orCtR8H9eTiEr/stuxnet-not-skynet-humanity-s-disempowerment-by-ai | Stuxnet, not Skynet: Humanity's disempowerment by AI ‚Äî LessWrong\n",
      "https://forum.effectivealtruism.org/posts/7SjtFYo6sCe3588Tx/why-scale-is-overrated-the-case-for-increasing-ea-policy | Why scale is overrated: The case for increasing EA policy efforts in smaller countries ‚Äî EA Forum\n",
      "https://docs.google.com/spreadsheets/d/1-iyQ2q3vH6eVQnTh7BnSM9l01VFCWNFnr4TTs658RUg/edit#gid=804926468 | IAPS budget [Aug 2023 OP application] - Google Sheets\n",
      "https://twitter.com/farairesearch/status/1719723764047315032 | FAR AI on X: \"Prominent AI researchers from West and East including Turing recipients Yoshua Bengio üá®üá¶ &amp; Andrew Yao üá®üá≥called for global action on AI safety and governance to prevent uncontrolled frontier model development posing unacceptable risks to humanity. üßµ\" / X\n",
      "https://builtin.com/artificial-intelligence/ftc-openai | FTC OpenAI Investigation  Built In\n",
      "https://docs.google.com/document/d/15OerqofHlipOYys0h8ls2YqjLNCk2BwXKsETGlXINSo/edit | Surveys Squad Meeting Agendas - Google Docs\n",
      "https://twitter.com/archanaahlawat/status/1719473234200957256 | Archana Ahlawat on X: \"Excited to share our paper on how the field of AI safety has developed! We focus on the epistemic culture of AI safety: not only the ideas, but also the intertwined social, intellectual, and institutional practices that have shaped and propelled them.\" / X\n",
      "https://futureoflife.org/project/artificial-escalation/ | Artificial Escalation - Future of Life Institute\n",
      "https://lesswrong.com/posts/SseoT9mKDTL3RCbE9/vaniver-s-thoughts-on-anthropic-s-rsp | Vaniver's thoughts on Anthropic's RSP ‚Äî LessWrong\n",
      "https://transparency.fb.com/en-gb/policies/ai-safety-policies-for-safety-summit/ | Overview of Meta AI safety policies prepared for the UK AI Safety Summit  Transparency Center\n",
      "https://forum.effectivealtruism.org/editPost?postId=unFycWDoyDHdHQGT5 | How Rethink Priorities is Addressing Risk and Uncertainty ‚Äî EA Forum\n",
      "https://lesswrong.com/posts/EaZghEwcCJRAuee66/my-thoughts-on-the-social-response-to-ai-risk | My thoughts on the social response to AI risk ‚Äî LessWrong\n",
      "https://techpolicy.press/us-senate-ai-insight-forum-tracker/ | US Senate AI ‚ÄòInsight Forum‚Äô Tracker\n",
      "https://twitter.com/freed_dfilan/status/1720700332479787458 | Daniel Filan üîé on X: \"i solved the problem of whether you should open-source LLMs t.co/enoTgnpUG2\" / X\n",
      "https://managing-ai-risks.com/ | Managing AI Risks in an Era of Rapid Progress\n",
      "https://docs.google.com/document/d/1bhl9kVF1_LkLzOnEaI8MPrUnRsegI0jittLmc5tZZQI/edit#heading=h.v9pzfmksvnbf | AI Regs / Lab Gov Sync Up Call - Oct 2023 - Google Docs\n",
      "https://constellation.org/programs/astra-fellowship | Astra Fellowship\n",
      "https://twitter.com/ModerateMarcel/status/1720818453127258188 | MetaSci/Forecasts/AI Guy üîçü¶äü¶î on X: \"@peterwildeford I‚Äôd like to see a meme like this but with chimps and humans, and stats like ‚Äúnumber of nuclear weapons‚Ä¶ moon landings‚Ä¶ natural predators‚Ä¶‚Äù\" / X\n",
      "https://docs.google.com/document/d/1zBS3J3R4sKrgLynZDbCIn2u5AdvOr3wcGcnglZ6HSSY/edit#heading=h.fi10dekku7ku | Evidence on the impact/quality of AIGS's work so far - Google Docs\n",
      "https://wired.com/story/slovakias-election-deepfakes-show-ai-is-a-danger-to-democracy/ | Slovakia‚Äôs Election Deepfakes Show AI Is a Danger to Democracy  WIRED\n",
      "https://cnas.org/publications/reports/strengthening-the-shield | Strengthening the Shield  Center for a New American Security (en-US)\n",
      "https://twitter.com/simonw/status/1718309019109220791 | (9) Simon Willison on X: \"This summary of the various common definitions looks about right to me\" / X\n",
      "https://docs.google.com/document/d/1m_-XgZgBs0LZHplodgBbGcpiGtNWalPxPBjGOwF6rig/edit | Macrocalendar - Google Docs\n",
      "https://workdrive.zohopublic.eu/writer/open/9eo4gf5ed77a14d5f4eeeada1dde9ef3442c1?authId=%7B%22linkId%22%3A%222hi4rY3knV7-lUMadS%22%7D | Summary - Relevant Aspects of Executive Order on AI\n",
      "https://forum.effectivealtruism.org/posts/GW3cxBurTNKHs352S/controlling-for-a-thinker-s-big-idea | Controlling for a thinker‚Äôs big idea ‚Äî EA Forum\n",
      "https://docs.google.com/document/d/1QufMubGOVuoOhY-WoNfNB0wRuxoKmlO70gksrUDbVaY/edit | Further responses to OP - Google Docs\n",
      "https://docs.google.com/document/d/1lP0XF_ncfIF5_hBitLW5wlfCV0W_xhYgeNzZdD2ZkVg/edit#heading=h.v21njykqjfyn | Collection: All FRAP conversation notes - Google Docs\n",
      "https://arxiv.org/pdf/2306.06924.pdf | TASRA: a Taxonomy and Analysis of Societal-Scale Risks from AI\n",
      "https://youtube.com/watch?v=LCLPHED9zWM | Biggest Questions in AI Forecasting by Matthew Barnett - YouTube\n",
      "https://lesswrong.com/posts/PvBpRu354uG7ypwRP/on-the-executive-order | On the Executive Order ‚Äî LessWrong\n",
      "https://docs.google.com/document/d/164f49D-XkH8XyEF0howZQx0HWPFUESB1X3st5hgnRQs/edit | Brainstorming & prioritizing potential uses of AIGS team budget - Google Docs\n",
      "https://transformative.org/wp-content/uploads/2023/10/AI_Consortium_v2.01arXiv.pdf | Working‚ÄîAI Consortium Draft #2 (arXiv)\n",
      "https://lesswrong.com/posts/cCbybRT8bgiMbEHEv/a-list-of-all-the-deadlines-in-biden-s-executive-order-on-ai#comments | A list of all the deadlines in Biden's Executive Order on AI ‚Äî LessWrong\n",
      "https://taigarchive.com/documents/a-blueprint-for-the-european-ai-office | A Blueprint for the European AI Office - TAIGA\n",
      "https://forum.effectivealtruism.org/s/WdL3LE5LHvTwWmyqj/p/i5cuLZH3SQJigiHMs | Charting the precipice: The time of perils and prioritizing x-risk ‚Äî EA Forum\n",
      "https://assets.publishing.service.gov.uk/media/653aabbd80884d000df71bdc/emerging-processes-frontier-ai-safety.pdf | Emerging processes for frontier AI safety\n",
      "https://arxiv.org/abs/2310.20563 | [2310.20563] Taking control: Policies to address extinction risks from advanced AI\n",
      "https://lesswrong.com/posts/J9eF4nA6wJW6hPueN/the-6d-effect-when-companies-take-risks-one-email-can-be | The 6D effect: When companies take risks, one email can be very powerful. ‚Äî LessWrong\n",
      "https://rand.org/pubs/testimonies/CTA2824-1.html | Advancing Trustworthy Artificial Intelligence  RAND\n",
      "https://stripe.com/newsroom/news/ceo-patrick-collisons-email-to-stripe-employees | CEO Patrick Collison's email to Stripe employees\n",
      "https://avidml.org/arva/ | AI Risk and Vulnerability Alliance  AVID\n",
      "https://ailabwatch.org/ | Lab Watch\n",
      "https://guarded-everglades-89687.herokuapp.com/?url=&title=&aggregator=-Custom&before=&after=&page=1&sort=&starred= | Upcoming Links\n",
      "https://blogs.microsoft.com/on-the-issues/2023/10/26/microsofts-ai-safety-policies/ | Microsoft‚Äôs AI Safety Policies - Microsoft On the Issues\n",
      "https://apnews.com/article/openai-chatgpt-investigation-federal-ftc-76c6218c506996942282d7f5d608088e | FTC investigating ChatGPT creator OpenAI over consumer protection issues  AP News\n",
      "https://twitter.com/Simeon_Cps/status/1719056018359906396 | Sim√©on on X: \"AI world experts including Yoshua Bengio, Yi Zeng, @GaryMarcus and Viktoria Krakovna are asking the UK international AI summit to launch a working group on an international AI Safety Treaty that would include: 1) Compute thresholds 2) CERN for AI 3) Safe APIs Very exciting‚Ä¶\" / X\n",
      "https://docs.google.com/spreadsheets/d/1AAIebjNsnJj_uKALHbXNfn3_YsT6sHXtCU0q7OIPuc4/edit#gid=0 | Parameter, Compute and Data Trends in Machine Learning - Google Sheets\n",
      "https://lesswrong.com/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1 | Model Organisms of Misalignment: The Case for a New Pillar of Alignment Research ‚Äî LessWrong\n",
      "https://docs.google.com/document/d/1rwM5tY8yS1YIhhfVOqUg868aZDWuHD-SOld4AMqCF4s/edit | Quarterly Review, Plan for 2023 Sep-Nov - Google Docs\n",
      "https://lesswrong.com/posts/vFqa8DZCuhyrbSnyx/integrity-in-ai-governance-and-advocacy | Integrity in AI Governance and Advocacy ‚Äî LessWrong\n",
      "https://twitter.com/norabelrose/status/1718388451584934150 | (1) Nora Belrose on X: \"The kind of AI deception that @dwarkesh_sp and @liron are worried about is less than 1% likely IMO, so I get why @ShaneLegg isn't actively planning for it. That said, I'm training 1000s of NNs with different random seeds right now to empirically test how likely it actually is.\" / X\n",
      "https://twitter.com/DrNikkiTeran/status/1719048031549505974 | Nikki Teran on X: \"Will releasing the weights of large language models grant widespread access to pandemic agents? Turns out, yes, probably. 1/5 t.co/nl5rawvToc\" / X\n",
      "https://thefuturesociety.org/heavy-is-the-head-that-wears-the-crown/?ct=t(EMAIL_CAMPAIGN_1NOV23) | Heavy is the Head that Wears the Crown - The Future Society\n",
      "https://twitter.com/saffronhuang/status/1719826694414995687 | (1) Saffron Huang on X: \"Humbled to have presented on ‚ÄòRisks from Frontier Al Integration into Society‚Äô at the #AlSafetySummit. And excited to be advancing sociotechnical evals at the Taskforce-turned-Al Safety Institute w/ @collect_intel :) t.co/qSbjqGcDzC\" / X\n",
      "https://gov.uk/government/news/leading-frontier-ai-companies-publish-safety-policies | Leading frontier AI companies publish safety policies - GOV.UK\n",
      "https://lesswrong.com/posts/AocXh6gJ9tJC2WyCL/book-review-going-infinite#Where_Was_This_Guy_ | Book Review: Going Infinite ‚Äî LessWrong\n",
      "https://thefuturesociety.org/a-blueprint-for-the-european-ai-office/?ct=t(EMAIL_CAMPAIGN_1NOV23) | A Blueprint for the European AI Office - The Future Society\n",
      "https://twitter.com/lxrjl/status/1717885547606118863 | alex lawsen on X: \"Very exciting to see that, ahead of the summit, Amazon, Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI have all been asked to outline their policies on 9 areas of AI safety.\" / X\n",
      "https://docs.google.com/spreadsheets/d/1VgX8Tq6VIwpbEUHHSem97enDeyUll2Fbd3uJu107hts/edit#gid=0 | SFF + Lightspeed details - Google Sheets\n",
      "https://docs.google.com/document/d/1Ys2WVoiskLlzMyILJxir-1FZwd2Ffmb60FggX8FGXOU/edit#heading=h.7nuantk77hs | Marcus / Peter Post - Draft 1 - How Rethink Priorities is Addressing Risk and Uncertainty - Google Docs\n",
      "https://twitter.com/Simeon_Cps/status/1719549343244378296 | Sim√©on on X: \"Could GPT hide information in its words that it understands but that humans don't? Yes! And it can allow it to perform better!\" / X\n",
      "https://twitter.com/kesvelt/status/1720440451059335520 | Kevin Esvelt on X: \"MWP v2: Can biology kill &gt;100m? Yes: smallpox. Can biology do worse? Yes: myxoma killed &gt;90% of rabbits. Could a biotech expert match this within 10y? Surprising if not. Would sharing future model weights give everyone an amoral biotech-expert tutor? Yes. Therefore, let‚Äôs not. t.co/nXleUJJbXv\" / X\n",
      "https://flf.org/ | The Future of Life Foundation\n",
      "https://docs.google.com/document/d/1zm4-Uga394tfrkX5Xj2oO6K2vhL_Nxj-7YyiL7Zixj4/edit | Potential AI Forecasting Qs - Google Docs\n",
      "https://metaculus.com/questions/18825/us-2024-election-balance-of-power/ | US 2024 election balance of power  Metaculus\n",
      "https://futureoflife.org/project/eu-ai-act/ | Strengthening the European AI Act - Future of Life Institute\n",
      "https://forum.effectivealtruism.org/s/WdL3LE5LHvTwWmyqj/p/LCfd56cBeRzrMiAhw | Is x-risk the most cost-effective if we count only the next few generations? ‚Äî EA Forum\n",
      "https://twitter.com/TheDevanshMehta/status/1719987035598151781 | (1) Devansh Mehta on X: \"A üßµ on how to conduct PRODUCTIVE group meetings of over 10 people 1. Create prompt questions in a figma jam 2. Have all participants submit ideas to the prompts via sticky notes 3. Give a thumbs up on sticky notes you like, briefly discuss the most upvoted ideas t.co/lRsbhtt4Ma\" / X\n",
      "https://openphilanthropy.org/research/europes-animal-welfare-reforms-are-under-threat/ | Europe‚Äôs animal welfare reforms are under threat  Open Philanthropy\n",
      "https://constellation.org/programs/researcher-program | Constellation Visiting Researcher Program\n",
      "https://lesswrong.com/posts/vm7FRyPWGCqDHy6LF/dario-amodei-s-prepared-remarks-from-the-uk-ai-safety-summit | Dario Amodei‚Äôs prepared remarks from the UK AI Safety Summit, on Anthropic‚Äôs Responsible Scaling Policy ‚Äî LessWrong\n",
      "https://forum.effectivealtruism.org/posts/pniDWyjc9vY5sjGre/rethink-priorities-cross-cause-cost-effectiveness-model?commentId=N98gLkDi87yJZpdaf | Rethink Priorities‚Äô Cross-Cause Cost-Effectiveness Model: Introduction and Overview ‚Äî EA Forum\n",
      "https://metaculus.com/questions/11608/self-driving-taxis-available-to-metaculites/?sub-question=5306 | Self-Driving Taxis Available to Metaculites  Metaculus\n",
      "https://docs.google.com/spreadsheets/d/1b8TfmWMxoHnOTFAlEfqJQZlvJo0487lPqqb-Z4NOoys/edit#gid=0 | [master copy] IAPS budget [for Aug 2023 fundraising] - Google Sheets\n",
      "https://docs.google.com/document/d/1OnhAKi9kBtxL_8yd4PT5NN9oN8I3QxIC79uhd-yUtSs/edit#heading=h.62zbnk202lxw | Nov 2023 US EO on AI - Google Docs\n",
      "https://twitter.com/acesounderglass/status/1721024661646152154 | Elizabeth Van Nostrand on X: \"This is your call to test the potato + watermelon diet\" / X\n",
      "https://twitter.com/Kirsten3531/status/1721474597709611226 | Kirsten on X: \"okay I want to have this for every part of my life. please tell me your backup safety mechanisms, the default bare minimum that just happens automatically so your basic needs get met\" / X\n",
      "https://mail.google.com/mail/u/2/#inbox | Inbox - peter@iaps.ai - Institute for AI Policy and Strategy Mail\n",
      "https://evals.alignment.org/blog/2023-08-01-new-report/ | New report: Evaluating Language-Model Agents on Realistic Autonomous Tasks - ARC Evals\n",
      "https://twitter.com/jesswhittles/status/1720009847830163878 | Jess Whittlestone on X: \"1/ I was pleased to have an opportunity to provide some brief closing remarks at the UK's #AISafetySummit yesterday (I start speaking around 52:00) A (very slightly edited) summary of my remarks below, including what I hope to see by the next summit in 6 months' time:\" / X\n",
      "https://redfin.com/news/november-layoff/#:~:text=To%20every%20departing%20employee%20who,More%20information%20is%20on%20Fin. | All-Hands Email on November Layoff - Redfin Real Estate News\n",
      "https://docs.google.com/document/d/1NHFNXfzMnKtaFUxByCXns27syAlhjjMqAP4co8y8Ue0/edit?userstoinvite=adam@bluedotimpact.org&sharingaction=manageaccess&role=writer | Peter's Reflections on Summit on Existential Security - Google Docs\n",
      "https://google.com/search?q=tent+pole+advocacy&rlz=1CDGOYI_enUS715US715&oq=tent+pole+advocacy&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIHCAEQABiiBDIHCAIQABiiBDIHCAMQABiiBDIHCAQQABiiBNIBCTIxODI2ajBqN6gCALACAA&hl=en-US&sourceid=chrome-mobile&ie=UTF-8 | tent pole advocacy - Google Search\n",
      "https://forum.effectivealtruism.org/posts/hFPbe2ZwmB9athsXT/clean-water-the-incredible-30-mortality-reducer-we-can-t | Clean Water - the incredible 30% mortality reducer we can‚Äôt explain ‚Äî EA Forum\n",
      "https://forum.effectivealtruism.org/posts/vQFBtHqgcJAwPpwEu/improving-the-welfare-of-ais-a-nearcasted-proposal | Improving the Welfare of AIs: A Nearcasted Proposal ‚Äî EA Forum\n",
      "https://meritalk.com/articles/fcc-prepares-for-splash-into-ai-regulatory-waters/ | FCC Prepares for Splash Into AI Regulatory Waters ‚Äì MeriTalk\n",
      "https://docs.google.com/spreadsheets/d/1hCOQy0D0xTCQeXYG_E6QbiHnQTU5wD_K4lDi9PzikCs/edit#gid=0 | [shared] Comparing \"Emerging Processes for Frontier AI Safety\" to companies' AI Safety Policies - Google Sheets\n",
      "https://aitreaty.org/ | Urging an International AI Treaty: An Open Letter  aitreaty.org\n",
      "https://lesswrong.com/posts/eJ7pm7LahehddYxNw/eli5-why-isn-t-alignment-easier-as-models-get-stronger | ELI5 Why isn't alignment *easier* as models get stronger? ‚Äî LessWrong\n",
      "https://twitter.com/fiiiiiist/status/1717546875849437542 | Tim Fist on X: \"Which frontier AI regulatory measures are actually shovel-ready today? @hlntnr and I ran a workshop series with ppl across academia, civil society, and industry, with voices both supportive and skeptical of frontier AI regs. Here's what we found: t.co/L8szoFyMNn\" / X\n",
      "https://rand.org/pubs/testimonies/CTA2723-1.html | Artificial Intelligence: Challenges and Opportunities for the Department of Defense  RAND\n",
      "https://twitter.com/andyzou_jiaming/status/1709365304789238201 | (1) Andy Zou on X: \"LLMs can hallucinate and lie. They can be jailbroken by weird suffixes. They memorize training data and exhibit biases. üß† We shed light on all of these phenomena with a new approach to AI transparency. üßµ Website: t.co/J7ikNYmZP9 Paper: t.co/b84PXQOOsc t.co/uwO7tzd2Ie\" / X\n",
      "https://docs.google.com/document/d/1PE8eMfwTHehf56w1aFChv31Uw-4_rYKc7S_TfgCH0TQ/edit | Ben, Peter, Renan - Meeting Notes - Google Docs\n",
      "https://twitter.com/robbensinger/status/1719051718674043090 | Rob Bensinger ‚èπÔ∏è on X: \"Institutions work better when people freely share ass numbers (and subjective probabilities more generally). E.g., a lot of the CDC's COVID errors stem from black-and-white thinking and speaking. But publicly communicating ass numbers is tricky.\" / X\n",
      "https://docs.google.com/document/d/1oD0tQuVkj0lWWVjbCy27tLfC2TjDJfRTUloh4sFSwQQ/edit | Executive Research Assistant - Interview - Google Docs\n",
      "https://docs.google.com/spreadsheets/d/1oWHhbn19dom2vW6tx8fCSfLjSAyQHIv1daI4wKDYv90/edit#gid=0 | Accomodation in Brussels - Google Sheets\n",
      "https://futureoflife.org/our-work/grantmaking-work/ | Grantmaking work - Future of Life Institute\n",
      "https://docs.google.com/document/d/1czt0fKR6S-JWw5_11TquXAAkE-hMu28Sl-4by-dCtsM/edit | Caro-Peter: Oxford, The Magical Retreat II - Google Docs\n",
      "https://twitter.com/JeffLadish/status/1720871947528220917 | twitter.com/JeffLadish/status/1720871947528220917\n",
      "https://forum.effectivealtruism.org/posts/ZuzK2s4JsJcexBJxy/will-releasing-the-weights-of-large-language-models-grant | Will releasing the weights of large language models grant widespread access to pandemic agents? ‚Äî EA Forum\n",
      "https://docs.google.com/document/d/15IRosp-aUqJ8dvMp9Zz1HNI3jjlX-l54iYynrKdbOdI/edit | ZW Thoughts - Responsibility Splits - Oct 2023 - Google Docs\n",
      "https://twitter.com/davidmanheim/status/1719687126923506017 | David Manheim (Follow me on $otherplatform!) on X: \"College is supposed to be full-time. Evidently, the expectation in elite schools is that a full course load should make sure you still have time for a part-time consulting project, protests, and the extra-curricular goals students have. It seems @Princeton doesn't do that - yet. t.co/RpoBIXnAO9\" / X\n",
      "https://twitter.com/davidmanheim/status/1719243636817248766 | David Manheim (Follow me on $otherplatform!) on X: \"@daniel_271828 \"Use AI to advance cybersecurity\" seems decidedly net-negative to me. Why is that not a dangerous domain for escalation? Instead, we should be rebuilding a secure computing stack, without AI - per the below. t.co/k3YvOphhiJ\" / X\n",
      "https://rand.org/pubs/working_papers/WRA2849-1.html | Securing Artificial Intelligence Model Weights: Interim Report  RAND\n",
      "https://omidyar.com/wp-content/uploads/2020/09/ON_POV_PDF-2020.pdf | ON_POV_PDF-2020.pdf\n",
      "https://bbc.com/news/technology-67172230 | Can Rishi Sunak‚Äôs big summit save us from AI nightmare? - BBC News\n",
      "https://twitter.com/simonw/status/1718637168602878169 | Simon Willison on X: \"I'm really surprised to see browsing and code interpreter made available in the same session - feels like a potent vector for creative prompt injection attacks against the combination of the two\" / X\n",
      "https://taigarchive.com/documents/navigating-ai-the-big-picture-(may-2023-slides) | Navigating AI ‚Äî the big picture (May 2023 slides) - TAIGA\n",
      "https://twitter.com/StephenLCasper/status/1720896730483548289 | twitter.com/StephenLCasper/status/1720896730483548289\n",
      "https://saidtwice.substack.com/ | Said Twice  Eirin Evjen  Substack\n",
      "https://metaculus.com/questions/19643/number-of-hostages-freed-by-2024/ | Number of Hostages Freed by 2024?  Metaculus\n",
      "https://localhost:8888/lab/tree/Tab%20sorts.ipynb | JupyterLab\n",
      "https://forum.effectivealtruism.org/posts/vq5pHzrxLgABAwkhD/shrimp-paste-might-consume-more-animal-lives-than-any-other | Shrimp paste might cause more animal deaths than any other food product. Who‚Äôs working on this? ‚Äî EA Forum\n",
      "https://wikiwand.com/en/Federal_Trade_Commission | Federal Trade Commission - Wikiwand\n",
      "https://rand.org/pubs/testimonies/CTA2654-1.html | Challenges to U.S. National Security and Competitiveness Posed by AI  RAND\n",
      "https://gov.uk/government/publications/frontier-ai-taskforce-second-progress-report/frontier-ai-taskforce-second-progress-report | Frontier AI Taskforce: second progress report - GOV.UK\n",
      "https://mail.google.com/mail/u/1/#inbox | Inbox - peter@rethinkpriorities.org - Rethink Priorities Mail\n",
      "https://twitter.com/shaunkeee/status/1720409161895473165 | Shaun K.E. Ee on X: \"[1/15] How can we adapt cybersecurity frameworks to address frontier AI risks? My colleagues at @_IAPS_ and I explore this question in our new ‚Äúdefense-in-depth‚Äù paper and provide recs for the Frontier Model Forum, @NIST, @CISAgov, @MITREcorp, and others: t.co/ROUiEd8JI8 t.co/Bwi3FcfiFa\" / X\n",
      "https://docs.google.com/spreadsheets/d/1TlWcxy-fuzXd93DEJ_sj8R6Ikm4HJMZd92sXULbnZvM/edit#gid=0 | [very confidential] Staff Risk - Google Sheets\n",
      "https://localhost:8889/lab/tree/Fundraising%20Forecast%20Lite.ipynb | Fundraising ‚Ä¶ - JupyterLab\n",
      "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4579773 | International AI Institutions: A Literature Review of Models, Examples, and Proposals by Matthijs M. Maas, Jos√© Jaime Villalobos :: SSRN\n",
      "https://omidyar.com/wp-content/uploads/2020/09/Omidyar-Network-POV_Platforms-and-Power_2.7.2020.pdf | Omidyar-Network-POV_Platforms-and-Power_2.7.2020.pdf\n",
      "https://forum.effectivealtruism.org/posts/S9H86osFKhfFBCday/how-bad-would-human-extinction-be | How bad would human extinction be? ‚Äî EA Forum\n",
      "https://taigarchive.com/documents/estimating-the-implications-of-advanced-ai-for-automating-chip-design | Estimating the implications of advanced AI for automating chip design - TAIGA\n",
      "https://www-p404.intacct.com/ia/acct/frameset.phtml?.sess=G0nS9jvXYvJtycPw8iACc3yH8mxUEhtJiS3eyK-UbcnD8PIgAmMyyc9t&.cc=KF0e7hxs_O9ZI_VPiHeunVSavExwUvROZCF-QUNCVnk. | Rethink Priorities\n",
      "https://thefuturesociety.org/response-to-u-s-ostp-request-for-information/?ct=t(EMAIL_CAMPAIGN_1NOV23) | Response to U.S. OSTP Request for Information on National Priorities for AI - The Future Society\n",
      "https://twitter.com/Kirsten3531/status/1721471962852016281 | Kirsten on X: \"So the best thing about having a cleaner, which I absolutely did not expect, is how much less anxiety I feel There's a part of my brain that was like \"if we don't clean the crumbs off the toaster today maybe it'll never ever ever happen\" And now I know it'll happen in 2 weeks\" / X\n",
      "https://twitter.com/AaronBergman18/status/1719066808127242410 | Aaron Bergman üîç ‚è∏Ô∏è (in that order) on X: \"Based and correct understanding of risk aversion-pilled @Laura_k_Duffy I am begging people to understand that ‚Äúrisk aversion‚Äù doesn‚Äôt just mean ‚Äúhave narrow confidence intervals on the magnitude of one of the first-order effects of an intervention‚Äù (link below) t.co/3pp6EpfEeX\" / X\n",
      "https://twitter.com/JgaltTweets/status/1718440984596422762 | (8) JgaltTweets on X: \"Fwiw I think his approval rating will probably improve a bit by the time of the election; but mostly because of polarization (i.e. voters comparing him to Trump) rather than actually evaluating him more positively. Will probably still be quite underwater then, though. t.co/dqJarRur4M\" / X\n",
      "https://metaculus.com/questions/16639/non-democratic-brontier-ai-lab-before-2026/ | Non-Democratic Frontier AI Lab before 2026?  Metaculus\n",
      "https://twitter.com/farairesearch/status/1719409979088884186 | FAR AI on X: \"Codebook Features make language models more interpretable and controllable, with minimal performance loss! Our method turns complex vectors into discrete codes, providing a potential path toward safer and more reliable machine learning systems. t.co/L24zY9AtrC\" / X\n",
      "https://whitehouse.gov/briefing-room/statements-releases/2023/11/01/fact-sheet-vice-president-harris-announces-new-u-s-initiatives-to-advance-the-safe-and-responsible-use-of-artificial-intelligence/ | FACT SHEET: Vice President Harris Announces New U.S. Initiatives to Advance the Safe and Responsible Use of Artificial Intelligence  The White House\n",
      "https://twitter.com/Simeon_Cps/status/1720513387304517816 | Sim√©on on X: \"I've been chatting to many of you about alternative architectures as a more credible path to safety than making transformers safe. If you want to dig deeper into one of the most exciting one out there, the Open Agency Architecture, check the bibliography of this announcement.\" / X\n",
      "https://twitter.com/CharlotteSiegm/status/1720110570832281623 | Charlotte Siegmann on X: \"Yesterday, I was fooled by this fake LLM-generated website. Took me more than 10 minutes to figure out this was fake. Why did it take me so long? The women in the photos looked real and trustworthy. My brain still needs to fully update that models can generate that.‚Ä¶\" / X\n",
      "https://docs.google.com/spreadsheets/d/1RFlb6tuqYH71ljpmqq1Y7BhbU6Cfy9CrMLAE-gXEhug/edit#gid=0 | Big rocks weekly planning - Google Sheets\n",
      "https://twitter.com/kesvelt/status/1718976444175425796 | (6) Kevin Esvelt on X: \"Will sharing the weights of future foundation models negate safeguards? To find out, we ran a hackathon at MIT in which participants playing compulsively honest bioterrorists asked Base and Spicy versions of Llama-2 how to obtain 1918 influenza virus. 1/7 t.co/qVxUc21sRr t.co/34IzMipTYL\" / X\n",
      "https://lesswrong.com/posts/jyM7MSTvy8Qs6aZcz/what-s-up-with-responsible-scaling-policies | What's up with \"Responsible Scaling Policies\"? ‚Äî LessWrong\n",
      "https://docs.google.com/spreadsheets/d/1sJlC9PLWGxQ6XQsvPVybnJtuKjNW7iKmYv_WqZ3cjXQ/edit#gid=1605902912 | DRAFTING RP 2024 Draft Budget - Google Sheets\n",
      "https://aiscc.org/2023/11/01/yougov-poll-83-of-brits-demand-companies-prove-ai-systems-are-safe-before-release/ | YouGov poll: 83% of Brits demand companies prove AI systems are safe before release ‚Äì AI Safety Communications Centre\n",
      "https://fmprc.gov.cn/mfa_eng/wjdt_665385/2649_665393/202310/t20231020_11164834.html | Global AI Governance Initiative\n",
      "https://docs.google.com/document/d/1_qZEVs4SE-F2uWBIb0cxynjceO0LkhjrlXssJd-eM34/edit#heading=h.xhhs1xccdr60 | [WIP] Late-stage IR - Google Docs\n",
      "https://doingwestminsterbetter.substack.com/p/will-ai-safety-policy-survive-rishi | Will AI safety policy survive Rishi Sunak?\n",
      "https://lesswrong.com/posts/9Fdd9N7Escg3tcymb/preventing-language-models-from-hiding-their-reasoning | Preventing Language Models from hiding their reasoning ‚Äî LessWrong\n",
      "https://rand.org/pubs/testimonies/CTA2953-1.html | Preparing the Federal Response to Advanced Technologies  RAND\n",
      "https://mail.google.com/mail/u/0/#inbox | Inbox (46) - peter@peterhurford.com - Peter Hurford Mail\n",
      "https://forum.effectivealtruism.org/posts/vwJcuyb8wCwXFktJS/ea-infrastructure-fund-june-2023-grant-recommendations | EA Infrastructure Fund: June 2023 grant recommendations ‚Äî EA Forum\n",
      "https://datasociety.net/library/ai-red-teaming-is-not-a-one-stop-solution-to-ai-harms-recommendations-for-using-red-teaming-for-ai-accountability/ | Data & Society ‚Äî AI Red-Teaming Is Not a One-Stop Solution to AI Harms: Recommendations for Using Red-Teaming for AI Accountability\n",
      "https://youtube.com/watch?v=5nb4vnpiMMI | Improving AI benchmarks by Isabel Juniewicz - YouTube\n",
      "https://docs.google.com/document/d/1AyANUnE-yZl9y1qUDz2cJdoBsG8zzeM4Kr7QFC8NmBA/edit#heading=h.xxfi8rk39qa7 | IAPS all-hands meetings 2023 Oct - Google Docs\n",
      "https://gov.uk/search/all?order=updated-newest&topical_events%5B%5D=ai-safety-summit-2023 | gov.uk/search/all?order=updated-newest&topical_events%5B%5D=ai-safety-summit-2023\n",
      "https://lesswrong.com/posts/gQyphPbaLHBMJoghD/comp-sci-in-2027-short-story-by-eliezer-yudkowsky | Comp Sci in 2027 (Short story by Eliezer Yudkowsky) ‚Äî LessWrong\n",
      "https://time.com/6327635/ai-needs-to-be-regulated-like-nuclear-weapons/ | How AI Needs to Be Regulated like Nuclear Energy  TIME\n",
      "https://docs.google.com/spreadsheets/d/1nZu1kxcmkiQ22tjx9pFwW8fOcuXsHBklBDVHBU4wDjw/edit#gid=539134426 | RP Revenue by Dept - Google Sheets\n",
      "https://arxiv.org/abs/2303.11341 | [2303.11341] What does it take to catch a Chinchilla? Verifying Rules on Large-Scale Neural Network Training via Compute Monitoring\n",
      "https://lesswrong.com/posts/a3RjXa2dryoH6Xgij/managing-ai-risks-in-an-era-of-rapid-progress | Managing AI Risks in an Era of Rapid Progress ‚Äî LessWrong\n",
      "https://openphilanthropy.org/about/team/benjamin-tereick/ | Benjamin Tereick  Open Philanthropy\n",
      "https://google.com/search?q=pentagon+fire+deepfake&rlz=1CDGOYI_enUS715US715&oq=pentagon+fire+deepfake&gs_lcrp=EgZjaHJvbWUyBggAEEUYOdIBCDg0NTlqMGo3qAIAsAIA&hl=en-US&sourceid=chrome-mobile&ie=UTF-8 | pentagon fire deepfake - Google Search\n",
      "https://twitter.com/kesvelt/status/1718977801431155173 | Kevin Esvelt on X: \"Examining worst-case pandemic scenarios, we identified two that could collapse civilization. We‚Äôve outlined defenses against both types of pandemic and are working on them, but for now we're extremely vulnerable. 4/7 t.co/rYzRiN4Tpm t.co/w1IraUdxaZ\" / X\n",
      "https://metaculus.com/questions/19492/israel-offensive-in-gaza-by-november-1-2023/ | Israel Offensive in Gaza by November 1, 2023?  Metaculus\n",
      "https://twitter.com/kanjun/status/1720502401067811242 | Kanjun üêôüè° on X: \"Reflections on UK AI Safety Summitüëá 1/ People agree way more than expected. National ministers, AI lab leaders, safety researchers all rallied on infrastructure hardening, continuous evals, global coordination. Views were nuanced; Twitter is a disservice to complex discussion.\" / X\n",
      "https://docs.google.com/document/d/1qjoLVsStP4iktkXcYP5oKSIM20FEWDcS/edit | 360 Reflection_Peter Wildeford.docx - Google Docs\n",
      "https://docs.google.com/document/d/1i1KAfwLU5G1fyjO82DJyRVrtTJHR5jPcCKjwjQ0scC4/edit#heading=h.72mwfkgg6lcn | *XST strategy meetings ‚Äì 2023 Q3 - Google Docs\n",
      "https://cnas.org/press/noteworthy/noteworthy-executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence | NOTEWORTHY: Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence  Center for a New American Security (en-US)\n",
      "https://arxiv.org/abs/2307.03718 | [2307.03718] Frontier AI Regulation: Managing Emerging Risks to Public Safety\n",
      "https://omidyar.com/omidyar_team/troy-perry/ | Troy Perry - Omidyar Network\n",
      "https://lesswrong.com/posts/JcLhYQQADzTsAEaXd/ai-as-a-science-and-three-obstacles-to-alignment-strategies | AI as a science, and three obstacles to alignment strategies ‚Äî LessWrong\n",
      "https://docs.google.com/spreadsheets/d/1FROfNCchRS8nYmPSkwsBF6V5ncQTaeMvjbB_5tkkde8/edit#gid=1708690655 | Program Service Revenue Tracking - Overall - Google Sheets\n",
      "https://pbs.twimg.com/media/F97ifwdW0AAzRZw?format=jpg&name=4096x4096 | F97ifwdW0AAzRZw (2366√ó1660)\n",
      "https://twitter.com/Simeon_Cps/status/1719935861314228591 | Sim√©on on X: \"Why is the Responsible Scaling Policies (RSPs) framework inadequate for risk management? 1) It's missing core components of basic risk management procedures. i) It's not measuring an aggregate level risk level ii) it's not specifying what the aggregate risk limit‚Ä¶ t.co/rPC00dEyTS\" / X\n",
      "https://docs.google.com/document/d/1gNHs1tEw6btzxYqFZIyDVyVlWa-L4rmdaHUG7-TNl0I/edit | Close Read of \"Keynote Remarks by U/S Jenkins (T) to the Summit on Responsible Artificial Intelligence in the Military Domain (REAIM) Ministerial Segment\" - Google Docs\n",
      "https://twitter.com/CFGeek/status/1720514327990694129 | Charles Foster on X: \"Worried about the future of openness in AI? Here is a way to help: We're putting together a public list of all the good work that's been enabled by open-weight foundation models, to show why transparency &amp; public scrutiny is worth protecting. ‚¨áÔ∏è Links below ‚¨áÔ∏è t.co/eSMumNhtA5\" / X\n",
      "https://docs.google.com/document/d/19rKkgNqaMPRISPqjJGU48GfXxNzZYjrtds-FTI52Dlc/edit | Peter <> Michael - 1-1s - 2023 Q3 - Google Docs\n",
      "https://technologyreview.com/2022/11/18/1063487/meta-large-language-model-ai-only-survived-three-days-gpt-3-science/ | Why Meta‚Äôs latest large language model only survived three days online  MIT Technology Review\n",
      "https://twitter.com/messages/25776739-27569373 | twitter.com/messages/25776739-27569373\n",
      "https://twitter.com/typedfemale/status/1718878835910045931 | twitter.com/typedfemale/status/1718878835910045931\n",
      "https://thefuturesociety.org/response-to-u-s-ntia-ai-accountability-policy-request-for-comment/?ct=t(EMAIL_CAMPAIGN_1NOV23) | Response to U.S. NTIA AI Accountability Policy Request for Comment - The Future Society\n",
      "https://twitter.com/MatthewJBar/status/1719244420929991165 | Matthew Barnett on X: \"@scholl_adam @peterwildeford If this executive order is in the 99th percentile of what people expected, it's a reasonable inference that the 50th percentile was something like \"almost no serious regulation\". And also more than a few people told me they didn't expect much AI regulation.\" / X\n",
      "https://twitter.com/HaydnBelfield/status/1719326991780913484 | Haydn Belfield on X: \"Meta 48% t.co/wH80BdW6re\" / X\n",
      "https://jefftk.com/p/computational-approaches-to-pathogen-detection | Computational Approaches to Pathogen Detection\n",
      "https://wikiwand.com/en/Madeleine_Albright | Madeleine Albright - Wikiwand\n",
      "https://arxiv.org/abs/2310.19736 | [2310.19736] Evaluating Large Language Models: A Comprehensive Survey\n",
      "https://google.com/search?q=bayh+dole+act&rlz=1CDGOYI_enUS715US715&oq=bay+doyle+act&gs_lcrp=EgZjaHJvbWUqCQgBEAAYChiABDIGCAAQRRg5MgkIARAAGAoYgAQyCQgCEAAYChiABDIJCAMQABgKGIAEMgkIBBAAGAoYgAQyCQgFEAAYChiABDIJCAYQABgKGIAEMgkIBxAAGAoYgAQyCQgIEAAYChiABDIJCAkQABgKGIAE0gEIMzA4M2oxajeoAgCwAgA&hl=en-US&sourceid=chrome-mobile&ie=UTF-8 | bayh dole act - Google Search\n",
      "https://facebook.com/spencer.greenberg/posts/pfbid0n55WBJXUipuDyuFu1BDzYxKoEx44wGnb7qi68D5UghKK6KoYADoCMc12Y9fAecw5l | People often talk about how women are... - Spencer Greenberg  Facebook\n",
      "https://forum.effectivealtruism.org/posts/zaSagsDjRmStfJ7MW/responsible-scaling-policies-are-risk-management-done-wrong | Responsible Scaling Policies Are Risk Management Done Wrong ‚Äî EA Forum\n",
      "https://metaculus.com/questions/19765/donald-trump-jailed-before-2024/ | Donald Trump Jailed Before 2024?  Metaculus\n",
      "https://twitter.com/StephenLCasper/status/1720910484441014525 | Stephen Casper ‚è∏Ô∏è on X: \"I like this! \"The goal of task blocking is to create models that increase the costs of fine-tuning on harmful downstream tasks such that an adversary would rather start from scratch than use the pretrained model, while remaining useful for desired tasks\" t.co/4cB6IrWaFK t.co/PTvTDI18TY\" / X\n",
      "https://safer-ai.org/ | SaferAI\n",
      "https://docs.google.com/document/d/1JKY9I1H3d9FJIEKOAjAklbNW0sXW54x-FJPDxU43tgQ/edit | Current USG Authority to Influence AI Development - Google Docs\n",
      "https://docs.google.com/document/d/1XVslWOt0wNVeEAhEe0e1PG9k6GWLMXckBi2NRHrWEBA/edit#heading=h.osty8jeclpyn | WAW risk aversion - Google Docs\n",
      "https://politico.com/news/magazine/2023/11/02/bruce-reed-ai-biden-tech-00124375 | The Man Behind Biden‚Äôs Sweeping AI Executive Order - POLITICO\n",
      "https://gov.uk/government/publications/emerging-processes-for-frontier-ai-safety/emerging-processes-for-frontier-ai-safety | Emerging processes for frontier AI safety - GOV.UK\n",
      "https://docs.google.com/document/u/0/d/1os_4YOw6Xv33KjX-kR76D3kW1drkWRHKG2caeiEWzNs/mobilebasic | Part 0 - What a compute-centric framework says about takeoff speeds: short summary and long summary\n",
      "https://twitter.com/ChanaMessinger/status/1719099354827374891 | (23) Chana on X: \"@peterwildeford What would a bad version of the executive order looked like to you? (I'm trying to figure out how to evaluate this kind of thing)\" / X\n",
      "https://reddit.com/r/ChatGPT/comments/17hbx8f/prompt_challenge_can_you_get_chatgpt_to_generate/ | reddit.com/r/ChatGPT/comments/17hbx8f/prompt_challenge_can_you_get_chatgpt_to_generate/\n",
      "https://docs.google.com/document/d/16sbBh7MRmXKrsD_oOLNfbMgB4Yn8jqoUwMq9IM0_dWs/edit | Manifund Regranting for Surveys (Branding EA vs Longtermism vs X-risk...) 2023-07 - Google Docs\n",
      "https://forum.effectivealtruism.org/posts/jCwuozHHjeoLPLemB/how-long-do-policy-changes-matter-new-paper | How Long Do Policy Changes Matter? New Paper ‚Äî EA Forum\n",
      "https://twitter.com/JacquesThibs/status/1719116575221977505 | Jacques on X: \"@MatthewJBar @peterwildeford I think, for the most part, it was that they expected there would be AI regulation but that it would be basically irrelevant to their main concerns regarding agentic AGI systems. At most, it would cover misuse, and you‚Äôd likely get AI ethics stuff covered.\" / X\n",
      "https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/ | What will GPT-2030 look like?\n",
      "https://taigarchive.com/documents/the-history-of-sematech-and-lessons-for-state-involvement-in-emerging-technologies | The history of Sematech and lessons for state involvement in emerging technologies - TAIGA\n",
      "https://wired.com/story/ftx-founder-sam-bankman-fried-guilty-fraud-trial/ | Sam Bankman-Fried Has Been Found Guilty of Fraud  WIRED\n",
      "https://twitter.com/robbensinger/status/1720516817234620833 | Rob Bensinger ‚èπÔ∏è on X: \"Back in the day, x-risk folks considered the 2014 short film \"The Awareness\" by far the least unrealistic film depiction of smarter-than-human AI risk. (Beating eg Ex Machina.) Are there any better contenders nowadays? (Give a reason if you suggest one.) t.co/J0SnIspqgy\" / X\n",
      "https://docs.google.com/spreadsheets/d/1p2kKN_7CxVDJlosPQTWwCePNpZzKlG15N1ZJWlOyehQ/edit#gid=0 | 2023-08 EAIF for Extra EA Survey budget - Google Sheets\n",
      "https://twitter.com/TolgaBilge_/status/1719024093284995205 | Tolga Bilge on X: \"Top AI and Policy Experts Call for an International AI Safety Treaty In an open letter we just published, top experts including Yoshua Bengio, and over 100 others urge AI treaty developent to begin. We encourage all members of the public to sign below: t.co/J6qo7nK3Us\" / X\n",
      "https://lesswrong.com/posts/6dn6hnFRgqqWJbwk9/deception-chess-game-1 | Deception Chess: Game #1 ‚Äî LessWrong\n",
      "https://docs.google.com/document/d/1e8-P-DI_9su6fTT7U8_L4Ku-PewxcveQFQOLGzbCKso/edit | Caro and Peter's Chocolatey Adventure in Brussels: The AI and Waffle Chronicles - Google Docs\n",
      "https://twitter.com/alxndrdavies/status/1720435535855149513 | Xander Davies on X: \"Proud to be a researcher at the UK's AI Safety Institute‚Äîwhat is it? üßµ based on yesterday's introduction: t.co/MmnsxQvZZK\" / X\n",
      "https://twitter.com/soundboy/status/1718925374623519155 | (5) Ian Hogarth on X: \"1/ The Taskforce is a start-up inside government, delivering on the mission given to us by the Prime Minister: to build an AI research team that can evaluate risks at the frontier of AI. We are now 18 weeks old and this is our second progress report: t.co/FdEUYWiY8a\" / X\n",
      "https://docs.google.com/document/d/1lKzgAC7VQu1mVC7DAZACFU4VVsYjRjAc2rSFuCHW_A0/edit | Untitled document - Google Docs\n",
      "https://simonwillison.net/2023/Oct/17/open-questions/#open-questions.036.jpeg | Open questions for AI engineering\n",
      "https://nature.com/articles/d41586-023-03272-3 | AI ‚Äòbreakthrough‚Äô: neural net has human-like ability to generalize language\n",
      "https://futureoflife.org/wp-content/uploads/2023/04/FLI_Policymaking_In_The_Pause.pdf | FLI_Policymaking_In_The_Pause.pdf\n",
      "https://deepmind.google/public-policy/ai-summit-policies/ | AI Safety Summit: An update on our approach to safety and responsibility - Google DeepMind\n",
      "https://twitter.com/james_acton32/status/1717978150355783772 | (1) (((James Acton))) on X: \"üßµThoughts on the U.S. decision to develop the B61-13 nuclear warhead. I think this has to be seen in the context of Congressional efforts to stop the administration from withdrawing the old megaton-class B83. The fact sheet hints at this. (1/n) t.co/JcHQGAZe2q t.co/xzi7swkZu0\" / X\n",
      "https://forum.effectivealtruism.org/posts/t5x9xLew2ZeENAzzT/let-s-celebrate-some-wins | Let's celebrate some wins ‚Äî EA Forum\n",
      "https://docs.google.com/document/d/1W_Ezc0-xj1_GanHVCHyTe6RvmlahzUMb1N5MlrGo9mw/edit#heading=h.qvxqhft5lddc | IAPS comments on Anth RSPs - Google Docs\n",
      "https://theaidigest.org/progress-and-dangers | How fast is AI improving? - AI Digest\n",
      "https://cs.princeton.edu/~arvindn/talks/insight_forum_statement.pdf | Copy of The urgent need for accountability in predictive AI\n",
      "https://twitter.com/StefanFSchubert/status/1719338046057837012 | (21) Stefan Schubert on X: \"Yes, people giving explanations too frequently fail to make the obvious test, \"what about other countries?\" A bit embarrassing that this simple error is so common. Rationality is to an underappreciated extent about paying attention to these very basic things.\" / X\n",
      "https://docs.google.com/presentation/d/1NjE49tDpSbE0FiXwroxpW_spY3TkKxkNDknxBvsndO8/edit#slide=id.g208ea025851_0_304 | [latest] 2023 Nov RP Fundraising Forecast - Google Slides\n",
      "https://lcfi.ac.uk/news-and-events/news/2023/oct/31/ai-safety-policies/ | Site is not secure\n",
      "https://docs.google.com/spreadsheets/d/1vS1D40zwEsA6oBtnXnxWd0juDbDV7D5k4edJRd0tsEY/edit#gid=875232347 | RP Net Assets - Google Sheets\n",
      "https://forum.effectivealtruism.org/posts/3qpaRKe8R4ptiqSkr/uk-prime-minister-rishi-sunak-s-speech-on-ai | UK Prime Minister Rishi Sunak's Speech on AI ‚Äî EA Forum\n",
      "https://foreignaffairs.com/united-states/henry-kissinger-path-artificial-intelligence-arms-control | Henry Kissinger: The Path to AI Arms Control\n",
      "https://forum.effectivealtruism.org/posts/j2TreuRZT9mBFEMEs/the-bletchley-declaration-on-ai-safety | The Bletchley Declaration on AI Safety ‚Äî EA Forum\n",
      "https://punchbowl.news/wp-content/uploads/Canvass-AI.pdf | punchbowl.news/wp-content/uploads/Canvass-AI.pdf\n",
      "https://futureoflife.org/our-work/policy-work/ | Policy work - Future of Life Institute\n",
      "https://twitter.com/felix_red_panda/status/1718916631512949248 | (6) Felix on X: \"Microsoft paper claims ChatGPT 3.5 has ~20 billion parameters t.co/gZxh0l2VqX t.co/EDCWbLdYEz\" / X\n",
      "https://forum.effectivealtruism.org/posts/g4fXhiJyj6tdBhuBK/survey-on-intermediate-goals-in-ai-governance | Survey on intermediate goals in AI governance ‚Äî EA Forum\n",
      "https://twitter.com/Chris_Said/status/1718983908866380227 | (6) Chris Said on X: \"For $200 you can fine tune an open-source LLM to provide nearly all key information needed to obtain the 1918 pandemic influenza virus.\" / X\n",
      "https://cnas.org/press/in-the-news/can-america-handle-two-wars-and-maybe-a-third | Can America handle two wars, and maybe a third?  Center for a New American Security (en-US)\n",
      "https://twitter.com/AkashWasil/status/1720061569478873442/photo/1 | (1) Akash ‚è∏Ô∏è Wasil on X: \"**Summary of various AI governance proposals** Excellent work by @FLIxrisk that scores several AI governance on some relevant axes. IMO the most important axes are burden of proof, quantitative risk bounds, and compute limits. See üëá for a link to the FLI report. t.co/zV9KSgUvGm\" / X\n",
      "https://twitter.com/ohlennart/status/1717243855781712300 | twitter.com/ohlennart/status/1717243855781712300\n",
      "https://forum.effectivealtruism.org/posts/hDNpHEA2Kn4xBoS8r/impact-evaluation-in-ea | Impact Evaluation in EA ‚Äî EA Forum\n",
      "https://tosummarise.com/four-thousand-weeks-10-practical-tools-to-help-embrace-your-finitude/ | Four Thousand Weeks - 10 Practical Tools to Help Embrace Your Finitude - To Summarise\n",
      "https://openai.com/global-affairs/our-approach-to-frontier-risk | OpenAI‚Äôs Approach to Frontier Risk\n",
      "https://anthropic.com/uk-government-internal-ai-safety-policy-response | Our response to the UK Government‚Äôs internal AI safety‚Ä¶ \\ Anthropic\n",
      "https://twitter.com/Simeon_Cps/status/1717558074523934977 | twitter.com/Simeon_Cps/status/1717558074523934977\n",
      "https://arxiv.org/abs/2310.19737 | [2310.19737] Adversarial Attacks and Defenses in Large Language Models: Old and New Threats\n",
      "https://aisnakeoil.com/p/evaluating-llms-is-a-minefield | Evaluating LLMs is a minefield\n"
     ]
    }
   ],
   "source": [
    "print('Shuffled tabs! ({})'.format(len(tabs)))\n",
    "\n",
    "random.shuffle(tabs)\n",
    "\n",
    "print('-')\n",
    "for t in tabs:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a7eece4-8649-45d2-bd1f-5d0e2554c42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tabs opened!\n"
     ]
    }
   ],
   "source": [
    "open_tabs_from_text(\"\"\"\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
